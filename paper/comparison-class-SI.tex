\documentclass[doc]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{subfigure}

%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\DeclareLanguageMapping{american}{american-apa}

  \title{Supplemental Information\\ 
  Warm (for Winter): Inferring comparison classes in communication}
    \author{Michael Henry Tessler\textsuperscript{1,2} and Noah D. Goodman\textsuperscript{2, 3}} 
%         \author{Anonymous} 

       \date{}
  
\shorttitle{ SI: Inferring comparison classes}
% \affiliation{Anonymized}
\affiliation{
\vspace{0.5cm}
\textsuperscript{1}Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology \\\textsuperscript{2}Department of Psychology, Stanford University
\\\textsuperscript{3}Department of Computer Science, Stanford University}

\keywords{comparison class; reference class; adjectives; pragmatics; Rational Speech Act; Bayesian cognitive model; Bayesian data analysis\newline\indent Word count: 5308}
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage{tikz}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{siunitx}

% set the name of the folder in which the CSV files with
% information from R is stored
\newcommand{\datafoldername}{csv_data_4_tex}

% the following code defines the convenience functions
% as described in the main text below

% rlgetvalue returns whatever is the in cell of the CSV file
% be it string or number; it does not format anything
\newcommand{\rlgetvalue}[4]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\myvalue}}

% rlgetvariable is a shortcut for a specific CSV file (myvars.csv) in which
% individual variables that do not belong to a larger chunk can be stored
\newcommand{\rlgetvariable}[1]{\csvreader[]{\datafoldername/myvars.csv}{#1=\myvar}{\myvar}\xspace}

% rlnum format a decimal number
\newcommand{\rlnum}[2]{\num[output-decimal-marker={.},
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlnumsci}[2]{\num[output-decimal-marker={.},
                          scientific-notation = true,
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlgetnum}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnum{\myvalue}{#5}}}

\newcommand{\rlgetnumsci}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnumsci{\myvalue}{#5}}}

\newcommand{\lmresults}[2]{\(\beta = \rlgetnum{#1}{Rowname}{#2}{Estimate}{3}\), t\((\rlgetnum{#1}{Rowname}{#2}{df}{0}) = \rlgetnum{#1}{Rowname}{#2}{t.value}{2}, p = \rlgetnum{#1}{Rowname}{#2}{Pr...t..}{3}\)}

\newcommand{\brmresults}[2]{\(\beta = \rlgetnum{#1}{key}{#2}{mean}{2}\) [\rlgetnum{#1}{key}{#2}{l95}{2}, \rlgetnum{#1}{key}{#2}{u95}{2}]}

\newcommand{\hdiresults}[2]{\rlgetnum{#1}{param_name}{#2}{MAP}{2} (\rlgetnum{#1}{param_name}{#2}{cred_lower}{2}, \rlgetnum{#1}{param_name}{#2}{cred_upper}{2})}

\begin{document}
\maketitle

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\red}[1]{\textcolor{Red}{#1}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

%\appendix
%\renewcommand\thefigure{\thesection.\arabic{figure}}  

%\section{Document Overview}
%\renewcommand{\thepage}{S\arabic{page}}  
%\renewcommand{\thesection}{S\arabic{section}}   
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thetable}{S\arabic{table}}   
\renewcommand{\thefigure}{S\arabic{figure}}

Here we present additional information about methods, results, and cognitive models. Readers who are interested in the model code and analysis code are encouraged to consult the associated online repository: \url{https://github.com/mhtess/comparison-class-paper}.


%The following sections present details on the analyses used to evaluate whether the different experimental manipulations produced different responses. 
%For all generalized linear mixed models (GLMM) we used the maximally converging random effects structure.




\section{Overview of Tasks}

Our experimental procedures involve three coordinated experiments to (1) elicit test stimuli, (2) measure comparison class inferences and (3) measure adjective endorsements to facilitate model comparison (Figure \ref{fig:exptOverview}). 
In Task 1, we empirically elicit test stimuli by having participants fill out phrasal templates that elicit sets of categories at the same level of abstraction, which differ in the general expectations participants have about those categories (e.g., basketball players [generally tall], jockeys [generally short], soccer players [sometimes tall and sometimes short]). 
From this set, we curate a set of experimental stimuli which we use in the other two tasks.
Task 2 is the comparison class inference experiment described in the main text; this task is designed to test the qualitative predictions shown in Figure 1E (main text).
Task 3 (an adjective endorsement, or truth judgment task) provides additional linguistic judgments that constrain model parameters to test the quantitative predictions of the comparison class inference model and quantitatively arbitrate between alternative models. 
%We use Bayesian data analysis to jointly model data from the Comparison Class Inference and Adjective Endorsement tasks, based on latent world knowledge that affects both tasks.

\section{Experimental Methods}

\subsection{Bot check}

In all tasks, participants were required to pass a simple language comprehension test that we designed in order to weed out bots and other bad-faith participants. 
The test involved a sentence in which a named speaker (e.g., Joseph) says to a named listener (e.g., Elizabeth) ``It's a beautiful day, isn't it?''. 
Participants were asked to type in a text box to whom the speaker (in this case: Joseph) is talking (i.e., Elizabeth).
Speaker and listener names were randomized in a way that could not be read off the source .html file.
Participants were given three attempts to correctly identify the listener. 
If they did not succeed within 3 attempts, they would be unable to proceed with the experiment.
Since participants who fail this test are not allowed to proceed with the experiment, we do not have a count of how many participants fail this check.

\subsection{Stimuli Generation Task (Task 1)}

In this task, participants ($n=50$) filled out phrasal templates for adjective pairs (e.g., \emph{big} and \emph{small}), in which three missing noun phrases which appeared in the grammatical subject of the sentence were described as either generally having one adjective apply to them (e.g., \emph{\_\_\_ are generally big}; \emph{\_\_\_ are generally small}) or as sometimes having either adjective apply to them (e.g., \emph{\_\_\_ are sometimes big and sometimes small}). 
Participants filled out one template for each of 15 pairs of adjectives that describe physical dimensions (Table 1). 

From this set of 750 responses, we curated a collection of 90 ``item sets'', each of which consists of 3 relatively subordinate level categories that differ in their general expectations about the degree (e.g., Winter, Spring, and Summer, which differ in their general expectations about the typical temperature). In addition, each of these item sets is associated with a common, relatively superordinate level categories (e.g., days of the year).

\begin{figure*}[htb]
{\centering \includegraphics[width=0.9\textwidth]{figs/expt_overview} }
\caption{\small Overview of Experimental Tasks. Task 1: Using a structured production task, we elicit sets of stimuli that all share the feature of containing categories generally judged as having either a positive or negative adjective  (\emph{X, Y}; e.g., big or small) applied to them as well as a control category (\emph{Z}; e.g., sometimes big and sometimes small). The task is designed in a way to elicit three categories of the same basic- or superordinate-level category (\emph{W}). Task 2: Free-production task to elicit the comparison class. Task 3: Forced-choice task where participants judge whether a member of the subordinate level category would be judged as a having the adjective applied explicitly relative to the basic/superordinate level category. This task serves to provide additional data to constrain the parameters of the comparison class inference model. }\label{fig:exptOverview}
\end{figure*}


\subsection{Comparison Class Inference (Task 2)}

In this experiment, we measure comparison class inferences by having participants rephrase a speaker's statement which involves a scalar adjective in a way that makes the comparison class explicit.
We measure comparison class inferences using a free-production measure to provide further ecological validity to our measurements of comparison classes: We wish to see if listeners spontaneously adjust their comparison class depending on world knowledge and pragmatic reasoning.
A smaller-scale, forced-choice version of this task was reported in Anonymous (2017). %\citeA{tessler2017warm}.  
Sample size, exclusion criteria, regression analysis, and cognitive model analysis were preregistered: \url{osf.io/xuc96}.

\subsubsection{Participant restrictions and exclusion criteria}

We recruited 837 participants from Amazon's Mechanical Turk. 
Participants were restricted to those with U.S. IP addresses with at least a 95\% work approval rating. 
In addition, participants were required to pass a simple language comprehension test that we designed in order to weed out bots and other bad-faith participants, described above.

In addition to the above restrictions, we excluded participants based on both a task comprehension question appearing before the main trials and a memory check trial appearing after the main trials. 
In the task comprehension / warm-up trial, participants were told they they would be asked to rephrase something a person said:  the person said a word that is relative and their task was to figure out what the word was relative to. They were given the example of \emph{John says: ``The Empire State Building is tall''} and asked to fill-in a sentence with the same kind of response they would do on the main trials (i.e., \emph{The Empire State Building is tall relative to other \_\_\_}). Participants were told to fill in the blank with a group or category that makes the most sense and to use their common sense.
Responses to this warm-up trial were used as a basis for exclusion (any response other than buildings, structures, towers, skyscrapers, or any misspellings thereof). 
Invalid responses to the warm-up trial were most often indicative of copying some part of the text on the screen and pasting it into the response box (e.g., responding with the name of the speaker, just putting the adjective ``tall'', or responding with a whole sentence without a comparison class ``that is tall'').
55 participants were excluded for providing an invalid response to this task comprehension question. 

After the main comparison class inference trials, participants completed a memory check trial asking which adjective--Noun Phrase (NP) combinations appeared on the main trials. 
At the end of the task, participants were asked a memory check question where they had to select, from a list of 10 options, all of the items they could recall seeing. In the memory check, items were shown as adjective -- noun pairs (``tall -- basketball player'') and the 5 distractors were either color or multidimensional adjectives paired with a category that was not used in our test stimuli (e.g., ``green -- tennis ball''; ``beautiful -- painting'').
Participants were excluded if they answered fewer than 7 out of 10 memory check questions correctly.
A total of 59 participants failed this check, though 27 of these also failed the task comprehension check. 

A total of 87 participants were excluded for meeting at least one of these exclusion criteria, leaving a sample of 750 participants for the primary analyses.

\subsubsection{Adaptive data collection procedure}

Our experiment contains 540 unique items (adjective -- NP pairs), which are highly heterogeneous.
In addition to testing the main qualitative predictions of our models (adjective polarity X category expectations interactions), this experiment was designed to elicit high inter-item variability in comparison class inferneces.
As a result, we expect some items to exhibit low intra-item variability  (i.e., all or almost all participants respond with the same comparison class); for these items, we would require relatively fewer data points to estimate the parameter of interest -- the probability of a subordinate~vs.~superordinate comparison class -- since it would either be close to ceiling or close to floor. 

We thus used a sequential sampling method that we deployed on an item-wise basis, wherein we paused data collection after collecting 35 responses for each of the 540 items.
We then analyzed the partial data set on a by-item basis to see which (if any) items had received exceedingly consistent responses, which we defined to be at least 33 out of 35  ($>94\%$ agreement) of the same response. 
For those items that received exceedingly consistent responses, we stopped data collection at these 35 responses. 
We then continued collecting data on the items that received variable responses, until we had data from 750 participants after exclusion criteria were applied.
This procedure allowed us to focus resources on providing better estimates for the items with more intra-item variability in responses. 
This adaptive procedure was decided ahead of time and is documented in the pre-registration report. 

\subsubsection{Response preprocessing}

Our text preprocessing procedure was divided into two stages: Exclusions and Corrections. 

\paragraph{Exclusions}

We excluded responses that did not make sense given the context.
These included but were not limited to responses that were simply the adjectives (e.g., \emph{tall}), the names of the characters (e.g., \emph{Alex}), and copied portions of the text on the screen (e.g., the sentence prompt: \emph{the street is wide}). These invalid responses totaled 443 in total (1.6\%). 

We additionally removed from the data set responses that seemed to be the result of a failure of the participant to grasp the intended referent of the utterance. For example, in the context sentence: ``Alexander is in the forest and hears a woodpecker. Alexander says, \emph{It is loud}'', the intended referent of the pronoun \emph{it} is the woodpecker; however, some participants responded \emph{it is loud relative to other forests}, which suggests they understood the referent to which \emph{it} refers to be the forest. This kind of ``reference failure'' responses numbered 267 in total (or, 1.0\% of the remaining responses). 

\paragraph{Corrections}

In order to improve alignment across responses, we corrected for misspellings  (\rlgetnum{correction_counts.csv}{key}{corrected_spelling}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_spelling}{prop}{1}\%), lemmatized to align plural and singular responses (e.g., \emph{baby} and \emph{babies}; \rlgetnum{correction_counts.csv}{key}{corrected_lemma}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_lemma}{prop}{1}\%), and replaced some responses with obvious synonyms given the context (e.g., \emph{child}--\emph{kid}; \emph{booze}--\emph{alcoholic drink}; \emph{humans}/\emph{individuals}--\emph{people}; \rlgetnum{correction_counts.csv}{key}{corrected_synonym}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_synonym}{prop}{1}\%). 


\subsubsection{Response analysis}

After preprocessing the raw text responses, we preformed an automatic analysis of the comparison class responses by checking whether or not the preprocessed responses contained the subordinate NP presented in the experiment (e.g., \emph{basketball player}) as a substring. 

Pilot testing suggested that participants primarily provide comparison class paraphrases that are identical to the subordinate noun phrase by which the referent is introduced (\emph{subordinate-NP}, e.g., a basketball player) or a more superordinate category (\emph{superordinate-NP}). 
In order to convey a category at least as specific as the subordinate-NP, one must include the subordinate-NP in the response (e.g., \emph{male basketball players} is more specific than \emph{basketball players} and includes the substring \emph{basketball player}).
One exception to this generality is utterances involving negation markers (e.g., ``all kinds of people except basketball players''), but no responses in our data set involved negation in this way.
Thus, we automatically categorized preprocessed responses as either subordinate or superordinate by checking whether the preprocessed response contained the subordinate-NP as a substring.

%\footnote{Some of our pre-designated superordinate-NPs turned out to be incorrect guesses as to what would be the most salient superordinate category in the context. These cases were all very clear, and we replaced our pre-designated superordinate-NPs with the empirical superordinate-NP for purposes of analysis.}


\subsubsection{Supplementary data analysis}

\paragraph{Distribution of response types across experimental factors}

In the main text, we reported that roughly 60\% of responses explicitly mentioned the subordinate category, 25\% of responses explicitly mentioned the modal superordinate category, and 15\% of responses referred to some other, relatively superordinate category. We examine this 3-way classification of responses by the experimental factors that are the focus of this paper: general expectations about the category and the adjective polarity. 
Figure \ref{fig:3wayBars} shows the distribution of response types by general expectations and adjective polarity. Overall, we find a pattern of responses that is consistent with the idea that when the adjective is consistent with the general expectations about the category, a listener will look for a more superordinate comparison class. 

\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{figs/3way_bars_cc_finalExpt.pdf}
\caption{Proportion of responses under a three-way classification: mention subordinate, mention the modal superordinate response, or mention another, relatively superordinate category. For negative polarity adjectives, the proportion of both the ``superordinate'' and ``other'' responses increases as general expectations lower (and thus become more consistent with the adjective polarity; e.g., a short gymnast). For positive polarity adjectives, we see the reverse trend. Error-bars denote bootstrapped 95\% CIs.}\label{fig:3wayBars}
\end{figure}



\paragraph{Regression on subset of responses that explicitly mention subordinate or superordinate-NP}
%
In the main text, we analyze the full data set of preprocessed responses. Here, we report statistics on a subset of the data set in which the participants preprocessed responses explicitly mention either the subordinate-NP (the NP by which the referent was introduced) or the superordinate-NP (an NP that the authors pre-specified as the superordinate-NP for a set of three subordinate-NPs). 
4007 responses (15.2\%) are thus excluded from this analysis. 

The results on the subset of responses that explicitly mention either the subordinate or superordinate-NP are entirely consistent with the results derived from the full data set.
When the subordinate category was expected to be near the high-end of the scale (e.g., basketball player), the positive-form adjective (e.g., tall) led to fewer subordinate comparison classes than the negative-form adjective (e.g., short) in comparison to the control, middle-of-the-scale items (e.g., soccer player): posterior mean beta-weight and 95\% Bayesian credible interval: \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{adjpol_nphigh}.
Again, this interaction was the result of the high-end-of-the-scale subordinate categories showing more subordinate comparison class inferences for the negative adjective (e.g., short) than for the positive adjective (e.g., tall): \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{highneg_highpos}, a preference which was not observed for the middle-of-the-scale, control items (e.g., \emph{soccer players}; \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{adjpol}). 
As before, a comparable interaction was observed for categories that were expected to be near the low-end of the scale (e.g., jockey): Hearing the positive-form adjective led to credibly more subordinate inferences than hearing a negative-form adjective, in comparison to the middle-of-the-scale subordinate categories:  \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{adjpol_nplow}.
Again, this was driven by the behavior of the low-end-of-the-scale subordinate categories, which showed a stronger preference for the superordinate comparison classes for negative than for positive adjectives (\brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{lowneg_lowpos}).

The inferences measured from this reduced data set are similarly highly symmetric across the low-end~vs.~high-end of the scale subordinate categories.
We observe an overall preference for subordinate comparison classes for the control (middle-of-the-scale) subordinate categories (e.g., \emph{soccer players}; \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{intercept}) and no overall differences in this preference for items at the high-end of the scale (e.g., \emph{basketball players}; \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{nphigh}) or the low-end of the scale (e.g., \emph{gymnasts}; \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{nplow}). 
The inferences that result from adjectives that are in conflict with a listener's general expectations of the categories (e.g., tall gymnasts~vs.~short basketball players) were not different between the low-end and high-end items (\brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{lowpos_highneg}), nor were the inferences  from adjectives that were consistent with general expectations about a category (e.g., short gymnasts~vs.~tall basketball players; \brmresults{expt_brm_exact-match_contrasts_10k_2.csv}{lowneg_highpos}).



\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figs/bars_cc_finalExpt_prereg_bars_exactMatch_syncDodge.pdf}
%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
\caption{Comparison Class Inference experimental results removing responses that mention neither subordinate nor superordinate categories. Proportion of paraphrases that contained the Subordinate NPs (e.g., \emph{basketball player}) with which the referent was introduced, as a function of the general expectations listeners have about the category (x-axis) and the polarity of the adjective used to describe the category (e.g., \emph{tall} = positive, \emph{short} = negative). Bars represent overall means and error bars are bootstrapped 95\% confidence intervals. Each dot represents the mean of a single item and lines connect Subordinate NPs described with different adjectives (e.g., \emph{tall} and \emph{short} basketball player). Dots are jittered horizontally to improve visual clarity.}\label{fig:ccInferenceMain2}
\end{figure}


\paragraph{Universally consistent responses}

16 of our 540 items (adjective -- noun pairs) had universally consistent lemmatized responses wherein all participants gave the same response; this number increases to 89 on the restricted data set that only includes responses that match either the subordinate or superordinate responses (described in the previous section). 
Of these universal consistent responses (i.e., responses that were at floor or at ceiling), all but one were cases where 100\% of participants gave a subordinate-NP response (i.e., a response that was the same as the NP with which the referent was introduced). 
For example, 100\% of valid responses for a ``loud church'' all concerned the subordinate category \emph{church}. 
The lone NP-adj pair that led to all superordinate responses (for either data set) was the ``cheap plastic bracelet'', which all participants said was cheap relative to other \emph{bracelets}. 
Intriguingly, the numeric counts of items that received 100\% subordinate-NP responses mirrors the overall pattern of responses collapsed across items shown in the main text: adjectives that conflict with the general expectations about the category (e.g., a short basketball player) are more likely to receive subordinate-NP comparison class responses (Table \ref{tab:counts} for counts of universally consistent responses from the full data set; Table \ref{tab:counts2} for universally consistent responses from the restricted data set).

%\begin{center}
  \begin{table}[h]
  \parbox{.45\linewidth}{
	\centering
%    \centering
    \pgfplotstabletypeset[sci zerofill,
    col sep = comma,
    every head row/.style={before row = \toprule, after row = \midrule},
    every last row/.style={after row = \bottomrule},
    columns/adjpol/.style={string type, column name={Adjective Polarity}, column type = l},
    columns/low/.style={string type, column name={low}, column type = l,  sci sep align, precision=3},
   columns/medium/.style={fixed, column name={medium}, column type = l, dec sep align, precision=4},
    columns/high/.style={string type, column name={high}, column type = l, sci sep align, precision=3}]{csv_data_4_tex/itemCounts_ceil_floor_fulldata.csv}\caption{Counts of items that had 100\% subordinate-NP comparison class responses, as a function of the adjective polarity (positive~vs.~negative) and general expectations about the category (low, medium, high) computed over the full data set.}
    \label{tab:counts}
}
\hfill
\parbox{.45\linewidth}{
%  \end{table}
%\end{center}
%\quad
%\begin{center}
%  \begin{table}[h]
%    \parbox{.45\linewidth}{
    \centering
    \pgfplotstabletypeset[sci zerofill,
    col sep = comma,
    every head row/.style={before row = \toprule, after row = \midrule},
    every last row/.style={after row = \bottomrule},
    columns/adjpol/.style={string type, column name={Adjective Polarity}, column type = l},
    columns/low/.style={string type, column name={low}, column type = l,  sci sep align, precision=3},
   columns/medium/.style={fixed, column name={medium}, column type = l, dec sep align, precision=4},
    columns/high/.style={string type, column name={high}, column type = l, sci sep align, precision=3}]{csv_data_4_tex/itemCounts_ceil_floor_exactmatch.csv}\caption{Counts of items that had 100\% subordinate-NP comparison class responses, as a function of the adjective polarity (positive~vs.~negative) and general expectations about the category (low, medium, high) computed over the restricted data set.}
    \label{tab:counts2}
    }
  \end{table}
%\end{center}


\subsection{Adjective Endorsement Task (Task 3)}
%\subsection{Stimuli Generation Task (Task 1)}

This experiment served to generate further data that could be used to constrain the model parameters shown in Figure \ref{fig:bayesnet}.
Sample size, sampling procedure, exclusion criteria, and data analysis were preregistered: \url{osf.io/vdkbp}.

\subsubsection{Participants}

We recruited 400 participants from Amazon's Mechanical Turk, restricted to those with U.S. IP addresses with at least a 95\% work approval rating.
This number was arrived at with the goal of estimating endorsement probabilities for each item with a 95\% confidence interval width of 0.2 for each item.
Because our measure is a two-alternative forced-choice and because we expect some items to receive fairly categorical judgments (i.e., low between-participant variability), we employed an analogous sequential sampling procedure to that employed in Task 2 (Comparison Class Inference, see above) in order to over-sample items that exhibit high between-participant variability.
We first collected 35 responses for each item, computed 95\% CIs for each item, and stopped collecting data for items whose 95\% CIs were smaller than 0.2.
After the first 35 responses, we collected more responses for each item until the 95\% CI for that item was smaller than 0.2 or we reached our predetermined sample size.
The experiment took about 10 minutes and participants were compensated \$1.50 for their work.

\subsubsection{Procedure}

On each trial, participants were given a sentence introducing a member of a subordinate category (e.g., \emph{You step outside during the winter.}).
This was followed by a question asking if the participant would endorse the positive- and/or negative-form adjective explicitly relative to the superordinate category (e.g., \emph{Do you think it would be warm relative to other days of the year? Do you think it would be cold relative to other days of the year?}).
Both questions appeared on the screen on the same trial.
Participants could respond to each question with either a \emph{yes} or \emph{no} judgment (2 judgments per trial).
Each participant completed 48 items.

As with the comparison class inference experiment, after the main trials, participants completed a memory check trial asking which adjective--NP combinations appeared on the main trials. 
Participants were asked a memory check question where they had to select, from a list of 10 options, all of the items they could recall seeing. In the memory check, items were shown as adjective -- noun pairs (``tall -- basketball player'') and the 5 distractors were either color or multidimensional adjectives paired with a category that was not used in our test stimuli (e.g., ``green -- tennis ball''; ``beautiful -- painting'').
Participants were excluded if they answered fewer than 7 out of 10 memory check questions correctly.
A total of 103 participants failed this check, leaving 297 participants for the main analysis. 

\subsubsection{Materials}

The experimental materials were the same as used in Task 2 (Comparison Class Inference).
%In addition to the sets of noun phrases forming the subordinate categories, a context sentence was crafted to provide an appropriate but minimal context in which the adjective could be uttered.
%In total, we had 126 sets of stimuli (3 subordinates with 1 superordinate), thus this experiment had 378 unique trials (one per subordinate category).
Each trial contained 2 judgments (positive and negative adjectives).

\subsubsection{Results}

The primary goal of this experiment is to validate the stimuli generated in Task 1 and to use to data to constrain the parameters governing world knowledge in the joint Bayesian data-analytic model. We build a Bayesian logistic mixed-effects model to predict participants' responses as a function of the general expectations about the subordinate category (low, medium, high; dummy coded with the medium category as the reference level), the adjective (positive vs. negative; difference coded), and their interaction; in addition, we include the maximal mixed effects structure by-item set and by-participant that mirrors this fixed effects structure.\footnote{
The model is judgment $\sim$ gen\_expectations * adjective\_polarity + (1 + gen\_expectations * adjective\_polarity | participant) + (1 + gen\_expectations * adjective\_polarity | item\_set).
}

%
%As expected by the generating procedure for these stimuli, the only regression coefficients that were credibly different from zero were the interaction terms (Figure \ref{fig:adjEndorseMain}).
%When the subordinate category was expected to be near the middle of the scale (e.g., a soccer player), there was not a credible difference between endorsements of the positive form adjective (e.g., tall) and the negative form adjective (e.g., short): beta-weight and 95\% Bayesian credible interval: \brmresults{expt2_brm_pilot.csv}{adj_positiveness1}.
%Also, there were no credible differences in average endorsements for the adjectives when the subordinate category was either near the high-end of the scale (e.g., basketball players; \brmresults{expt2_brm_pilot.csv}{np_positivenesspositive}) or the low-end of the scale (e.g., gymnasts; \brmresults{expt2_brm_pilot.csv}{np_positivenessnegative}).
%When the subordinate category was expected to be near the high-end of the scale (e.g., the height of a basketball player), endorsements of the positive form adjective (e.g., tall) were credibly higher than endorsements of the negative form adjective (e.g., short) in comparison to middle-of-the-scale subordinate categories (e.g., soccer player):  \brmresults{expt2_brm_pilot.csv}{np_positivenesspositive:adj_positiveness1}.
%The opposite interaction term was also credibly different from zero: When the subordinate category was expected to be near the low-end of the scale (e.g., the height of a gymnast player), endorsements of the positive form adjective were credibly lower than endorsements of the negative form adjective in comparison to middle-of-the-scale subordinate categories:  \brmresults{expt2_brm_pilot.csv}{np_positivenessnegative:adj_positiveness1}.
%This analysis confirms that our experimental items bring to mind the kinds of general expectations that we hypothesize are relevant for our primary comparison class inference hypothesis.

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{figs/bars_adj_finalExpt_prereg_bars_syncDodge.pdf}
\caption{Adjective Endorsement (Task 3) experimental results. Proportion of sentence endorsements as a function of the general expectations listeners have about the category (x-axis) and the polarity of the adjective used to describe the category (colors; e.g., \emph{tall} = positive, \emph{short} = negative). Bars represent overall means and error bar is a bootstrapped 95\% confidence interval. Each dot represents the mean of a single item and lines connect Subordinate NPs described with different adjectives (e.g., \emph{tall} and \emph{short} basketball player). Dots are jittered horizontally to improve visual clarity. The biggest outlier for the low general expectations was the loudness of the \emph{owl}, which was judged to be more loud than quiet; the biggest outlier for the high general expectations was the fabric of \emph{wool}, which was judged to be more light than heavy. }\label{fig:adjEndorseMain}
\end{figure}

The pattern of endorsements was highly consistent with the a priori general expectations by which the stimuli were generated (Figure \ref{fig:adjEndorseMain}).
When the subordinate category was expected to be near the high-end of the scale (e.g., basketball player), the positive-form adjective (e.g., tall) was endorsed much more strongly than the negative adjective (e.g., short) in comparison to the control, middle-of-the-scale items (e.g., soccer player): posterior mean beta-weight and 95\% Bayesian credible interval: \brmresults{expt_brm_adj_contrasts_10k.csv}{adjpol_nphigh}.
This interaction was the result of the high-end-of-the-scale subordinate categories showing more endorsements for the positive (e.g., tall) than for the negative (e.g., short) adjective: \brmresults{expt_brm_adj_contrasts_10k.csv}{highneg_highpos}.
The same endorsement profile was not observed for the middle-of-the-scale, control items (e.g., \emph{soccer players}, for which the positive and negative adjective were endorsed equally as often \brmresults{expt_brm_adj_contrasts_10k.csv}{adjpol}). 
A comparable interaction was observed for categories that were expected to be near the low-end of the scale (e.g., jockey): The positive-form adjective was endorsed credibly less than the negative-form adjective, in comparison to the middle-of-the-scale subordinate categories:  \brmresults{expt_brm_adj_contrasts_10k.csv}{adjpol_nplow}.
Again, this was driven by the behavior of the low-end-of-the-scale subordinate categories, which showed strong endorsements for the negative than for the positive adjectives (\brmresults{expt_brm_adj_contrasts_10k.csv}{lowneg_lowpos}).

The results from the endorsement task were remarkably symmetric across the low-end~vs.~high-end of the scale subordinate categories.
We observe for the control (middle-of-the-scale) subordinate categories (e.g., \emph{soccer players}), a slight preference to reject both adjectives over accepting both (i.e., \emph{neither tall nor short} was a more common pattern than \emph{both tall and short}; \brmresults{expt_brm_adj_contrasts_10k.csv}{intercept}). The mean endorsements, averaging over positive and negative adjectives, for items at the high-end of the scale (e.g., \emph{basketball players}) was not different from chance  (\brmresults{expt_brm_adj_contrasts_10k.csv}{nphigh}) nor were the average endorsements of adjectives for categories at the low-end of the scale (e.g., \emph{gymnasts}; \brmresults{expt_brm_adj_contrasts_10k.csv}{nplow}). 
%We observe no credible difference in the overall preference for subordinate~vs.~superordinate comparison class inferences when the subordinate category was either near the high-end of the scale (e.g., \emph{basketball players}; \brmresults{expt_brm_contrasts_10k.csv}{nphigh}) or the low-end of the scale (e.g., \emph{gymnasts}; \brmresults{expt_brm_contrasts_10k.csv}{nplow}).
Furthermore, the endorsements of adjectives that are in conflict with a listener's general expectations of the categories (e.g., tall gymnasts~vs.~short basketball players) were not different between the low-end and high-end items (\brmresults{expt_brm_adj_contrasts_10k.csv}{lowpos_highneg}), nor were the inferences  from adjectives that were consistent with general expectations about a category (e.g., short gymnasts~vs.~tall basketball players; \brmresults{expt_brm_adj_contrasts_10k.csv}{lowneg_highpos}).








\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/bars_adj_finalExpt_pilot_byItem.pdf}
%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
\caption{Task 3 (Adjective Endorsement) results for a subset of the items.}\label{fig:adjEndorseItems}
\end{figure}





\section{Alternative Models}


The inference about the comparison class outlined in the main text involves a listener reasoning about a speaker reasoning about a listener. The full Rational Speech Act model is given by these equations (reproduced from the main text): 
%
\begin{align}
L_1(x, c \mid u; k) &\propto S_1(u \mid x, c) \cdot P(x \mid k) \cdot P(c \mid k)  \label{eq:L1a} \\ 
S_1(u \mid x, c) &\propto \exp{(\alpha \cdot (\ln L_{0}(x \mid u, c) - \text{cost}(u) ))} \label{eq:S1b} \\
L_{0}(x \mid u, c) &= \int_{\theta} L_{0}(x, \theta \mid u, c) \diff\theta \label{eq:L0_marg} \\ 
L_{0}(x, \theta \mid u, c) &\propto {\delta_{[\![u]\!](x, \theta)} \cdot P(x \mid c)} \cdot P(\theta) \label{eq:L0}
\end{align}



\subsection{Literal alternative model}

One might question whether the inference about the comparison class is necessarily a pragmatic inference which requires recursive reasoning. 
We can investigate this question by reformulating the comparison class inference  spelled out in Equation \ref{eq:L1a} in terms of a literal listener model (\emph{a la} Equation \ref{eq:L0}). 
This literal comparison class inference model is given by:
%
  \begin{align}
L_{0}(x, \theta, c \mid u, k) &\propto \delta_{[\![u]\!](x, \theta)} \cdot P(x \mid c)\cdot P(\theta) \cdot P(c \mid k)  \label{eq:L0alt}
\end{align}
%
Similar to the pragmatic listener model (Eq.~\ref{eq:L1a}), this listener can use their knowledge of the referent $k$ to constrain the hypothesis space of comparison classes (e.g., with the knowledge that the referent is a basketball player, consider comparison classes that the same as or superordinate to the class of basketball players).
Unlike the pragmatic listener model (Eq.~\ref{eq:L1a}), however, the literal listener version of the model does not hold different representations of the referent in mind: The pragmatic listener has their private representation of the referent---given by the prior distribution of the degree $P(x \mid k)$---and imagines a speaker who acts assuming some comparison class---$S_1(u \mid c)$---where $c$ and $k$ may or may not index the same class (e.g., the listener may know the referent is a basketball player -- $k = \emph{basketball players}$ -- but believe the speaker was assuming a \emph{person} comparison class -- $c = \emph{people}$).
The literal listener version of the model has no way of separating these representations.
In effect, this listener is answering a slightly different question from the comparison class inference problem. The question this alternative model is answering is: what is more likely---a basketball player whose height is greater than some threshold or a person whose height is greater than some threshold? 
This alternative model predicts the exact opposite pattern of results from the pragmatic listener model (Figure \ref{fig:altModels}A). 


\begin{figure}[t]
    \centering
    \subfigure{\label{fig:alternativeModelPredictions}\includegraphics[width=0.48\textwidth]{figs/cc_inference_L0.pdf}}
    \subfigure{\label{fig:alternativeModelPredictions2}\includegraphics[width=0.48\textwidth]{figs/cc_inference_L2.pdf}}
    \caption{Alternative model predictions. A: A literal model that does not represent a speaker's representation of the context as separate from their own. This listener effectively answers the question of what is more likely: a basketball player who is tall or a person who is tall? This model predicts the exact opposite pattern of data from the pragmatic model reported in the main text. B: A more sophisticated pragmatic listener model than that reported in the main text. This model reasons about which comparison class a speaker believes that the listener would infer. This model produces the same pattern of results as the simpler pragmatic model reported in the main text.}
    \label{fig:altModels}
\end{figure}


\subsection{Alternative pragmatic model}

The pragmatic comparison class inference listener model (Eq.~\ref{eq:L1a}) reasons about which comparison class a speaker is more likely to be assuming.
That is, the speaker (Eq.~\ref{eq:S1b}) is presumed to be assuming that a particular comparison class is already in the common ground, analogous to a presupposition (e.g., saying ``My car is in the shop'' presupposes that the speaker owns a car). 
Speakers may be aware, however, that the comparison class is not in the common ground; still, they may still avoid articulating a comparison class  (e.g., uttering ``He's short for a basketball player'') if the listener can reasonably be assumed to infer the comparison class. 

This kind of inference is more sophisticated: It involves a listener reasoning about the comparison class that a speaker believed the listener would infer.
As demonstrated above with the Literal Alternative model, this first-order listener inference about the comparison class itself involves pragmatic reasoning. 
This higher-order pragmatic reasoning model is given by the following equations:

\begin{align}
L_2(x, c \mid u, k) &\propto S_2(u \mid x, c) \cdot P(x \mid k) \cdot P(c \mid k) \label{eq:L2} \\
S_2(u \mid x, c) &\propto \exp{(\alpha \cdot (\ln L_{1}(x, c \mid u) - \text{cost}(u) ))}\label{eq:S2} 
\end{align}
%
where $L_1$ is the pragmatic inference model defined above and in the main text (Eq.~\ref{eq:L1a}).
The primary difference between this model and the model given by Eqs.~\ref{eq:L1a} and \ref{eq:S1b} is in the speaker $S_2$ (Eq.~\ref{eq:S2}) (vs.~the $S_1$ shown in Eq.~\ref{eq:S1b}).
$S_2$ chooses their utterance by taking into account the fact that the listener $L_1$ (Eq.~\ref{eq:L1a}) is uncertain about the comparison class. 
As shown in Figure \ref{fig:altModels}B, this more sophisticated pragmatic inference model arrives at the same conclusions about the likely comparison class given different general expectations about the category and the adjective heard.
Further, the inferences of this model, like those of the simpler pragmatics model, are resilient to reasonable choices of alternative utterances; most notably, if the set of alternative utterances provides a way to explicitly articulate the comparison class (e.g., the speaker could have said \emph{They're tall for a basketball player}), the same inferences result from hearing the utterance without a comparison class.

Since this alternative pragmatic model derives the same qualitative inference for the contexts we study about the comparison class as the model presented in the main text, we do not compare the two models in the Quantitative Analysis section. It is likely that the predictions of these two models would come apart under different contextual manipulations of speaker and/or listener knowledge. For example, the model presented in the main text might be better suited for third-party conversations and its architecture could be useful for developing models of word learning from overheard speech \cite{akhtar2001learning, martinez2011orienting}. On the other hand, the more sophisticated pragmatic inference model presented in this supplement might be better suited for dyadic interactions with informed and intentional interlocutors. 
%
%\ndg{suggest situations where these two models might come apart?}
%\ndg{nts: do we compare to this in results? or is it so similar it's not worth it?}
%\mht{paragraph above. okay?}
%
%



% bThe comparison class prior is   a construct of theoretical interest in its own right, and we take the first steps to understand this construct in this analysis. 

\begin{figure}[ht!]
  \begin{center}
    \begin{tabular}{cc}
\begin{tikzpicture}


  % Y
  \node[obs]          (d-c)   {$d^{cci}_{d,k,u,i}$}; %
  \node[obs, left=2.5 of d-c]          (d-a)   {$d^{adj}_{d,k,u,i}$}; %
  \factor[above=of d-a] {da-f} {left:Bern} {} {} ; %
  \factor[above=of d-c] {dc-f} {left:Bern} {} {} ; %

  \node[det, above=of d-c] (L1) {$L_1(c \mid u)$} ; % 
  \node[det, above=of d-a] (S2) {$S_1(u \mid x)$} ; % 
  
  \node[latent, above=of S2]  (x)   {$x_{d, k}$}; %
  \node[latent, above=of L1] (c)   {$c_{d,k}$}; %
   \node[det, above= of c]  (phi) {$\phi_{d,k}$} ; %   

   \node[obs,  above=1 of phi, xshift=-0.5cm] (fhat) {$\hat{f}_k$} ; %
   \node[latent, above=0.9 of phi, xshift=0.5cm] (zi) {$z_{d, k}$} ; %
%   \edge {zi} {c}; %
%   \node[const, right=1.3 of c, yshift=1.5cm] (fshat) {$\hat{f}_s$} ; %
%   \node[latent, right=1.2 of phi, yshift=1.8cm]  (b1) {$\beta^{0}_{0}$} ; %
%   \node[latent, right=1.9 of phi, yshift=1.5cm]  (b2) {$\beta^{0}_{1}$} ; %
%   
%   \node[latent, right=1.2 of phi, yshift=0.2cm]  (b11) {$\beta^{1}_{0}$} ; %
%   \node[latent, right=1.9 of phi, yshift=0.5cm]  (b12) {$\beta^{1}_{1}$} ; %


   \node[latent, right=1.2 of phi, yshift=1.8cm]  (b1) {$\beta^{0}_{0}$} ; %   
   \node[latent, right=1.9 of phi, yshift=1cm]  (b12) {$\beta_{1}$} ; %
   \node[latent, right=1.2 of phi, yshift=0.2cm]  (b11) {$\beta^{1}_{0}$} ; %


%   \factor[above=of phi] {phi-f} {left:logistic} {fhat,b1,b2, b11, b12, zi} {phi}; %
   \factor[above=of phi] {phi-f} {left:logistic} {fhat,b1, b11, b12, zi} {phi}; %
   \factor[above=of c] {c-f} {left:Bern} {phi} {c}; %

  \node[latent, left=1.2 of x, yshift=0.5cm] (mx) {$\mu_{d, k}$} ; %
  \node[latent, left=1.2 of x, yshift=-0.5cm]  (sx) {$\sigma_{d, k}$} ; %
  \factor[left=of x] {x-f} {above:$\mathcal{N}$} {mx,sx} {x} ; %

  % sopt
   \node[latent, right=0.8cm of L1, yshift=0.0cm]         (a1)   {$\alpha_1$}; %
%   \node[latent, right=0.8cm of L1, yshift=2.5cm]         (cost)   {$cost$}; %
   \node[latent, left=2.2cm of S2]         (aa1)   {$\alpha_2$}; %
%   \node[latent, left=2.5cm of S2, yshift=-0.6cm]         (aa2)   {$\alpha^{adj}_2$}; %
 % \node[const, above=of t, xshift=-0.5cm] (at)  {$\alpha_\tau$} ; %
  % \node[const, above=of t, xshift=0.5cm]  (bt)  {$\beta_\tau$} ; %

  % Factors
%  \factor[above=of t] {t-f} {left:$\mathcal{G}$} {at,bt} {t} ; %
\factoredge {L1} {dc-f} {d-c} ; %
\factoredge {S2} {da-f} {d-a} ; %
%\factor       {W'-f}   {Multi} {} {};  %
  % Connect w and x to the dot node
  \edge[-] {c,x} {L1} ;
  \edge[-] {x} {S2} ;
%  \edge[-] {L1} {RSA} ;
%  \edge[-] {S2} {RSA} ;
%  \edge[-] {cost} {L1} ;
%  \edge[-] {cost} {S2} ;
  \edge[-] {a1} {L1} ;
%  \edge[-] {a1} {S2} ;
  \edge[-] {aa1} {S2} ;
%  \edge[-] {aa2} {S2} ;

\gate {RSAgate} {(L1)(S2)(x)(c)(x-f)(c-f)} {} ; %
\node[text width=1cm] at (-3.7,5.6) {$RSA$};
  % Plates
    \plate {dataPlate} {
     (d-c)(d-a)
  } {$i \in \textsc{participants}$} ;
  
  \plate {adjPlate} { %
    (dataPlate)
%     (S2) (L1)
  } {$u \in  \textsc{adjectives}$} ;
  
    \plate {subPlate} { %
  (adjPlate)    (dataPlate)
   (fhat) (c-f) (c)   (x)(x-f)(mx)(sx)(zi)(fhat)
    %(RSA) 
    (S2) (L1) (RSAgate)%%
  } {$k \in  \textsc{categories}$} ;
  
        \plate {degreePlate} { %
 (adjPlate)    (dataPlate)(RSAgate)(subPlate)
  (x)(x-f)(mx)(sx)(zi)(fhat)
  } {$d \in  \textsc{degree scales}$} ;
  

%    \plate {superPlate} { %
%(adjPlate)  (subPlate)(fshat)    (dataPlate)(degreePlate)
%  } {$s \in  \textsc{superordinate categories}$} ;

    
%  \plate {} {%
%    (d-c)(dc-f)(dc-f-caption) %
%    (c)(da-f)(da-f-caption) %
%    (RSA) %
%    (yx.north west)(yx.south west) %
%  } {$M$} ;




%  % Define nodes
%  \node[latent]                             (u) {$u$};
%  \node[latent, above=of u, xshift=-1.2cm] (c) {${c}$};
%  \node[latent, above=of u, xshift=1.2cm]  (x) {${x}$};
%  \node[latent, above=of c, xshift=0cm] (f) {${f}$};
%  \node[latent, above=of x, xshift=0cm] (g) {${g}$};
%
%  % Connect the nodes
%  \edge {c,x} {u} ; %
%  \edge {f} {c} ; %
%  \edge {f,g} {x} ; %

\node[] at (4,6) {$\beta^0_0, \beta^1_0 \sim \text{Gaussian}(0, 2)$};
%\node[] at (4,5.5) {$\beta^0_1, \beta^1_1 \sim \text{Uniform}(-3, 3)$};
\node[] at (4,5.5) {$\beta_1 \sim \text{Uniform}(-3, 3)$};
\node[] at (4,5) {$z_{d, k} \sim \text{Bernoulli(0.5)}$};
\node[] at (4,4.5) {$\mu_{d, k}\sim \text{Uniform(-3, 3)}$};
\node[] at (4,4) {$\sigma_{d, k}\sim \text{Uniform(0, 3)}$};
\node[] at (4,3.5) {$\alpha_1, \alpha_2 \sim \text{Uniform(0, 20)}$};

\node[] at (5,0) {$\phi_{d,k} = \text{logistic} (\begin{cases} 
	\beta^0_0 +\beta^0_1\cdot \hat{f}_k  &\mbox{if } z_{k, d} = 0 \\ 
	\beta^1_0 +\beta^1_1\cdot \hat{f}_k  &\mbox{if } z_{k, d} = 1 
	\end{cases}$)};
\node[] at (4,-1) {$c_{d,k} \sim \text{Bernoulli}(\phi_{d,k})$};
\node[] at (4,-1.5) {$x_{d,k} \sim \text{Gaussian}(\mu_{d,k}, \sigma_{d,k})$};
%(3n +1)/2 & \mbox{if } n \equiv 1 \end{cases} \pmod{2}.


\end{tikzpicture}

    \end{tabular}
  \end{center}
  \caption{\small Joint Bayesian data analytic strategy of the maximal model. Two related RSA models (the comparison class inference model $L_1$ and the adjective endorsement model $S_1$) directly predict the data from Tasks 2 \& 3 ($d^{cci}$ and $d^{adj}$), respectively. Each of these models relies upon world knowledge, which varies by the degree $d$ (e.g., height) and category $k$ (e.g., basketball players)---$P(x^{d,k})$, assumed to be Normal distributions with unknown mean $\mu^d_k$ and variance $\sigma^d_k$). The prior probability of a comparison class $c_{d,k}$ is used only in the $L_1$ model, and is assumed to be a  logistic linear function encoding a basic level bias $\beta_0$ and an effect of frequency of the noun phrase $\hat{f}_k$ given by $\beta_1$. The referent category of each stimulus may be either a subordinate level category or a basic-level category, determined by parameter $z_{d,k}$, which gates between using two different basic-level bias comparison class parameters. Finally, each RSA model has its own speaker optimality parameter $\alpha$.}
  \label{fig:bayesnet}
\end{figure}



%We nd combine it with a basic-level bias encoded as an intercept in a logistic linear model: $P(c) \propto \beta_0 + \beta_1 \cdot \log (\hat{f}_k)$.


%The prior distribution over comparison classes $P(c \mid k)$ reflects listeners' expectations of what comparison classes are likely to be used, given that the referent is a member of a particular subordinate category $k$.
%We simplify the full comparison class inference problem to deal with reasoning about only a specific~vs.~general class.
%This prior probability of these comparison class plausibly includes a basic-level bias \cite{rosch1975family} but could also reflect how frequently these classes are used in conversation.
%\mht{update with final model: inference about sub--basic vs. basic--super item?}


%This \emph{descriptive Bayesian approach} \cite{tauber2017} coupled with explicit models of language understanding allow us to predict and explain broader coverage data sets while still constraining flexibility by forcing models to predict more distinct data points. 

\section{Bayesian Data Analysis}

We generate quantitative predictions for the Comparison Class Inference by constructing a joint Bayesian data-analytic model that synthesizes the data from the Comparison Class Inference experiment (Task 2) and the Adjective Endorsement experiment (Task 3) using Rational Speech Act models to predict the data from each experiment. 
The RSA model for the comparison class inference data is described in the main text of the experiment and is given by Eq.~\ref{eq:L1a}. 

We model the adjective endorsement data using the pragmatic speaker component $S_1$ (Eq.~\ref{eq:S1b}) of the comparison class inference model, with three slight modifications: (1) the comparison class $c$ is assumed to be the superordinate comparison class, since the endorsement task concerns sentences drawing explicit comparisons to a superordinate class, (2) the speaker has two alternative utterances: produce the adjective in question (e.g., tall or short) or stay silent  (following \citeNP{degen2014lost, Tessler2019psychrev}), and (3) the speaker's utility is computed via an expectation about the degree value $x$ for the referent given by its category membership $k$: $x\sim P_k$ (e.g., the speaker does not have a particular height in mind to convey but averages over their expectations about the height of the referent).
The adjective endorsement RSA model we use is given by: 

\begin{equation}
S_{1}(u \mid k) \propto \exp{(\alpha_2 \cdot {\mathbb E}_{x\sim P_k}} \ln{L_0(x \mid u)}) \label{eq:S1a}
\end{equation} 

\noindent where $L_0(x \mid u)$ is Equation \ref{eq:L0} printed above.

Each RSA model has a global speaker optimality parameter $\alpha_{i}$ which determines the degree to which speakers are assumed to be informative.
This parameter is not of direct theoretical interest and we use priors consistent with the previous literature on RSA models: $\alpha_i \sim \text{Uniform}(0, 20)$.

\subsection{Degree priors (World knowledge)}

The two RSA models (Eq.~\ref{eq:L1a} for the Comparison Class Inference task and Eq.~\ref{eq:S1a} for the Adjective Endorsement task) depend upon the same world knowledge priors $P(x \mid k)$ (e.g., expectations about the height of basketball players), which vary as a function of the subordinate category $k$. 
Thus, we put uncertainty over these world knowledge prior distributions and infer the priors that best jointly explain the data from the two tasks. 

Only the relative values for \(P(x \mid c = c_{sub})\) and \(P(x \mid c = c_{super})\) affect model predictions. 
We further assume the degree priors follow Gaussian distributions.
Hence we fix each superordinate distribution to be a standard normal distribution \(P(x \mid c = c_{super}) = \mathcal{N}(0, 1)\) and the subordinate priors are Gaussian distributions with unknown means and variances \(P(x \mid c_{sub}) = \mathcal{N}(\mu_{sub}, \sigma_{sub})\); the subordinate priors thus have standardized units.
%We put priors over these mean and variance parameters and infer their likely values using Bayesian data analysis. 
We put priors over these mean and variance parameters of each subordinate category distribution $k$ for each degree $d$: $\mu^d_k \sim \text{Uniform}(-3, 3)$, $\sigma^d_k \sim \text{Uniform}(0, 3)$, and infer their likely values using Bayesian Data Analysis (left-hand side of Figure \ref{fig:bayesnet}). 

\subsection{Comparison class priors and description of alternative models}

The comparison class prior $P(c)$ reflects listeners' expectations of what comparison classes are likely to be used in conversation. 
Baseline expectations about conceptual comparison classes could be a function of the level of abstraction of the categories in question as well as the usage frequency of the noun phrases used to describe those categories. 
For example, basic-level categories may be more probable conceptual comparison classes because of their utility in everyday reasoning \cite{rosch1975family}; additionally, we might expect the relative probability of basic-level~vs.~subordinate level categories to differ from basic~vs.~superordinate categories.
To investigate these possibilities, we construct and compare models which differ in how the comparison class prior is parameterized.

%We describe a set of alternative models that differ in how they construct this comparison class prior.

\subsubsection{Flat prior model (baseline)}

Our baseline model makes inferences based only on distributional knowledge about scalar properties of categories $P(x \mid k)$ (described above).
For this model, the comparison class prior is uninformed and does not provide an \emph{a priori} preference for the relatively subordinate or relatively superordinate comparison class; thus, all of the by-item variability in comparison class inferences in this model must be explained as a function of the distributional knowledge about properties.

\subsubsection{Basic-level bias}

The \emph{basic-level bias} model assumes some \emph{a priori} preference for the Subordinate or Superordinate comparison classes.
We parameterize the comparison class prior via a logistic model, where a basic-level bias plays the role of an intercept term $\beta_0$: $P(c) = \text{logistic}(\beta_0)$.
Our crowd-sourced stimuli generation procedure (Task 1) presents interesting inferential challenges for this analysis. 
\emph{A priori}, we do not know if the NPs we use to introduce the referents (Referent NPs; those generated by participants in Task 1) are basic-level categories or subordinate level categories, and hence, whether the more general comparison class would correspond to a superordinate-level category or a basic-level category.
A basic-level bias could plausibly operate differently for a subordinate~vs.~basic-level inference than for a basic~vs.~superordinate level inference. 
Specifically, superordinate comparison classes might be the most implausible, because superordinate categories are more heterogenous in comparison to basic-level or subordinate-level categories.
Thus, we endow our data-analytic model with two regression coefficient parameters corresponding to the intercept term (i.e., the basic-level bias term), and introduce a Bernoulli random variable $z$ for each NP to indicate whether it is a subordinate-level term or basic-level term (Figure \ref{fig:bayesnet}).

\subsubsection{Frequency effect}

The \emph{frequency effect} model assumes that the NPs that denote comparison classes which are more common in usage could have a higher \emph{a priori} probability of serving as the comparison class. 
The comparison class prior in this model is thus a (logistic) function of the relative usage frequency $f_k$ of the subordinate NP~vs.~superordinate NP estimated from the Google WebGram corpus\footnote{Corpus accessed via
\url{https://corpora.linguistik.uni-erlangen.de/cgi-bin/demos/Web1T5/Web1T5_freq.perl}}: $P(c) = \text{logistic}(\beta_1\cdot \log (\frac{\hat{f}_{sub}}{\hat{f}_{super}}))$.
We assume a usage-based frequency effect to operate independent of the level-of-abstractness of the category; hence, we do not infer separate $\beta_1$'s for the imputed subordinate~vs.~basic-level categories. 

%We do not infer a separate 

\subsubsection{Full model: Basic-level bias and frequency effect}

To test whether both a basic-level bias and frequency effect jointly contribute to the comparison class prior, our full model assumes both contribute via a logistic-linear model: $P(c) = \text{logistic}(\beta_0 + \beta_1\cdot \log (\frac{\hat{f}_{sub}}{\hat{f}_{super}}))$.

\subsubsection{Priors for comparison class parameters}

The priors over the regression parameters (which also apply to the alternative models described above, if they include that parameter) are $\beta^0_0 , \beta^1_0\sim \text{Gaussian}(0, 2)$, $\beta_1 \sim \text{Uniform}(-3, 3)$. 
The binary level-of-abstractness parameter which governs the direction and magnitude of the basic-level bias comes from a uninformed prior: $z_{k} \sim \text{Bernoulli}(0.5)$. 


%The comparison class prior $P(c \mid k)$ is parameterized by logistic function, which can encode a basic-level bias and an effect of corpus frequency, depending on the model, with the following parameters on the regression coefficients:  $\beta_0 \sim \text{Gaussian}(0, 2)$, $\beta_1 \sim \text{Uniform}(-3, 3)$. 

\begin{figure}[t!]
    \centering
      \subfigure[Human data and model predictions for items with small residuals (left 4) and largest residuals (right 4) for comparison class inference data. Top two rows show data and model predictions for the main Comparison Class Inference experiment; bottom two rows correspond to the Adjective Endorsements (norming study). The log ratio frequency of subordinate NP to superordinate NP is shown in brackets next to the x-axis labels; a more negative number corresponds to a stronger prior belief in the superordinate category as the comparison class.]{\label{fig:residuals}\includegraphics[width=\textwidth]{figs/bars_ccinfOnly_finalExpt_byNP_topResdiuals_intercept_slope_300k.pdf}} \\
    \subfigure[Imputed prior distributions over degrees for 8 of the 90 item sets. These distributions were generated from the Maximum A-Posteriori parameter values inferred by conditioning on the Comparison Class Inference and Adjective Endorsement data sets.]{\label{fig:worldPriors}\includegraphics[width=\textwidth]{figs/reconstructed_world_priors.pdf}} 
    \caption{Quantitative modeling results for eight sets of items which show the lowest and highest residuals for the comparison class inference data.}
    \label{fig:by_item_results}
\end{figure}

\subsection{Model implementation}

We ran four different BDA models, corresponding to the different ways of parameterizing the comparison class prior: (1) Flat Prior model (assumes the comparison class prior is always 50/50 between subordinate and superordinate class); (2) Basic-level bias, or intercept only model (assumes a basic-level bias), (3) Frequency effect, or slope only model (assumes an effect of corpus frequency, but no basic-level bias), (4) Basic-level bias and Frequency effect (slope and intercept).
Figure \ref{fig:bayesnet} shows the plate diagram corresponding the BDA model for the full, slope and intercept model. 
We implemented the RSA and Bayesian data analysis models in the probabilistic programming language WebPPL \cite{dippl} and performed inference by running 7 MCMC chains with 500,000 iterations each, discarding the first 250,000 for burn-in. 
%For the full (slope and intercept) model, we ran additional 5 MCMC chains to be more confident in our conclusions about the parameters. 
Convergence was checked through visual inspection of the different chains to ensure similar conclusions would be drawn from each chain independently. 

We estimated the marginal likelihood of the data under each model by using an Annealed Importance Sampling algorithm implemented in WebPPL \cite{neal2001annealed}. We collected 4 samples from the AIS algorithm, which was run 250,000 steps per sample. 

%where a basic-level bias plays the role of an intercept term $\beta_0$: $P(c) = \text{logistic}(\beta_0)

%The alternative models that do not have both basic-level bias terms or frequency effect terms follow the same parameterization with the parameters of the various lesioned components set to 0. %\footnote{
%	The reader may wonder why we introduce a Bernoulli variable for each NP rather than for each set of 3 NPs (e.g., \emph{basketball players}, \emph{soccer players}, \emph{gymnasts}~vs.~\emph{people}) since that is the format in which we elicited our NPs from participants. We noticed that not all of our item sets were consistent in how salient a common \emph{general} category would be, possibly due to typicality effects (e.g., marshmallows, chocolate, and jolly ranchers are not equally good examples of the general category \emph{candy}).  In addition, since we do not introduce participants to our General NPs (e.g., \emph{people}), it is plausible that different General NPs come to mind given different Specific NPs in the same item set. 



% + \beta_1 \cdot \log (\frac{\hat{f}_{specific}}{\hat{f}_{general}}) )$, where $\hat{f}_k$  represents the frequency of the comparison class  $k$ NP estimated from the Google WebGram corpus, $\beta_1$ is the sensitivity of the comparison class prior to relative frequency, and $\beta_0$ is a basic-level bias.






\subsection{Supplementary model results}

Figure \ref{fig:by_item_results} shows model predictions and inferred parameters for four sets of items which for which the full model does well at predicting the comparison class inference data and four sets of items where the full model does not do well at predicting the comparison class inferences. Predictions in the Comparison Class Inference task (Fig.~\ref{fig:residuals}, top rows) can be triangulated by examining the predictions in the Adjective Endorsement task  (Fig.~\ref{fig:residuals}, bottom rows) and the inferred world knowledge parameters (Fig.~\ref{fig:worldPriors}).

%}




%We assume this distributional knowledge about properties $P(x \mid k)$ (e.g., heights of basketball players) takes the form of Gaussian distributions. Since the judgments we model are binary (Expt.~2: Specific NP vs. General NP; Expt.~3: binary felicity judgments), we assign relativized units to the world knowledge distributions. We fix the General category distribution to be a unit-normal distribution and infer the parameters of the Specific category distribution. 


% and a set of alternative models which differ in how the comparison class prior $P(c \mid k)$ is parameterized. 



%As a proxy for comparison class usage frequency, we use empirical frequency \(\hat{f}\) estimated from the Google WebGram corpus\footnote{Corpus accessed via
%\url{https://corpora.linguistik.uni-erlangen.de/cgi-bin/demos/Web1T5/Web1T5_freq.perl}. 
%Due to potential polysemy and idiosyncracies of our experimental
%  materials (Table 1), we made the following substitutions when querying
%  the database for empirical frequency: produce $\rightarrow$
%  ``fruits and vegetables''; things you watch online
%  $\rightarrow$ ``online videos''; days in \{season\}
%  $\rightarrow$ ``\{season\} days''; dishwashers $\rightarrow$
%  ``dishwashing machines''; videos of cute animals $\rightarrow$
%  ``animal videos''.}, and scale it by a free parameter $\beta$ such that $P(c) \propto \exp{(\beta \cdot \log \hat{f})}$.
  
  


% the data from the Comparison Class Inference experiment and 

%\subsection{Model overview}
%
%
%The variability across items in comparison class inferences that cannot be explained by distributional knowledge about properties could be attributed to baseline expectations about what comparison classes are likely to be used: the comparison class prior $P(c \mid k)$. 
%
%% one in which the inference is influenced by the frequency of the noun phrase, and one in which both frequency and a basic-level bias have an influence (maximal model). 
%We infer all model parameters using a Bayesian data analytic model that infers the world knowledge shared between these two tasks (comparison class inference and adjective endorsements) along with the comparison class prior parameters (which depend on the model variant) and the speaker optimality free parameters of the RSA models . 


%\subsection{Model details}
%
%
%
%%The RSA listener (Eq. \ref{eq:L1a}) and speaker (Eq. \ref{eq:S2}) models make quantitative predictions about comparison class interpretation and adjective endorsement, respectively.
%We construct data-analytic models with both the comparison class inference and adjective endorsement RSA components as sub-models that make quantitative predictions about the data from both of these experiments (Figure~\ref{fig:bayesnet}).
%The inference and endorsement sub-models share their prior world knowledge $P(x \mid k)$ (e.g., heights of basketball players), which we infer from both data sets.
%
%The comparison class prior $P(c \mid k)$ is parameterized by logistic function, which can encode a basic-level bias and an effect of corpus frequency, depending on the model, with the following parameters on the regression coefficients:  $\beta_0 \sim \text{Gaussian}(0, 2)$, $\beta_1 \sim \text{Uniform}(-3, 3)$. 
%Each RSA model has an additional speaker optimality parameter $\alpha_{i}$ which determines the degree to which speakers are assumed to be informative and which is not of direct theoretical interest.
%We use priors consistent with the previous literature: $\alpha_i \sim \text{Uniform}(0, 20)$.
%

%The comparison class inference model has one speaker optimality: $\alpha^\text{1}_{1}$.
%The adjective endorsement model has two speaker optimality parameters: 
%$\{\alpha^\text{2}_{1}, \alpha^\text{2}_{2}\}$.


%\subsection{Model results}
%
%We examined the posterior distributions over parameters and posterior predictive distributions separately by chain to confirm that the model results were consistent across runs of MCMC chains, which they were. 
%Our BDA model returns posterior distributions over the parameters of the RSA models, governing world knowledge, the comparison class prior, and the speaker optimality parameters. We first determine which parameterization of the comparison class prior (flat, basic-level bias, frequency, basic-level bias and frequency) is the best for our comparison class inference RSA model to predict the comparison class inference data. 
%We do so by examining the model's posterior predictive distribution over comparison class inference choices.
%The posterior predictive distribution marginalizes over the inferred values of the parameters to show what data the model would expect to see, given the parameters it has learned from the data. 
%Posterior predictive checks are an important step in model validation and provide a window into the model's strengths and shortcomings. 
%and (2) comparing the marginal likelihood of the data under each model to compute a Bayes Factor (BF) as a measure of formal model comparison. BFs quantify how well the model predicts the data, averaging over the prior distribution over parameters; by taking the average over the model's prior distribution over parameters, the measure explicitly takes into account model complexity because higher complexity models have wider prior distributions over the parameter \cite{lee2014bayesian}.


%All models do a good job at accommodating the Adjective Endorsement data (Figure \ref{fig:scatters}, bottom row).
%The BDA models adjust the parameters of the world knowledge priors used in RSA in such a way as to make the adjectives felicitous for the categories in question; in this way, the adjective endorsement task (Expt.~3) serves as a norming data set. 
%This result is a good sanity check; it shows that the adjective endorsement data is directly constraining the parameters of the world knowledge priors. 
%We see this reflected in the imputed world knowledge priors, which reflect intuitively accurate general expectations about the categories (Figure \ref{fig:worldPriors}). 

%\begin{figure}[t!]
%\centering
%\includegraphics[width=\textwidth]{figs/reconstructed_world_priors.pdf}
%%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
%\caption{xxx.}\label{fig:worldPriors}
%\end{figure}


%\begin{figure}[t!]
%    \centering
%    \subfigure[Imputed prior distributions over degrees for 8 of the 90 item sets. These distributions were generated from the Maximum A-Posteriori parameter values inferred by conditioning on the Comparison Class Inference (Expt.~2) and Adjective Endorsement (Expt.~3)  data sets. See Figure \ref{fig:bayesnet} for full data-analytic model.]{\label{fig:worldPriors}\includegraphics[width=\textwidth]{figs/reconstructed_world_priors.pdf}} \\
%    \subfigure[Human data and model predictions for items with small residuals (left 4) and largest residuals (right 4). ]{\label{fig:residuals}\includegraphics[width=\textwidth]{figs/bars_ccinfOnly_finalExpt_byNP_topResdiuals_intercept_slope_300k.pdf}}
%    \caption{Quantitative modeling results.}
%\end{figure}


%\begin{figure}[t!]
%\centering
%\includegraphics[width=\textwidth]{figs/bars_ccinfOnly_finalExpt_byNP_topResdiuals_intercept_slope_300k.pdf}
%%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
%\caption{Human data and model predictions for items with small residuals (left 4) and largest residuals (right 4).}\label{fig:residuals}
%\end{figure}

%The predictions of the different model variants come apart for the Comparison Class Inference data (Expt.~2; Figure \ref{fig:scatters}). 
%The baseline Flat Prior model predicts variability in comparison class inferences only as a function of world knowledge about the properties, assuming all comparison classes are equally likely \emph{a priori}; this model explains roughly 17\% of the variance.
%The Frequency effect model assumes NPs with higher usage frequency will more likely be used as comparison classes and is able to explain roughly 52\% of the variance.
%The Basic-level bias model explains roughly 73\% of the variance by assuming that basic-level comparison classes may be more likely \emph{a priori} and segregates the set of stimuli into those for which it believes the basic-level category is the Specific NP or those for which the basic-level category is the unmentioned, more general category. 
%Finally, the maximally parameterized \emph{Basic-level and Frequency effect} -- a combination of the two alternative models -- gives rise to the best model predictions in terms of variance explained (78\%) and mean squared error (Table \ref{tab:r2bf}), suggesting that the structure of the comparison class reflects both a basic-level bias and an effect of the frequency of the NP, in addition to other possible factors. 

%The parameterization of the comparison class prior does not directly affect the model's predictions for the Adjective Endorsement (Expt.~3) data; these predictions are purely a function of the RSA model (see Supplement) and the distributional world knowledge used for both experiments (e.g., the distributions of heights for people). Thus, it is no surprise that all model variants do a good job at explaining the Adjective Endorsement data, and it is a good sanity check that the adjective endorsement RSA model is of the right form to accommodate the Adjective Endorsement data. 
%Our full model does a quite good job at accounting for the gradability of these inferences ($r = 0.88; r^2 = 0.77$)

%\begin{center}
%  \begin{table}[h]
%    \centering
%    \pgfplotstabletypeset[sci zerofill,
%    col sep = comma,
%    every head row/.style={before row = \toprule, after row = \midrule},
%    every last row/.style={after row = \bottomrule},
%    columns/model_variant/.style={string type, column name={Model}, column type = l},
%    columns/subordinateCC_r2/.style={string type, column name={$r^2$ Expt.~2}, column type = l,  sci sep align, precision=3},
%   columns/subordinateCC_mse/.style={fixed, column name={$MSE$ Expt.~2}, column type = l, dec sep align, precision=4},
%    columns/adjEndorse_r2/.style={string type, column name={$r^2$ Expt.~3}, column type = l, sci sep align, precision=3},
%     columns/adjEndorse_mse/.style={fixed, column name={$MSE$ Expt.~3}, column type = l, dec sep align, precision=4}]{csv_data_4_tex/mse_r2_table.csv}\caption{Model evaluation results. Full basic-level and frequency model exhibits the best fit to both data sets in terms of variance explained $(r^2)$ and mean squared error (MSE).}
%    \label{tab:r2bf}
%  \end{table}
%\end{center}




%To gain further insight into the comparison class inference results, we examine the full model's posterior distribution over parameters.
%We start with the global parameters. As a sanity check, the speaker optimality parameters for each task were inferred to be values for which listeners assuming a rational speaker and consistent with the prior literature on RSA models --- means and 95\% Bayesian credible intervals: $\alpha_1 =  \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_speakerOptimality_subordinateCC_NA}$, $\alpha_2 =  \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_speakerOptimality_adjEndorse_NA}$.
%We also find a positive effect of frequency of the noun phrase -- $\beta_1 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_frequency_basic_super}$.
%
%The full model infers on a by-item basis whether the basic-level category is the supplied NP or whether the basic-level category is an unmentioned, more superordinate category, because we expect the basic-level bias to operate differently for these different regimes of items.
%As expected, the comparison class prior NPs that were categorized as basic-level categories strongly favored those mentioned categories -- $\beta^1_0 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_basic_super}$.
%For NPs that were categorized as subordinate categories (i.e., the basic-level category was an unmentioned, more superordinate category), the comparison class prior showed no appreciable basic-level bias -- $\beta^0_0 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_sub_basic}$.
%From these parameters values, we can compute the overall imputed prior probabilities of subordinate, basic, and superordinate comparison classes (assuming a constant effect of usage frequency), which reflect the finding that basic and subordinate comparison classes are highly accessible and superordinate comparison classes less so (Figure \ref{fig:parameters}).
%%and a positive effect of frequency of the noun phrase -- $\beta^0_1 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_frequency_sub_basic}$.
%


%\begin{figure}[t]
%\centering
%\includegraphics[width=0.7\textwidth]{figs/model_comparisonClassPriorParameters.pdf}
%\caption{Imputed distributions over the prior probabilities of comparison classes at different levels of abstraction. Basic- and subordinate-level categories comprise \emph{a priori} likely comparison classes, while superordinate categories are less likely to serve as comparison classes.}\label{fig:parameters}
%\end{figure}
%
%Though our fully parameterized model does the best job at accounting for the variability in comparison class inferences, the data set exhibits even more variability than our model can account for. Figure \ref{fig:residuals} shows the four item sets with largest mean model residuals (as well as the four item sets with smallest mean model residuals). 



%
%\section{Bayesian Data Analysis}
%
%The model's quantitative predictions can be generated by explicitly specifying the interlocutors' relevant prior knowledge (e.g., beliefs about temperatures). 
%The current methodological standard is to measure prior beliefs empirically by creating tasks for participants to estimate quantities or give likelihood judgments \cite{Franke2016}. 
%We pursue a different methodology. 
%The RSA model captures a productive fragment of natural language; thus, it makes predictions about a related natural language task.
%Critically, we can use the model to predict natural language judgments that require the \emph{same prior knowledge} as in Expt. 1 and use Bayesian data analysis to jointly infer the shared priors. 
%This approach harnesses the productivity of language into experiment design and allows us to reconstruct priors without having participants engage in challenging numerical estimation tasks.
%
%\subsection{Overview of data analytic approach}
%
%The behavioral experiment bore out our two qualitative predictions
%derived from the comparison class inference model. The model is a
%quantitative model, however, and can make quantitative predictions
%concerning the strength of comparison class inferences. Indeed, we
%observe substantial variability in the predicted inferences both within-
%and across- scales. Here we test whether the observed variability in
%inferences can be accounted for by the constructs posited in our model.
%We describe the two relevant constructs---the comparison class prior
%\(P(c)\) and the degree priors \(P(x \mid c)\)---before outlining our
%Bayesian data analytic strategy.
%
%\subsubsection{Comparison class prior}
%
%\(P(c)\) reflects listeners' expectations of what classes are likely to be discussed. As a proxy for comparison class usage frequency, we use empirical frequency \(\hat{f}\) estimated from the Google WebGram corpus\footnote{Corpus accessed via
%\url{https://corpora.linguistik.uni-erlangen.de/cgi-bin/demos/Web1T5/Web1T5_freq.perl}. 
%Due to potential polysemy and idiosyncracies of our experimental
%  materials (Table 1), we made the following substitutions when querying
%  the database for empirical frequency: produce $\rightarrow$
%  ``fruits and vegetables''; things you watch online
%  $\rightarrow$ ``online videos''; days in \{season\}
%  $\rightarrow$ ``\{season\} days''; dishwashers $\rightarrow$
%  ``dishwashing machines''; videos of cute animals $\rightarrow$
%  ``animal videos''.}, and scale it by a free parameter $\beta$ such that $P(c) \propto \exp{(\beta \cdot \log \hat{f})}$.
%
%\subsubsection{Degree priors (World knowledge)}
%
%Only the relative values for \(P(x \mid c_{sub})\) and \(P(x \mid c_{super})\) affect model predictions. 
%Hence we fix each superordinate distribution to be a standard normal distribution \(P(x \mid c_{super}) = \mathcal{N}(0, 1)\) and the subordinate priors to also be Gaussian distributions \(P(x \mid c_{sub}) = \mathcal{N}(\mu_{sub}, \sigma_{sub})\); the subordinate priors thus have standardized units.
%
%Specifying the relevant prior knowledge yields two free parameters per subordinate class. We put priors over these parameters and infer their likely values using Bayesian data analysis. The data from the comparison class inference experiment would be insufficient, however, to reliably estimate all of the parameters of this data analytic model. To alleviate this, we use the same RSA model to predict additional data about related language used in the same domains. Specifically, we gather judgments about adjectives when the comparison class is explicit (\(n = 100\)): whether or not an adjective would apply to a subordinate member explicitly relative to the superordinate category (e.g., Is a day in winter warm relative to other days of the year?).\footnote{The judgments in this experiment were consistent with the \emph{a priori} ordering of the subordinate categories on the degree scale (e.g., a day in winter is likely to be cold for the year).}
%
%To model this adjective endorsement data, we remove comparison class uncertainty by setting \(P(c_{super}) = 1\), since the sentence being endorsed include an explicit comparison to the superordinate class. We model sentence endorsement using a pragmatic speaker (following Qing \&Franke, 2014a; Tessler \& Goodman, 2016a, 2016b):
%
%\begin{equation}
%S_{2}(u \mid c_{sub}) \propto \exp{(\alpha_2 \cdot {\mathbb E}_{x\sim P_{c_{sub}}} \ln{L_1(x \mid u)})} \label{eq:S2a}
%\end{equation} 
%
%\noindent Note that $L_1(x \mid u)$ is defined from Eq. \ref{eq:L1a} by marginalization.
%
%Eqs. \ref{eq:L1a} and \ref{eq:S2} define models for the comparison class inference task and adjective endorsement task, respectively, and depend on the same background knowledge \(P(x\mid c)\). We can thus use data from both experiments to jointly reconstruct the shared prior knowledge and generate predictions for the two data sets.
%
%\section{Full model analysis and results}
%
%The RSA listener (Eq. \ref{eq:L1a}) and speaker (Eq. \ref{eq:S2}) models make quantitative predictions about comparison class interpretation and adjective endorsement, respectively. We construct a single data-analytic model with each of these RSA components as sub-models in order to make quantitative predictions about the data from both of our experiments.
%
%The listener and speaker sub-models share their prior world knowledge \(P(x \mid c)\) (e.g., temperatures in winter), described in the \textbf{Degree Priors} section. We put the same priors over the parameters of each subordinate distribution:
%\(\mu \sim \text{Uniform}(-3, 3)\), \(\sigma \sim \text{Uniform}(0, 5)\), since they have standardized units. The comparison class prior \(P(c)\) in Eq. \ref{eq:L1a} scales the empirical frequency \(\hat{f}\) by a free parameter, which we give the following prior: \(\beta \sim \text{Uniform}(0, 3)\).
%
%The full model has three additional parameters not of direct theoretical interest: the speaker optimality parameters \(\alpha^\text{expt}_{i}\), which can vary across the two tasks. The pragmatic listener \(L_1\) model (Eq. \ref{eq:L1a}) has one speaker optimality: \(\alpha^\text{1}_{1}\). The pragmatic speaker \(S_2\) model (Eq. \ref{eq:S2}) has two speaker optimality parameters: \(\{\alpha^\text{2}_{1}, \alpha^\text{2}_{2}\}\). We use priors consistent with the previous literature: \(\alpha_1 \sim \text{Uniform}(0, 20)\), \(\alpha_2 \sim \text{Uniform}(0, 5)\)
%
%We implemented the RSA and Bayesian data analysis models in the probabilistic programming language WebPPL (Goodman \& Stuhlmuller, 2014). To learn about the credible values of the parameters, we collected 2 chains of 50k iterations (after 25k burn-in) using an incrementalized version of MCMC (Ritchie, Stuhlmuller, \& Goodman, 2016).
%
%\subsubsection{Results}
%
%The full model's posterior over the RSA and data-analytic parameters were consistent with prior literature and intuition. The maximum a-posteriori (MAP) estimate and 95\% highest probability density (HPD) intervals for model parameters specific to the \(L_1\) model used for comparison class inference were \(\alpha^{1}_{1} = 1.60 [1.10, 2.50]\),
%\(\beta = 0.13 [0.11, 0.19]\). Model parameters specific to the \(S_2\) model used for adjective endorsement:
%\(\alpha^{2}_{1} = 3.50 [0.60, 13.20]\), \(\alpha^{2}_{2} = 3.20 [2.60, 3.80]\). The inferred distributions corresponding to subordinate class priors were consistent with the \emph{a priori} ordering of these subordinate classes (low, medium, high) used in these tasks (Figure \ref{fig:modelParameters} top).
%
%Finally, the full model's posterior predictive distribution does an excellent job at capturing the quantitative variability in comparison class inferences: \(r^2(30) = 0.96\), and adjective endorsements: \(r^2(30) = 0.98\) (Figure \ref{fig:posteriorPredictiveScatters}). Because of the overall preference for the subordinate comparison class, many of the data points are distributed above 0.5. Even for these fine-grained differences, the model does a good job at explaining the quantitative variability in participants' data (Figure \ref{fig:posteriorPredictiveScatters} right). Thus, the variability in comparison class inferences we observe in our behavioral data can be accounted for the constructs posited in our model (namely, the comparison class prior and degree priors).
%
%\subsection{Fully Bayesian analysis of Bayesian language
%models}
%
%The second contribution of this paper is a novel data-analytic approach,
%where prior knowledge used in the Bayesian language model is
%reconstructed from converging evidence gathered from a related language
%experiment, also explicitly modeled using a language understanding
%model. In previous work, we have attempted to measure prior knowledge by
%decomposing what would be a single, implicitly multilayered, numerical
%estimation question into multiple simpler questions. Then, we construct
%a Bayesian data analytic model to back out the prior knowledge (Tessler
%\& Goodman, 2016a, 2016b). We extend this approach by using the same
%core RSA model to model behavior across two language experiments. The
%major feature of this method is that participants respond only to
%simple, natural language questions rather than estimating numerical
%quantities for which complicated linking functions must be designed
%(e.g., Franke et al., 2016). The fully Bayesian language approach we
%pioneer here also provides a further constraint on the language model,
%which must predict data from two similar but distinct language
%experiments. The productivity of natural language can thus be harnessed
%to productively design experiments that further constrain and test
%computational models of language and cognition.
%

\bibliographystyle{apacite}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliography{comparison-class}

\end{document}
