\documentclass[man, floatsintext]{apa6}
\usepackage[american]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{tikz}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{subfigure}
\usetikzlibrary{bayesnet}
\usepackage{bm}
\usepackage{cprotect}
\lstset{
    language=C,
    basicstyle=\small
}


%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\DeclareLanguageMapping{american}{american-apa}

%  \title{Warm (for Winter): Inferring reference classes in communication}
    \title{Warm (for Winter): Inferring comparison classes in communication}

  \shorttitle{Inferring comparison classes}
     \author{Michael Henry Tessler\textsuperscript{1,2} and Noah D. Goodman\textsuperscript{2,3}}
%    \author{Anonymous}
    \date{}

\shorttitle{ Inferring comparison classes}
\affiliation{
%\vspace{0.5cm}

\textsuperscript{1}Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology \\\textsuperscript{2}Department of Psychology, Stanford University
\\\textsuperscript{3}Department of Computer Science, Stanford University}
%\textsuperscript{1}Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology \\\textsuperscript{2}Department of Psychology, Stanford University}
\keywords{comparison class; reference class; adjectives; pragmatics; Rational Speech Act; Bayesian cognitive model; Bayesian data analysis\newline\indent Word count: 7367}
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage{tikz}
\usepackage{caption}
\usepackage{booktabs}


% these packages are needed to insert results
% obtained from R into the LaTeX document
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{siunitx}

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\red}[1]{\textcolor{Red}{#1}}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

% set the name of the folder in which the CSV files with
% information from R is stored
\newcommand{\datafoldername}{csv_data_4_tex}

% the following code defines the convenience functions
% as described in the main text below

% rlgetvalue returns whatever is the in cell of the CSV file
% be it string or number; it does not format anything
\newcommand{\rlgetvalue}[4]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\myvalue}}

% rlgetvariable is a shortcut for a specific CSV file (myvars.csv) in which
% individual variables that do not belong to a larger chunk can be stored
\newcommand{\rlgetvariable}[1]{\csvreader[]{\datafoldername/myvars.csv}{#1=\myvar}{\myvar}\xspace}

% rlnum format a decimal number
\newcommand{\rlnum}[2]{\num[output-decimal-marker={.},
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlnumsci}[2]{\num[output-decimal-marker={.},
                          scientific-notation = true,
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlgetnum}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnum{\myvalue}{#5}}}

\newcommand{\rlgetnumsci}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnumsci{\myvalue}{#5}}}

\newcommand{\lmresults}[2]{\(\beta = \rlgetnum{#1}{Rowname}{#2}{Estimate}{3}\), t\((\rlgetnum{#1}{Rowname}{#2}{df}{0}) = \rlgetnum{#1}{Rowname}{#2}{t.value}{2}, p = \rlgetnum{#1}{Rowname}{#2}{Pr...t..}{3}\)}

\newcommand{\brmresults}[2]{\(\beta = \rlgetnum{#1}{key}{#2}{mean}{2}\) [\rlgetnum{#1}{key}{#2}{l95}{2}, \rlgetnum{#1}{key}{#2}{u95}{2}]}

\newcommand{\hdiresults}[2]{\rlgetnum{#1}{param_name}{#2}{MAP}{2}  (\rlgetnum{#1}{param_name}{#2}{cred_lower}{2}, \rlgetnum{#1}{param_name}{#2}{cred_upper}{2}) }

%\authornote{\emph{Manuscript under review.} %Please do not copy or cite without author's permission. 
%Correspondence concerning this article should be addressed to Michael Henry Tessler, Department of Brain and Cognitive Sciences, Building 46, Room 3027, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139.
%E-mail: tessler@mit.edu}

\authornote{Correspondence concerning this article should be addressed to Michael Henry Tessler, Department of Brain and Cognitive Sciences, Building 46, Room 3027, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139.
E-mail: tessler@mit.edu}

\abstract{
The meanings of natural language utterances are highly context dependent. 
Yet, what counts as context is often only implicit in conversation. The utterance \emph{itâ€™s warm outside} signals that the temperature outside is relatively high, but the temperature could be high relative to a number of different \emph{comparison classes}: other days of the year, other weeks, other seasons, etc. Theories of context-sensitivity in language agree that the comparison class is a crucial variable for understanding meaning, but little is known about how a listener decides upon a comparison class.
%Context is central to understanding the world, but what counts as context is often only implicit in everyday conversation.
%Knowledge about the world is structured into categories which enable predictions about future events or individuals. 
%But attempting to make a prediction about an individual introduces a \emph{reference class problem}: Any individual can belong to multiple categories, each of which could lead to a different prediction.
%How do we decide upon an appropriate reference class? 
%We explore this problem in natural language, where the reference or comparison class problem manifests in a wide variety of context-sensitive linguistic expressions. 
% The problem manifests in natural language via context-sensitivity, where there are many ways of deciding what counts as the appropriate context. 
Using the case study of gradable adjectives (e.g., \emph{warm}), we extend a Bayesian model of pragmatic inference to reason flexibly about the comparison class and test its qualitative predictions in a large-scale free-production experiment.
We find that human listeners infer the comparison class by reasoning about the kinds of observations that would be remarkable enough for a speaker to mention, given their shared knowledge of the world.   
We further quantitatively synthesize the model and data using Bayesian data analysis, which reveals that usage frequency and a preference for basic-level categories are two main factors in comparison class inference.
 This work presents new data and reveals the mechanisms by which human listeners recover the relevant aspects of context when understanding language. 
%The methods and results we present open the door to studying implicit aspects of ordinal language use. 
% The reference class problem manifests in natural language when interpreting context-sensitive 
% The problem is well known in philosophy, law, and linguistics 
%The meaning of an utterance can change depending on the context. Yet,
%what counts as context is often only implicit in everyday
%conversation. The utterance ``it's warm outside'' signals that the temperature outside is relatively high, but the temperature could be high relative to a number of different \emph{comparison classes}: other days of the year, other weeks, other seasons, etc.
%Theories of context-sensitive language use agree that the comparison class is a crucial feature of meaning understanding, but little is known about how a listener decides upon a comparison class.
% We extend a Bayesian model of pragmatic reasoning to be able to reason flexibly about the comparison class intended by the speaker and test the qualitative predictions of this model using a large-scale free-production experiment.
%We then quantitatively synthesize the model and data using Bayesian data analysis, which further reveals  that usage frequency and a preference for basic-level categories are two main contributors to comparison class inference.
%We introduce a hybrid experimental-Bayesian data analytic approach to examine the quantitative predictions of the model, finding%We present free-production and forced-choice experiments showing reliable patterns of comparison class inference.
%The patterns of inference we observe are consistent with a model of Bayesian reasoning about the likely comparison class, which we incorporate into a probabilistic model of adjective interpretation, furthering the breadth of computational models of language understanding.
%The methods and results we present open the door to studying richer aspects of context-sensitive language understanding.
%We test the qualitative predictions of the model in a free production experiment and use a forced-choice version of the task to test the finer-grained, quantitative predictions.
%The resolution of a comparison class requires not only reasoning about what is likely to be the case but also what would be informative to talk about, thus incorporating comparison class inference into the larger study of pragmatic reasoning.
}



\begin{document}
 \maketitle



\section{Introduction}

%A man with a height of 5'2'' is short.
%A 6'1'' man is not short; that is, unless the man is a basketball player; they could be short for a basketball player.


A 75\degree F (24\degree C) day is warm. A 50\degree F (10\degree C) day is not. That is, unless it is Winter; 50\degree F (10\degree C)  could be warm for Winter. 
Whether or not something is \emph{warm} depends upon what \emph{reference class} you consider: a warm Winter day is different than just a warm day of the year.
Deciding upon a \emph{reference class} is an open-ended problem, articulated  most explicitly in the philosophy of probability when one tries to compute a probability for a singular future event \cite{Reichenbach1949, hajek2007reference}: Any singular event or entity belongs to multiple categories, giving rise to multiple possible reference classes. 
For example, to calculate the probability that Joe Biden will be (re-)elected President of the United States in 2024, one would ideally construct a set of similar past events and compute the frequency of successful outcomes (e.g., re-elections) relative to the total number of outcomes (number of elections); but, what ``similar'' past events should be considered?
There is the set of all elections involving older white males at the state and federal level in Western democracies between the years 1890-2020, the set of elections of US Presidents born in Pennsylvania, the set of elections of heads-of-state who have governed during global pandemics, among countless other possibilities. 
%this set of other similar events could be constructed in multiple ways, by appealing to Joe's age, his gender, his height, how many push-ups he can do, his home state of Pennsylvania, any number of events in his life history among a multitude of other factors.
Reference classes are deployed in contexts as diverse as providing a price valuation for a house (e.g., by appealing to the prices of ``similar'' houses) to informing the severity of sentences for defendants convicted in the legal system \cite<e.g., by appealing to sentences of ``similar'' convictions;>{colyvan2001crime, cheng2009practical}.
Even in ordinary language, the reference class problem is present (e.g., what do you mean by \emph{warm}?), and yet human speakers and listeners seem to fill in this critical aspect of context with hardly a notice. 

%hardly notice when they 
% this problem effortlessly. 

%
% \cite<e.g., the probability that Joe Biden will be re-elected president in 2024>: What is the set of relevant information that should inform such a prediction?
%Any particular entity or event belongs to multiple categories (e.g., Joe Biden is the head-of-state of a major Western democracy as well as an older white male, 6'0'' tall, from Scranton, Pennsylvania, who can do some number of push-ups, among many other things), giving rise to multiple possible reference classes (e.g., should we consider his home state of Pennsylvania when evaluating Biden's probability of being re-elected president?).



%. \ndg{it'd be good to state the problem clearly here.}
% which could imply different probabilities, price values, or even time spent in jail.
% Determining the appropriate reference class to bring to mind is a general problem that touches on issues in philosophy, language, and the law.


% The problem of determining the comparison class is a special case of the \emph{reference class problem}, originally developed in the domain of probability and statistics 

% though speakers rarely articulate comparison classes explicitly.
%The problem extends beyond both probability and language to touch on broad societal issues ranging from providing a price valuation for a house (e.g., by appealing to the values of ``similar'' houses) to informing the severity of sentences for defendants convicted in the legal system \cite<e.g., by appealing to ``similar'' convictions;>{colyvan2001crime, cheng2009practical}.
% \ndg{given that this is the first paragraph, it could use some work to make the key idea pop out. what is a reference class, why do we care, how do linguistic comparison classes form a special case?}

In natural language, the problem of reference or comparison classes\footnote{
The terms \emph{reference classes} and \emph{comparison classes} seemingly have independent lives in independent literatures, but they are extremely related concepts. Given that our work uses language as a way to gain insight into the reference class problem, we use the terminology \emph{comparison classes} -- familiar to the linguistics literature -- throughout.} is front and center when interpreting \emph{relative} statements (e.g., \emph{warm relative to what?}).
The problem thus affects a wide swath of linguistic expressions, including adjectives \cite<\emph{warm}, \emph{tall};>{bartsch1974semantic, cresswell1976semantics, klein1980semantics, kennedy2005scale}, quantifiers \cite<e.g., \emph{many}, \emph{a lot};>{Scholler2017}, and, in a nonobvious way, category generalizations such as generic or habitual statements \cite<e.g., \emph{Robins lay eggs}, \emph{John smokes};>{Tessler2019psychrev}.
Formally, the comparison class is a free (underspecified) variable that linguistic theories assume is filled in by context \cite{kennedy2005scale, bale2008universal, Bale2011, Solt2009}.
Psychological and/or communicative mechanisms presumably are responsible for that ``filling in'' process, but how those mechanisms operate to determine a comparison class remains obscure. 
Comparison classes thus also serve as a case study of representations at the interface of psychological and linguistic theory.

Empirically, the question of how human listeners decide upon the comparison class has  received little systematic attention.
Empirical work with adults and children has primarily interrogated how judgments and interpretations of relative adjectives (e.g., \emph{dark}, \emph{tall}) depend upon the statistical details of a pre-determined comparison class  \cite{Barner2008, Qing2014, Schmidt2009, Solt2012}.
Indirect evidence that the comparison class is flexibly inferred comes from studies with young children, which find that a directly-modified noun phrase (e.g., \emph{tall pimwit}, where \emph{pimwit} is a novel category label) can be used to constrain the kinds of objects that go into the comparison class: What counts as a \emph{tall pimwit} depends on the distribution of heights of \emph{pimwits} and not the heights of other categories like \emph{daxes} \cite<i.e., \emph{daxes} are not included in the comparison class;>{Barner2008}.
Additionally, strong linguistic and perceptual cues can provide a signal to children as young as two-and-a-half that the comparison class can change \cite<e.g., an objectively small mitten can be \emph{big}, if there are many tiny mittens on the table;>{Ebeling1988, Ebeling1994}.
Still, a systematic investigation into how listeners flexibly adjust the comparison class is outstanding. 

In what follows, we examine the problem of inferring comparison classes in communication using gradable adjectives like \emph{warm} or \emph{tall} as our case study. 
We describe how comparison classes can be incorporated into computational models of pragmatic communication.
The model predicts an intuitive interaction between a listener's world knowledge (e.g., winter days are generally cold) and the valence (or, polarity) of the adjective heard (e.g., \emph{warm}~vs.~\emph{cold}), which contrasts with the predictions of a literal Bayesian reasoner model. 
%a control model without a Theory of Mind based reasoning mechanism does not predict.
The pragmatic model also makes fine-grained quantitative predictions that relate distributional knowledge about the property in the category (e.g., plausible temperatures of Winter days) and prior beliefs about the comparison class to comparison class inferences.
We test these predictions in a pre-registered, large-scale free-production experiment in which participants freely generate comparison classes when interpreting relative adjectives in context.
We further investigate the structure of the comparison class prior and show that it credibly contains elements that encode a preference for basic-level categories \cite{rosch1975family} and category labels that are more common in everyday speech (i.e., usage frequency).
%For these investigations, we present empirical data from a pre-registered, large-scale free-production experiment in which participants freely generate comparison classes for interpreting relative adjectives in context.
%A model that infers the comparison class through a Theory of Mind based reasoning mechanism makes unique predictions, which are empirically confirmed, to 


%We show that a control model that infers comparison classes without Theory of Mind does not make the correct predictions. 


%Through informal model comparison, we show how inferring comparison class requires a Theory of Mind based reasoning mechanism.


%We show how one of these ways -- 

% a pair of computational model that integrate reference classes into communication in different ways. 

%integrates psychological, linguistic, and pragmatic representations in a coherent way and makes quantitative predictions about comparison class inferences. 





%AWe formalize additional predictions about the nature of the reference class prior and show that it credibly contains elements that encode a basic level bias \cite{Rosch1975} and a notion of usage frequency.
%We test these hypotheses in a large-scale, pre-registered, free-production experiment in which participants are freely generate comparison classes when interpreting relative adjectives in context.




%the reference class -- (as 
%In formal linguistic semantics,Comparison classes need to be inferred because any particular referent of discourse can be conceptualized or categorized in multiple ways, giving rise to multiple possible comparison classes:
%A day in January is also a day of the year; if a listener hears ``It's warm'', it could be \emph{warm in comparison to the last week}, \emph{warm for the season}, or \emph{warm relative to other seasons}; it could also be \emph{warm for Boston}, \emph{warm for the northeast USA}, \emph{warm for a place with currently six inches of snow on the ground}, among an infinity of possibilities.


%Any individual thing or event may be incorporated into many different references classes (e.g., a day in Winter is also a day of the year as well as a day of a specific individual week, and so on), which can result in many different expected values or probabilities being assigned to that thing or event  
%
%Frequencies require a well-defined 
%
%If we are asked to find the probability holding for an individual future event, we must first incorporate the case in a suitable reference class. An individual thing or event may be incorporated in many reference classes, from which differ- ent probabilities will result. 
%
%This ambiguity has been called the problem of the reference class. 
%
%to compute frequency for a well-defined category (e.g., to assign a frequentist interpretation of probability to an event. 
%
%The problem stems from trying to prove a value, probability, or interpretation to a specific instance
%The problem is rife in natural language where context-sensitive meanings depend upon what a speaker is assuming to be the reference class.
%
%
%The problem is rife 
%
%; all based not
%
%a speaker uses as a basis of comparison---the \emph{reference class} (e.g., warm relative to days in Winter~vs.~days in other seasons). 
%Assign a valuation, interpretation, or probability to a specific example 
%Determining the reference class that is implied in ordinary language is a problem whose implications range from interpretating relative adjectives (\emph{warm} relative to what?) to assigning valuations to houses by appealing to other ``similar'' houses (e.g., similar houses) 
%
%
%\emph{Warm} is a relative adjective, and its felicity depends upon what a speaker uses as a basis of comparison---the \emph{comparison class} The comparison class is canonically a category used for interpreting relative comparisons (\emph{warm relative to what?}) and can be deployed to make sense of a wide swath of linguistic expressions, including relative adjectives \cite<\emph{warm} or \emph{tall};>{cresswell1976semantics, klein1980semantics, kennedy2005scale}, vague quantifiers \cite<e.g., \emph{many}, \emph{a lot};>{Scholler2017}, and category generalizations such as generic or habitual statements \cite<e.g., \emph{Robins lay eggs}, \emph{John smokes};>{Tessler2019psychrev}.
%


%The fact that comparison classes often go unsaid gives rise to an inferential problem for listeners.
%Furthermore, the connection between inferring the comparison class and theories of adjective meaning remains obscure because models of reasoning developed in psychology rarely talk to models of semantic meaning developed in linguistics.
%Theoretical work in semantics has instead focused on how information from an already determined comparison class is integrated with a compositional semantics and which representations might be preferred \cite{Bale2011, Solt2009}.

% mechanisms that allow contextual and linguistic factors to interface.

%is a case study in the larger project of understanding how human listeners use context to make sense of the words they hear.
%The problem with comparison classes, as with notions of context more generally, is that they are almost never described explicitly (e.g., most speakers would probably articulate \emph{he's tall for person} as ``He's tall'').

%Direct evidence that the comparison class can be flexibly adjusted based on systematic principles and the parameters that guide and constrain these inferences has yet to be uncovered.




%Rational Speech Act models are a framework for formalizing precise hypotheses about linguistic and psychological representations and describe their integration. 



%demonstrate one systematic aspect of comparison class inference: dependence on background knowledge about categories and properties.
%We argue that the problem of comparison class inference is one of pragmatic inference, which we
%%by weighting the prior probability of a comparison class with the likelihood that a speaker would use the adjective heard to describe the referent given that comparison class.
%demonstrate in a computational model of pragmatic reasoning \cite<a Rational Speech Act model;> {Frank2012, Goodman2016, scontras2017probabilistic}.
%When the adjective is inconsistent with the listener's general expectations (e.g., the day in winter is warm), listeners should prefer a relatively subordinate comparison class (e.g., \emph{warm for winter}), whereas when the adjective is consistent with general expectations (e.g., the day in winter is cold), listeners should prefer a relatively superordinate comparison classes (e.g., \emph{cold for the year}).
%
%We show formally that this inference relies upon a sophisticated pragmatic mechanism: A listener uses their knowledge of the specific category (e.g., winter) to guide their expectations of what is likely to be true in the world, while simultaneously imagining how a speaker would behave given a different comparison class (e.g., what a speaker would say given the comparison class of \emph{days of the year}).

%Psychological representations are constructs like knowledge or beliefs, aspects of which can be represented by probabilities. There are also communicative expectations (the domain of pragmatics). To build precise models of language understanding, we do not only describe the factors but articulate how they are integrated together. 


%Comparison classes are necessary for understanding relative adjectives  like \emph{warm} or \emph{tall} \cite{cresswell1976semantics, klein1980semantics, kennedy2005scale, bale2008universal, Bale2011, Solt2009}; in fact, comparison classes can be deployed in any linguistic expression that conveys something relative (\emph{relative to what?}), including vague quantifiers \cite<e.g., ``He ate a lot of burgers'' [\emph{relative to a typical person}];>{Scholler2017} and generic sentences \cite<e.g., ``Dogs are friendly'' [\emph{relative to other animals}];>{Tessler2019psychrev}.
%
%Interpreting an utterance by appealing to a comparison class 





%The particular comparison class that a speaker uses in generating in an utterance, however, is almost never actually articulated.
%%In most contexts, the utterance .
%The fact that comparison classes often go unsaid gives rise to an inferential problem for listeners.
%Any particular referent of discourse can be conceptualized or categorized in multiple ways, giving rise to multiple possible comparison classes.
%A day in January is also a day of the year; if a listener hears ``It's warm'', it could be \emph{warm in comparison to the last week}, \emph{warm for the season}, or \emph{warm relative to other seasons}; it could also be \emph{warm for Boston}, \emph{warm for the northeast USA}, \emph{warm for a place with currently six inches of snow on the ground}, among an infinity of possibilities.
%Furthermore, the connection between inferring the comparison class and theories of adjective meaning remains obscure because models of reasoning developed in psychology rarely talk to models of semantic meaning developed in linguistics.
%Theoretical work in semantics has instead focused on how information from an already determined comparison class is integrated with a compositional semantics and which representations might be preferred \cite{Bale2011, Solt2009}.



%Thus, comparison classes are central to human understanding of relative statements and from the point at which children acquire the meaning for a relative adjective like ``big'', they already understand that the meaning can change with the comparison class.

%The hypothesis space of possible comparison classes is unbounded and deciding what goes into the hypothesis space is probably tantamount to the development of a general theory of concepts.
%Once a hypothesis space is determined, however, a listener still must decide which amongst multiple possible comparison classes a speaker intends (e.g., \emph{warm for winter} or \emph{warm for the year}).
%It is this aspect of the problem---lying between a theory of concepts (which generates possible comparison classes) and natural language (how relative statements are interpreted)---that we bring light to in this paper.



%Intuitively, the comparison class is not a fixed property of the referent nor the referent--predicate pair.
%A ``tall basketball player'' might be tall for a basketball player or just tall for a person.
%We propose a simple hypothesis about how comparison classes are determined
%\mht{i wonder if we should have this paragraph here at all... it kind of breaks the flow. we could just do quantitative modeling, and put this in the discussion section. alternatively, we could double down on this paragraph and make it more clear that this is one of the contributions of this paper. ``In addition to a model of comparison class inference and novel systematic empirical data about comparison class, we introduce a novel data analytic approach...''}
%The model we propose is quantitative in nature and thus, can predict graded inferences about comparison class as a result of background knowledge.
%Background knowledge for language understanding models is often measured empirically by having participants estimate relevant quantities and probabilities \cite<e.g., plausible temperatures of days in winter, summer, etc...;>{Franke2016}.
%These explicit prior elicitation techniques are limited, however, to domains where participants have an accurate representation of the underlying scale (e.g., temperature); many domains can be reasoned about intuitively (e.g., the loudness of a diesel engine~vs.~an electric car) without a clear representation of the underlying scale (e.g., how many decibels is the typical sound of a diesel engine?).
%For this reason, we take a different approach: We utilize the productivity of our computational model to predict data from a related language experiment (truth judgments about adjectives) and synthesize the two data sets using a Bayesian data analytic model, wherein the parameters that govern background knowledge in the pragmatics models are constrained by both data sets.
%This joint data modeling provides a way to pin-down model parameters governing background knowledge by asking participants only simple, natural language questions, while also holding the cognitive models to the high standard of predicting data from multiple experiments using the same parameters.


%with a Bayesian data analytic approach
%\ndg{the following is hard to follow. i'd suggest moving diascussion of 'descriptive bayesian' to discussion and here say more intuitively what we do..}
%We embed our model of pragmatic reasoning in a Bayesian data-analytic model where the parameters that govern background knowledge in the pragmatics model are inferred from the experimental data, a kind of \emph{descriptive Bayesian} modeling \cite{tauber2017}.
%We go further, however, than merely asking what background knowledge would account for comparison class inferences; we harness the productivity of natural language and the Rational Speech Act modeling framework to predict data from a related language experiment (truth judgments about adjectives) that relies upon the same background knowledge.

%Deciding on the relevant comparison class is a case study in the larger question of inferring the appropriate aspects of context for interpreting an utterance.


%We suggest this method
%We find that the comparison class can be flexibly adjusted based on prior knowledge, which our model can predict with high quantitative accuracy.
%\red{[zoom back out. bigger picture.?]}

% listeners will flexibly adjust the comparison class when an adjective signals a degree (e.g., temperature) that is \emph{a priori} consistent with the listener's knowledge of the referent (e.g., as a member of a category that generally has a high or low temperature), the comparison class is likely to be a relatively general category (e.g., a basic or superordinate level category), whereas when the adjective signals a degree inconsistent with the listener's prior beliefs about the referent, the comparison class is likely to be a more specific (e.g., subordinate level) category.\footnote{Here, \emph{generally consistent} means generally high or low relative to a basic-level or superordinate level category that has some non-negligible probability of being a comparison class.}


%We explore such a hypothesis in this paper.
%The prior distribution over comparison classes is a theoretical object of interest in its own right---in its most general form, it is a probability distribution over possible contexts---and we will only begin to understand this distribution's properties via our experiments and model.%  of a comparison class is a

%For example, in Winter, hearing \emph{it's warm} should signal \emph{warm for winter} (subordinate comparison class), while hearing \emph{it's cold} signals \emph{cold for the year} (a more basic or superordinate class).
%The opposite relationship should hold in summer, where \emph{it's cold} should signal cold \emph{for summer} more so than \emph{it's warm}.
%We describe in detail the mechanism behind this inference, formalized in a probabilistic model of language understanding, which in turn generates quantitative predictions that depend on background knowledge about categories and their properties.
%We test the qualitative predictions in a large-scale free-production task where participants are asked to infer the comparison class the speaker had in mind.

%This work provides
%\red{We find X... [zoom back out. big picture] }

%This inference results from pragmatic reasoning and is not predicted by the alternative, non-pragmatic Bayesian model.
%These predictions fall out of a Rational Speech Act (RSA) model for gradable
%adjectives \cite{Lassiter2013, Lassiter2017}, extended to flexibly
%reason about the implicit comparison class.

% a result of the \emph{a
%priori} probability of different temperatures in different seasons: In
%winter, temperatures are relatively low, and thus it is unlikely to
%actually be \emph{warm for the year}.
%In addition, regardless of the
%season and the adjective (e.g., ``warm'' or ``cold''),
%listeners prefer comparison classes that are relatively specific (e.g.,
%relative to \emph{the current season} as opposed to \emph{the whole
%year}); more specific comparison classes have lower variance, and a
%vague adjectives like \emph{warm} carries more information when it is
%interpreted with respect to a lower variance comparison class.

%\textcolor{Blue}{[mht: move to end of first expt]}
%
%The model's quantitative predictions can be generated by explicitly
%specifying the interlocutors' relevant prior knowledge (e.g., beliefs
%about temperatures). The current methodological standard is to measure
%beliefs by having participants estimate quantities or give likelihood
%judgments \cite{Franke2016}. We pursue a different methodology. The
%RSA model captures a productive fragment of natural language; thus, it
%makes predictions about a related natural language task (Expt. 2).
%Critically, we can use the model to predict natural language judgments
%that require the \emph{same prior knowledge} as in Expt. 1 and use
%Bayesian data analysis to jointly infer the shared priors. This approach
%harnesses the productivity of language into experiment design and allows
%us to reconstruct priors without having participants engage in
%challenging numerical estimation tasks.

\section{Computational Model}

%To explicate our model, we use the running example of hearing a basketball player described as either \emph{tall} or \emph{short} (Figure \ref{fig:modelCartoon}).
%The guiding intuition that our model tries to explain is that when the basketball player is described as \emph{short}, it is likely that the comparison class intended by the speaker is the subordinate-level category (i.e., \emph{short for a basketball player}).
%On the other hand, when the basketball player is described as \emph{tall}, the comparison class intended by the speaker is likely to be the more general category (i.e., \emph{tall for a person}).
%The opposite pattern should follow for a member of a category that is generally short, such as a jockey: \emph{he's short} should mean \emph{short for a person}, while \emph{he's tall} should signal \emph{tall for a jockey}.
%Though an intuitive inference,
Here we articulate the computations that underly comparison class inference in communication and how that inference interfaces with standard linguistic theories of adjective meaning.
%We develop a computational model that integrates general purposes reasoning capabilities with a semantic model of adjective meaning.
When interpreting an adjectival utterance $u$ which lacks an explicit comparison class $c$ (e.g., \emph{he's tall} [relative to other c]), a listener is faced with the joint inference problem of determining the comparison class $c$  and the property value (or, degree) $x$  described by the adjective (e.g., the height of the referent).
A listener $L_1$  can use their knowledge of the referent $k$ (e.g., the category membership of the referent such as the referent is a basketball player) to guide both their expectations about the property value $P(x \mid k)$ (e.g., the plausible height of a basketball player) as well as the hypothesis space of comparison classes $P(c \mid k)$ (described in more detail below).
A Bayesian listener would combine these prior expectations with the likelihood that a speaker $S_1(u \mid x, c)$ would bother to say that the adjective applies to the referent (i.e., would the speaker bother to say that the referent is tall) and infer the likely property value (e.g., height) of the referent $x$ and the likely comparison class $c$.

%To explicate our model, we use the running example of hearing a basketball player described as either \emph{tall} or \emph{short} (Figure \ref{fig:modelCartoon}).


%This inference is a result of the adjective \emph{short} conflicting with the general expectation that basketball players are tall people.
%Thus,


%More generally, when an adjective conflicts with a listener's \emph{general expectations} about members of a category (e.g., \emph{short} described of a basketball player; general expectation: basketball players are tall), listeners should accommodate this utterance by positing a more specific comparison class (e.g., \emph{short for a basketball player}).
%Our model grounds the \emph{general expectations} a listener would have about members of the category are, in fact, an interpretation of the scalar adjective under a basic or superordinate-level class (e.g., basketball players tend to be tall people).


% predicted when the basketball player is described as \emph{short} (i.e., more likely to be a \emph{short basketball player} than a \emph{short person}).


%The basic intuition that the model formalizes is that, when describing the height of a basketball player, a speaker is more likely to say \emph{he's a tall person} than \emph{he's a tall basketball player} because it is a more likely state of affairs given distributional knowledge of the heights of people and the heights of basketball players.
%A listener then uses this knowledge of the speaker to infer the unsaid comparison class when the speaker says only: \emph{he's tall}.
%
%Finally, the same adjectives used to describe a jockey should invoke the opposite inferences (i.e., \emph{tall} $\rightarrow$ \emph{tall jockey}; \emph{short} $\rightarrow$ \emph{short person}).

%\subsection{Model specification}


\begin{align}
L_1(x, c \mid u, k) &\propto S_1(u \mid x, c) \cdot P(x \mid k) \cdot P(c \mid k) \label{eq:L1a}
\end{align}
%

Eq. \ref{eq:L1a} describes a pragmatic listener $L_1$ updating their beliefs about the degree $x$ and the comparison class $c$ by assuming that the speaker $S_1$ intentionally produced an adjectival utterance $u$ so that the speaker could accurately communicate about the degree (e.g., height).
The prior distribution of comparison classes $P(c \mid k)$ is assumed to be in common ground and constrained by prior knowledge about categories and the situation (e.g.,~that the referent is a basketball player; a basketball player is a person; etc.).
Comparison classes can be constructed in various ways, including sets of objects in the perceptual environment (e.g.,~\emph{big relative to the things around it}) or from the hypothetical functions of an object \cite<e.g., \emph{this shirt is big for the doll;}>{Ebeling1994}; for simplicity, we restrict our analysis to a hypothesis space of comparison classes constructed out of a taxonomic hierarchy (i.e., \emph{conceptual comparison classes}), with the lowest level of hierarchy being the subordinate category to which the referent belongs (e.g., a basketball player; Figure \ref{fig:modelCartoon}A). 
The kind of property values (or, degrees) under discussion (e.g., heights) is given by the semantics of the adjectives (e.g., \emph{tall} $\rightarrow$ height), and the hypothesis space of degrees is informed by the category membership of the referent $k$ (e.g., plausible heights of basketball players).\footnote{
More generally, we can distinguish shared beliefs from private beliefs. The hypothesis space of comparison classes is constrained by the shared beliefs of the speaker and listener: A listener will not assume the speaker is using a comparison class about which the speaker is ignorant, and a listener cannot construct a comparison class from information that they do not have. By contrast, expectations about the degree value for the referent (e.g., the referent's height) will be guided by the totality of the listener's beliefs --- both those shared with the speaker and those not shared, the listener's private beliefs. For example, if the listener had private knowledge that the referent in question was taking a growth-stunting medication, they would use that knowledge to guide their expectations about that person's height. 
If the listener knew that the speaker did not know about the growth-stunting medication, that information would not enter in the hypothesis space of comparison classes. Formally, this more general model would be given by: \begin{align}
L_1(x, c \mid u, f, g) &\propto S_1(u \mid x, c) \cdot P(x \mid f, g) \cdot P(c \mid f) \label{eq:L1}
\end{align}
\noindent where $f$ denotes the beliefs shared between speaker and listener, and $g$ denotes the listener's private beliefs. Since our paradigm does not distinguish between these kinds of beliefs, we use a simplified-form of the model, where beliefs are represented by a single variable $k$ and can be thought of as the most specific relevant category information about the referent (e.g., the referent is a basketball player).
}

Following work in the Rational Speech Act modeling framework \cite{Frank2012, Goodman2016, scontras2017probabilistic}, the speaker $S_1$ in this model is a soft-max rational agent (with degree of rationality $\alpha$) who produces utterances $u$ in order to convey information about the degree $x$ to a listener $L_0$.
%
\begin{equation}
S_1(u \mid x, c) \propto \exp{(\alpha \cdot (\ln L_{0}(x \mid u, c) - \text{cost}(u) ))}\label{eq:S1}
\end{equation}
%exp{(\alpha_1 \cdot \ln {L_{0}(x \mid u, \theta)} )}
%\ndg{the next two paragraphs are a bit too abstract. perhaps move the content down to where the corresponding model terms are introduced.}
%For both of these random variables, the listener can employ knowledge to constrain the inference problem.
%Specifically,
%\begin{figure}[ht]
%  \begin{center}
%    \begin{tabular}{cc}
%\begin{tikzpicture}
%
%  % Define nodes
%  \node[latent]                             (u) {$u$};
%  \node[latent, above=of u, xshift=-1.2cm] (c) {${c}$};
%  \node[latent, above=of u, xshift=1.2cm]  (x) {${x}$};
%  \node[latent, above=of c, xshift=0cm] (f) {${f}$};
%  \node[latent, above=of x, xshift=0cm] (g) {${g}$};
%
%  % Connect the nodes
%  \edge {c,x} {u} ; %
%  \edge {f} {c} ; %
%  \edge {f,g} {x} ; %
%
%
%\end{tikzpicture}
%
%    \end{tabular}
%  \end{center}
%  \caption{Generative model of utterances in the mind of a listener. An utterance $u$ is a function of a comparison class $c$ and degree $x$, via the $S_2$ model (Equation \ref{eq:S2}). A listener's best guess about the degree is a function of both the shared beliefs about the referent $f$ and  the listener's private beliefs $g$. The comparison class $c$ is a function of only the shared beliefs between speaker and listener $f$.}
%  \label{fig:bayesnet}
%\end{figure}
%Formally, this inference can be captured in a Bayesian formulation:
%
%
The listener in the mind of the speaker $L_{0}(x \mid u, c)$ is assumed to know the comparison class and is simply a mapping (described below) from prior beliefs about the degree $x$ (e.g., height) to posterior beliefs about $x$ given a fully-specified adjectival utterance (i.e., an adjective and comparison class e.g., \emph{tall for a basketball player}).
The speaker, as a rational actor, balances the information conveyed by the utterance, $\ln L_{0}(x \mid u, c)$, with the cost of producing the utterance $u$ when deciding what to say.\footnote{For simplicity, for all of our qualitative and quantitative modeling, we assume no difference in production cost for different utterances. Hence, our model reduces to simply: $S_1(u \mid x, c) \propto L_{0}(x \mid u, c)^{ \alpha}$}
We assume the speaker has three utterances she can say: \{\emph{tall}, \emph{short}, silence\}, where silence is a semantically vacuous utterance (i.e., a null action).\footnote{The determination of the set of alternative utterances in a Rational Speech Act model is a degree of freedom on the side of the modeler. We think this set is reasonable, and we note that the inferences we model are invariant to other reasonable choices of alternative utterances. Most notably, including alternatives that allow the speaker to explicitly communicate the comparison class (e.g., the speaker could have said \emph{tall for a basketball player} or \emph{tall for a person}) do not change the key qualitative patterns we describe.
}

The listener who updates their beliefs about the temperature given a vague adjectival utterance and a fixed comparison class, $L_0(x \mid u, c)$, is model of context-sensitive adjective interpretation, a problem which has garnered recent attention by those modeling language understanding \cite{Lassiter2013, Qing2014a, Lassiter2017}.
We use a model of a literal listener which, following standard treatment in formal semantics \cite<e.g.,>{Kennedy2007}, takes the literal meaning of a gradable adjective to be simply a threshold function on the degree (e.g., \([\![tall]\!] = {x>\theta}\)) and is used to update the listener's prior beliefs about the degree in the comparison class $P(x \mid c)$ to produce an interpretation of the adjective, contextualized by the comparison class (Figure \ref{fig:modelCartoon}B).\footnote{Following standard treatment of antonyms, the semantics of \emph{short} are a threshold function on a distinct threshold variable: \([\![u_{short}]\!] = height < \theta_{short}\)), which is also inferred by the listener model (i.e., the listener infers a threshold for both \emph{tall} and \emph{short}). The pragmatic inferences about the comparison class that are the focus of this paper are invariant to whether or not the antonym is included in the alternative set. The comparison class inferences are also invariant to whether or not the antonym is assigned its own unique threshold
  (\(\theta_{short}\)). See \citeA{Lassiter2017} and \citeA{tessler2018not} for an exploration of the implications of assigning antonyms their own thresholds vs. negating the threshold of the positive adjective.}
  %
 \begin{align}
L_{0}(x, \theta \mid u, c) &\propto {\delta_{[\![u]\!](x, \theta)} \cdot P(x \mid c)} \cdot P(\theta) \label{eq:L0}\\
L_{0}(x \mid u, c) &= \int_{\theta} L_{0}(x, \theta \mid u, c) \diff\theta \label{eq:L0_marg}
\end{align}
%
Equation \ref{eq:L0} is a model of literal listener who updates their prior beliefs about the degree given a comparison class $P(x\mid c)$ via a threshold function, represented by the Kronecker delta function \(\delta_{\mbox{ $[\![ u ]\!]$}(x, \theta)}\) that returns \(1\) when the utterance is true (i.e., when \(x > \theta\)) and \(0\) otherwise.
\citeA{Lassiter2013, Lassiter2017} showed how the context-sensitivity of gradable adjectives can be modeled as uncertainty about the threshold $P(\theta)$ (where $\theta$ comes from a uniform prior distribution over the support of the degree prior), which we adopt here in our literal listener model.\footnote{
	The treatment of the threshold inference at the level of literal listener is a point of divergence from the model of \citeA{Lassiter2013, Lassiter2017}, who instead argue for ``lifting'' the threshold inference to the level of the pragmatic listener (i.e., the pragmatic listener infers only what is likely to be an \emph{informative threshold}, whereas the literal listener merely infers what is likely to be \emph{true threshold}). The treatment of threshold inference at the level of the literal listener is consistent, however, with \citeA{Tessler2019psychrev}'s model of generic language interpretation.
	The question of whether the threshold inference should be thought of as a pragmatic inference is an empirical question, and one that is beyond the scope of this paper.  The qualitative predictions of the model about the comparison class are similar if the threshold is lifted to the pragmatic listener in such a way that the pragmatic listener jointly reasons about the threshold $\theta$, the degree $x$, and the comparison class $c$.
}
Finally, we assume the communicative goal of using an adjective like \emph{tall} is to convey information about the height of the referent $x$; thus, the speaker model $S_1$ (Equation \ref{eq:S1}) chooses utterances to convey the height $x$ to the literal listener $L_0$, which we calculate by marginalizing out the threshold variable $\theta$ (Equation \ref{eq:L0_marg}).
%Our model of comparison class inference builds on top of these formal theories, and we treat this model component $L_{0}(x \mid u, c)$ as a black-box function that produces a probability distribution over degrees (e.g., heights) in a manner that is sensitive to the comparison class (e.g., respecting the interpretative difference between \emph{tall person} and \emph{tall basketball player}; ).
%\red{Figure 1 shows the behavior of this model component.}
%We adopt the model of \citeA{Lassiter2013, Lassiter2017}   A full presentation of this  part of the model is given in Appendix A.


\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{figs/model_cartoon.pdf}
\caption{\small \label{fig:modelCartoon}Model overview. A listener hears a basketball player described by a speaker as \emph{tall} . A: A hypothesis space of comparison classes is constructed over a taxonomic hierarchy. B: A comparison class is realized as a probability distribution over the relevant degree (e.g., height; shown in black). A probabilistic interpretation of a gradable adjective (e.g., \emph{tall}; shown in red)---given by $L_{0}(x \mid u, c)$---is specific to the comparison class (facets). C: Listener $L_1$ imagines what a speaker $S_1$ would say given different heights of the referent (x-axis) and assuming different comparison classes (facets); the opacity of the lines is proportional to the listener's prior probability of a particular height for the referent (e.g., since the listener knows the referent is a basketball player, heights towards the upper range of the scale are \emph{a priori} more likely). D: Averaging over the plausible heights of the referent, the speaker would only have a preference to say \emph{tall} over \emph{short} if the comparison class were \emph{people}. E: The pragmatic listener infers that \emph{people} is more likely to be the comparison class given that the speaker said \emph{tall}. If the referent is described as \emph{short}, the listener infers the speaker meant \emph{short for a basketball player}. Schematic prior distribution of heights for people is a unit normal distribution $\mathcal{N}(0, 1)$ and the heights of basketball players is a right-shifted normal with smaller variance $\mathcal{N}(0.5, 0.5)$.
% speaker production probability distributions are shown with the.
}
\end{figure}


%\subsection{Model behavior}

For convenience, we consider an idealized case where there are only two comparison classes: a relatively subordinate-level category ($c_{sub}$) or a relatively superordinate-level category ($c_{super}$) (e.g., tall relative to \emph{basketball players} or relative to \emph{people}): \(c \in \{c_{sub}, c_{super}\}\).
The choice of hierarchically structured categories is convenient because subordinate category membership entails the superordinate category membership, but all that we assume in our modeling is that both categories have some non-zero prior probability of being the comparison class. 
Figure \ref{fig:modelCartoon}C shows the speaker production probabilities of the three possible utterances (\emph{tall}, \emph{short}, silence) for each value along the degree scale (e.g., each height) for each of the two different comparison classes (facets).
If the comparison class is \emph{basketball players}, the speaker will be reluctant to produce \emph{tall} unless the height of the referent is substantially greater than that of the average basketball player (red line shifted to the right with respect to the \emph{person} comparison class).
The listener uses Bayes' rule to ``invert'' this generative model of the utterance (i.e., the speaker model), inferring the implicit comparison class.
In doing so, the listener deploys their prior knowledge about the height of the referent: The listener knows the referent is a basketball player and thus their height is distributed according to the distribution of heights for basketball players  (Figure \ref{fig:modelCartoon}B, bottom, black; this distribution is superimposed as an opacity on Figure \ref{fig:modelCartoon}C).
Averaging over the plausible heights of a basketball player, the listener reasons that a speaker who says \emph{tall} would be more likely to do so assuming the comparison class of \emph{people}  (Figure \ref{fig:modelCartoon}D).
Thus, the listener who hears the basketball player described as \emph{tall} tends to think the speaker meant \emph{tall for a person}, whereas the listener who hears a jockey described as \emph{tall} tends to think the speaker meant \emph{tall for a jockey} (Figure \ref{fig:modelCartoon}E).
Overall, the model predicts an intuitive interaction, wherein a listener infers a more superordinate comparison class when the polarity of adjective (e.g., \emph{tall}~vs.~\emph{short}) is consistent with the listener's background knowledge about the category (e.g., the heights of basketball players~vs.~jockeys, respectively) than when it it inconsistent.

In our model, a pragmatic listener reasons about what comparison class a speaker would be more likely to assume.
One might wonder whether such social reasoning is necessary, or whether these inferences could result from some simpler, non-social Bayesian reasoning. 
We examine this question by reformulating the comparison class inference formalized by a pragmatic listener (Eq.~\ref{eq:L1a}) in terms of a literal listener model that updates its beliefs about the world and the comparison class only via the literal meaning of the utterance and not via pragmatic reasoning (\emph{a la} Eq.~\ref{eq:L0}). 
%Though an intuitive pattern, this reasoning requires a listener separating the comparison class from their own knowledge of the category when reasoning about the meaning of the utterance; an analogous non-pragmatic, literal listener cannot draw this inference (see Supplementary Information).
%
%One might question whether the inference about the comparison class is necessarily a pragmatic inference which requires recursive reasoning. 
%This literal comparison class inference model is given by:
%
  \begin{align}
L_{0}(x, \theta, c \mid u, k) &\propto \delta_{[\![u]\!](x, \theta)} \cdot P(x \mid c)\cdot P(\theta) \cdot P(c \mid k)  \label{eq:L0alt}
\end{align}
%
Similar to the pragmatic listener model (Eq.~\ref{eq:L1a}), this listener knows the category membership of the referent $k$ and can use this knowledge to constrain the hypothesis space of comparison classes via $P(c \mid k)$ (e.g., with the knowledge that the referent is a basketball player, the listener considers comparison classes that are the same as or superordinate to the class of basketball players).
Unlike the pragmatic listener model (Eq.~\ref{eq:L1a}), however, the literal listener version of the model does not simultaneously hold different representations of the referent in mind: The pragmatic listener has both their private representation of the referent---which informs the prior distribution of the degree $P(x \mid k)$---and imagines a speaker who acts by assuming some comparison class is in common ground---$S_1(u \mid c)$---where $c$ and $k$ may or may not refer to the same class (e.g., the listener may know the referent is a basketball player -- $k = \emph{basketball players}$ -- but believes the speaker was assuming a \emph{person} comparison class -- $c = \emph{people}$).
The literal listener version of the model has no way of separating these representations.
% In effect, the question that this listener can address is different from the comparison class inference problem. The question this alternative model is answering is: what is more likely---a basketball player whose height is very great or a person whose height is very great? 
This alternative literal listener model also predicts an interaction between background knowledge and the adjective polarity, but the directionality of these effects is exactly the opposite from the predictions of the pragmatic model (Figure \ref{fig:altModels}).
%Thus, a literal listener model is insufficient to reason about comparison classes and this inference seems to require a Theory of Mind based reasoning mechanism.
% \ndg{this paragraph is rather confusing to me. and as before the use of ToM feels awkward and imprecise...}

\begin{figure}[t]
    \centering
    %\subfigure{\label{fig:alternativeModelPredictions}
    \includegraphics[width=0.48\textwidth]{figs/cc_inference_L0.pdf}
    \caption{Predictions of the alternative model of a literal listener that does not represent a speaker's representation of the context as separate from their own. This listener effectively answers the question of what is more likely: a basketball player who is tall or a person who is tall?}
    \label{fig:altModels}
\end{figure}






%To understand the model behavior, consider the speaker model \(S_1\) under different assumptions (by the listener) about the implicit comparison class: \(S_{1}(u \mid x, c = c_{sub})\) and \(S_{1}(u \mid x, c = c_{basic})\).
%  If the comparison class is \emph{basketball players}, the speaker's criterion shifts to the right and she becomes more reluctant to produce \emph{tall} until the height of the referent is much greater.

%  If the listener marginalizes out the height of the referent, to focus on the comparison class, they believe the speaker is more likely to produce \emph{tall} if the comparison class were \emph{people} (Figure \ref{fig:modelCartoon}D).
%If, however, the listener hears of the basketball player that he is \emph{short}, the more likely comparison class is the subordinate class of \emph{basketball players}.
%This inference is driven by prior knowledge about the category, and thus, we would expect the inferences to change if the prior knowledge changes.
%Indeed, the same adjectives used to describe a member of a subordinate category that tends to fall low on the degree scale (e.g., jockeys, who tend to be short people) will result in the opposite inferences about the comparison class (Figure \ref{fig:modelCartoon}E left bars).
%Thus, our model predicts that the comparison class can be flexibly adjusted and it provides a precise quantitative formulation in how this inference should depend upon the quantitative details of the priors.


%\begin{figure}
%\centering
%\includegraphics[width=0.6\textwidth]{figs/cc_inference_L0.pdf}
%\caption{\small \label{fig:alternativeModelPredictions}}
%\end{figure}

%The utterance which does not reference a class (e.g., It's warm}) inherits the same meaning as the utterance with the explicit class that
%matches the assumed implicit class (i.e., if \(c_{i} = c_{sub}\), then
%\(u_i\) has the same meaning as \(u_{sub}\)). Therefore, reasoning about
%the likely comparison class reduces to reasoning about which explicit
%utterance the speaker would have been more likely to say.


% it involves representing a speaker's beliefs about the context separately from the listener's beliefs about the context.
%This can be seen by comparing and \ref{eq:L0}: the pragmatic listener $L_1$ uses a prior distribution over the degree given their knowledge of the referent $P(x \mid k)$ whereas the literal listener $L_0$ uses a distribution conditional on the comparison class $P(x \mid c)$.
%This separation of the listener's knowledge of the referent $k$ from the comparison class $c$ is necessary for the comparison class inferences shown in Figure \ref{fig:modelCartoon}E.
%A purely Bayesian listener model, who has uncertainty about the comparison class as well as all of the other parameters of the full model (the degree $x$ and the threshold $\theta$), but does not separate their knowledge of the referent from the speaker's representation of the context is given by:
%


\section{Experiment}

%\section{Overview of Experiment}

We test the model's main prediction that the comparison class can be flexibly adjusted based on world knowledge and pragmatic principles in a large-scale, web-based free-production experiment.
In the experiment, participants rephrase a speaker's statement, which involves a scalar adjective, in a way that makes the comparison class explicit.
(A smaller-scale, forced-choice version of this task was reported in \citeA{tessler2017warm}.)
%Using a free-production measure increases ecological validity: We wish to see if listeners spontaneously adjust their comparison class depending on world knowledge and pragmatic reasoning.
 %; here, we conduct a free-production version of this task in order test whether the comparison classes are spontaneously inferred by participants.
To norm the world knowledge needed in our quantitative modeling, we also measure truth judgments for statements involving scalar adjectives and an explicit comparison class (described more below and described fully in SI).
%We model both data sets using the same RSA architecture.
Sample size, exclusion criteria, regression analysis, and cognitive model analysis were preregistered for the Comparison Class Inference task: \url{osf.io/xuc96}.
All data reported, analysis scripts, code for cognitive models and web experiments can be found at \url{github.com/mhtess/comparison-class-paper}.


%The quantitative predictions of the model are tested using a joint Bayesian data analytic strategy, where we explicitly model the data from both the Comparison Class Inference and Adjective Endorsement tasks.
%This joint Bayesian data-analytic strategy allows us to infer from simple linguistic judgments the world knowledge that guides the inference about the comparison class in the computational model.

%\mht{may want to reverse the order, if we use the modal superordinate comparison class from the Inference experiment as the comparison class in the Adjective Endorsement experiment}






%To match the  we code the behavioral responses as to whether or not the mention the subordinate level category that is used to describe the referent in the experimental context.
%
%
%Inferring the comparison class is necessarily a problem of inferring the intentions of another person.
%The relevant question is not \emph{is today a day in winter?} (an inference about the world), but rather \emph{did the speaker mean to draw a comparison to days to winter?} (an inference about intentions).
%Thus, a model for comparison class inference necessarily involves social reasoning about the speaker's intentions.
%
%Thus, the listener's beliefs about the temperature is a probability distribution conditional on the speaker and listener shared beliefs as well as the listener's private beliefs: $P(x \mid f, g)$.
%In the contexts we consider, the listener has no private beliefs and the totality of relevant shared beliefs boils down to the most specific categorization that the listener believes to be true of the referent (i.e., the day is a day in winter).

%On the other hand, on the shared beliefs $g$ can be used to guide

%
%Both sets of beliefs can constrain the listener's belief distribution over the relevant degree as applied to the referent: If the listener knows that the day is a day in winter, they should use that information to guide their knowledge about the likely value of the degree $P(x \mid f, g)$.
%
%
%We model the scenario where a listener hears a gradable adjective describing a referent (e.g., that the temperature outside is warm) but does not know the comparison class assumed by the speaker.
%To do this, a listener draws on their knowledge of the referent (e.g., that the day is a day in winter)
%
%When understanding vague language like a gradable adjective in context, a listener must integrate what they know about
%
%Thus, we begin with a model for gradable adjective interpretation and build a mechanism for comparison class inference on top of this model.
%. Where does this comparison class come from?
%
%We hypothesize that listeners maintain uncertainty about multiple
%possible comparison classes, but can reduce their uncertainty by
%combining world knowledge with pragmatic reasoning. More specifically,
%listeners use their world knowledge of what worlds are plausible under
%different comparison classes \(P(x \mid c)\) (e.g., the likelihood of
%different temperatures within different seasons), what implicit
%comparison classes are likely to be talked about \emph{a priori}
%\(P(c_i)\) (\(i\) for implicit), and how a rational speaker would behave
%in a given world assuming a particular comparison class
%\(S_{1}(u \mid x, c_i, \theta)\) (Eq. \ref{eq:L1a}).
%
% As in previous models, we
%assume the listener is aware that the referent is a member of the
%subordinate class (and by extension, the superordinate as well). We
%additionally assume the pragmatic listener uses the most subordinate
%class information to inform the likely values of the degree (e.g., the
%listener's prior over temperatures is given by the distribution of
%temperatures for a specific class such as \emph{winter}
%\(P(x \mid c = c_{sub})\)). With these assumptions, the model becomes:
%
%\begin{align}
%L_{1}(x, c_{i}, \theta \mid u) &\propto S_{1}(u \mid x, c, \theta) \cdot P(x \mid c =  c_{sub}) \cdot P(c_{i}) \cdot P(\theta) \label{eq:L1a}\\
%S_{1}(u \mid x, c_i, \theta) &\propto \exp{(\alpha_1 \cdot \ln {L_{0}(x \mid u, c_i, \theta)}- \text{cost}(u)) } \label{eq:S1a}\\
%L_{0}(x \mid u, c_i, \theta) &\propto {\delta_{[\![u]\!](x, \theta)} \cdot P(x \mid \text{parseClass}(u, c_i))} \label{eq:L0a}
%\end{align}
%
%We are interested in the behavior of the pragmatic listener model with
%he hears an utterance without an explicit comparison class \(u_{i}\)
%(e.g., ``It's warm''). The listener reasons about alternative
%utterances the speaker could have said in order to draw pragmatic
%inferences. In this model, we assume the speaker has the option of
%conveying the adjective with an explicit comparison class \(u_{sub}\)
%and \(u_{super}\) (e.g., ``It's warm relative to other days in
%winter'' and ``It's warm relative to other days of the year'').
%The literal meanings of these alternatives are the same as the
%underspecified utterance (i.e., a threshold function:
%\([\![u_{warm}]\!] = x > \theta\)), but have the additional feature of
%overriding the implicit comparison class \(c_i\)and forcing the literal
%listener into a particular comparison class encoded in the utterance via
%the function \(\text{parseClass}\). That is:
%
%\begin{eqnarray}
%\text{parseClass}(u, c_i) & = &
%\begin{cases}
%c_{i} & \text{if } u = u_{i}\\
%c_{sub} & \text{if } u = u_{sub}\\
%c_{super} & \text{if } u = u_{super}\\
%\end{cases}
%\end{eqnarray}
%
%Thus, the speaker conditioning on a particular value for \(c_{i}\) only
%has implications for the literal listener if the speaker chooses to
%produce the implicit utterance (e.g., ``It's warm''). Should the
%speaker instead choose an utterance that explicitly articulates the
%comparison class (e.g., ``It's warm for winter''), the literal
%listener will use the explicit class to set his prior expectations
%\(P(x \mid c)\) via the \(\text{parseClass}\) operator.
%




%\subsubsection{Qualitative model predictions}


%\begin{figure*}[htb]

%{\centering \includegraphics[width=1\textwidth]{figs/expt1results-1}
%
%}
%
%\caption{Empirical comparison class judgments in terms of proportion in favor of subordinate comparison class.  Error bars correspond to 95\% Bayesian credible intervals.}\label{fig:expt1results}
%\end{figure*}
%
%\begin{figure*}[htb]
%
%{\centering \includegraphics[width=1\textwidth]{figs/modelParameters-1}
%
%}

%\caption{Reconstructed degree priors (top) and empirically derived comparison class priors (botton). Top: Inferred prior distributions of world knowledge used to model Experiment 1 and 2 data. Bottom: Inferred prior probability of the subordinate comparison classes based on Google WebGram frequencies. Error bars correspond to 95\% Bayesian credible intervals, derived from the posterior on the $\beta$ scale parameter.}\label{fig:modelParameters}
%\end{figure*}





%\subsection{Task 2: Comparison class inference}

%We report detailed methods for Task 2 (Comparison Class Inference) here.
%Methods and results concerning Task 1 (Stimuli Generation) and Task 3 (Adjective Endorsement) are described in brief here, and in full detail in the Supplement.



\subsection{Methods}

\subsubsection{Participants}

We recruited 837 participants from Amazon's Mechanical Turk.
This number was arrived at with the goal of estimating the probability of a subordinate vs. superordinate comparison class (in a 2AFC setting) with confidence intervals no larger than 0.20 for each unique item in the data set, which conservatively produced an estimate of roughly 50 responses per item.

Participants were restricted to those with U.S. IP addresses with at least a 95\% work approval rating.
In addition, participants were required to pass a simple language comprehension test that we designed in order to weed out bots and other bad-faith participants (see SI).
Participants who failed this check were required to exit the experiment, and so we do not have an estimate for how many participants failed this check.

%\red{describe other exclusion criteria}

\subsubsection{Materials}

The experiment involved the interpretation of gradable adjectives that describe physical dimensions (15 adjective pairs in total, e.g., \emph{tall}/\emph{short}; \emph{warm}/\emph{cold}; see Table \ref{tab:1} for full list).
The categories (nouns) used in the experiment were a modified subset of a set of adjective-noun pairs generated by a separate group of participants ($n=50$) in a task designed to elicit stimuli for this experiment (see SI for details).
From this set of participant-generated stimuli, we curated 90 item sets, each composed of three categories at the same level of abstraction  (e.g., they were all subordinate categories of the same basic-level category) that were sensible in the contexts we used in the Comparison Class Inference experiment.
Two examples from each adjective pair are shown in Table \ref{tab:1}.

Each of the 90 item sets contains 3 relatively subordinate categories, which vary according to common-sense, general expectations given by background knowledge about a degree (or, property value). For example, a basketball player, a soccer player, and a jockey vary according to common-sense expectations about their relative heights (i.e., these categories fall on the high-, medium-, and low-ends of the height scale, respectively). Furthermore, each set of 3 subordinate categories fall under the same relatively superordinate category (e.g., basketball players, soccer players, and jockeys are all \emph{people} [the superordinate category]).\footnote{
We refer to the level of abstraction of these categories as \emph{relatively} subordinate or superordinate, because the item sets may be sets of (i) subordinate-level categories within the same basic-level category (e.g., Great Danes, chihuahuas, within dogs) or (ii) basic-level categories within the same superordinate-level category (e.g., elephants, squirrels, within animals). Later, we will explicitly model the precise level of abstraction of these categories.
}
The categories within each item set are then paired with either a positive-form or negative-form adjective (e.g., tall or short).
Thus, this experiment had 270 unique categories (90 sets $\times$ 3 subordinate categories of different general expectations/background knowledge) predicated in sentences by 2 adjectives each (e.g., tall and short), for a total of 540 unique items.


\begin{table}[!ht]
\centering
\begingroup\fontsize{10pt}{10pt}\selectfont
\begin{tabularx}{\textwidth}{lll}
  \hline
Adjectives (scale) & Example subordinate classes (superordinate class) \\
  \hline
  \emph{big}, \emph{small} (size) & \emph{great dane}, \emph{poodle}, \emph{chihuahua} (dogs) \\
						 & \emph{elephant}, \emph{monkey}, \emph{mouse} (animals) \\
\emph{tall}, \emph{short} (height) &  \emph{basketball player}, \emph{golfer}, \emph{jockey} (people) \\
							&  \emph{redwood}, \emph{alpine}, \emph{bonsai} (trees) \\
  \emph{expensive}, \emph{cheap} (price) & \emph{boots}, \emph{sneakers}, \emph{sandals} (footwear) \\
						   & \emph{steakhouse}, \emph{buffet}, \emph{diner} (restaurants) \\
    \emph{warm}, \emph{cold} (temperature) & \emph{summer}, \emph{fall}, \emph{winter} (seasons) \\
							& \emph{soup}, \emph{salad}, \emph{ice cream} (food) \\
    \emph{hot}, \emph{cold} (temperature) & \emph{coffee}, \emph{juice}, \emph{milkshake} (drinks) \\
								& \emph{sauna}, \emph{shopping mall}, \emph{ice rink} (places) \\
  \emph{heavy}, \emph{light} (weight) &  \emph{wool}, \emph{cotton}, \emph{silk} (materials) \\
							  &  \emph{rock}, \emph{stick}, \emph{feather} (objects) \\
  \emph{long}, \emph{short} (duration / length) & \emph{slacks}, \emph{capris}, \emph{shorts} (pants) \\
								  & \emph{novel}, \emph{story}, \emph{poem} (readings) \\
  \emph{loud}, \emph{quiet} (loudness) &   \emph{baby}, \emph{teenager}, \emph{adult} (people) \\
							  &  \emph{auditorium}, \emph{classroom},   \emph{study hall} (rooms) \\
 \emph{noisy}, \emph{quiet} (loudness) &  \emph{horn}, \emph{guitar}, \emph{harp} (instruments) \\
							 &  \emph{powerboat}, \emph{sailboat}, \emph{row boat}  (boats) \\
  \emph{light}, \emph{dark} (luminance) & \emph{day}, \emph{dusk}, \emph{night}  (times of day) \\
								  & \emph{white paint}, \emph{blue paint}, \emph{black paint}  (paints) \\
   \emph{fast}, \emph{slow} (speed)   &  \emph{runner}, \emph{skier}, \emph{weight lifter} (athletes) \\
								  &  \emph{glider}, \emph{helicopter}, \emph{plane} (aircraft) \\
  \emph{quick}, \emph{slow} (speed) &  \emph{rabbit}, \emph{cat}, \emph{turtle} (pets) \\
							  &  \emph{instant pot}, \emph{frying pan}, \emph{crockpot} (cookware) \\
  \emph{strong}, \emph{weak} (strength) &  \emph{hurricane}, \emph{thunderstorm}, \emph{rain} (storms)\\
						  &  \emph{lion}, \emph{dog}, \emph{mouse} (animals)\\
  \emph{hard}, \emph{soft} (hardness) &  \emph{jolly rancher}, \emph{chocolate}, \emph{marshmallow} (sweets)\\
							  &  \emph{tile}, \emph{wood}, \emph{carpet} (floor materials)\\
  \emph{wide}, \emph{narrow} (width) & \emph{boulevard}, \emph{street}, \emph{country lane} (roads) \\
							  & \emph{truck}, \emph{car}, \emph{golf cart} (vehicles) \\
   \hline
\end{tabularx}
\caption{Example sets of adjectives and categories used in the primary experiment.
Categories were curated from a set of empirically elicited noun phrases from a stimulus generation task (see SI).}
\label{tab:1}
\endgroup
\end{table}



\subsubsection{Procedure}

%Following the language comprehension test (described above), participants read instructions about the task.
The experiment began with a warm-up trial that was designed to convey the intuition behind comparison class inference.
%Participants were told they they would be asked to rephrase a relative statement that somebody made and the their task was to figure out what the speaker meant  was relative to.
Participants were given the example of \emph{John says: ``The Empire State Building is tall''} and asked \emph{What do you think John meant?}; participants responded by filling-in a sentence in the same manner as they would do on the main trials (i.e., \emph{The Empire State Building is tall relative to other \_\_\_}).
%Participants were told to fill in the blank with a group or category that makes the most sense and to use their common sense.
Invalid responses to this warm-up trial were used as a basis for exclusion (see SI).

%On each trial, participants were given a context sentence to introduce
%the subordinate category (e.g., \emph{Tanya lives in Maryland and
%steps outside in winter}). This was followed by an adjective sentence, which predicated either a positive- or negative-form gradable adjective over the item (e.g., \emph{Tanya says to her friend, ``It's warm.''}). Participants were asked \emph{What do you think Tanya meant?} and given a sentence frame they could complete with with a freely-produced comparison class:

Participants then completed 36 main trials. Each main trial began with a \emph{context sentence} that introduced the referent as a member of a subordinate category and provided an appropriate, minimal context in which the adjective could be uttered (e.g., \emph{John sees a \{basketball player, golfer, jockey\}}); the same context sentence was used for all 3 categories in an item set.
Then, a speaker utters an adjectival utterance predicating the adjective of a pronoun used to refer to the referent (e.g., \emph{John says: ``They're tall''}).\footnote{
We had the speaker use a pronoun so as to not provide a strong linguistic cue as to the intended comparison class. If the referent was a person, we used  the singular ``they'' to refer to them; otherwise, we used either ``it'' or ``they'' depending on the plurality of the referent.
}
The participant was asked what they thought the speaker meant (e.g., \emph{What do you think John meant?}). Participants responded by freely filling in a sentence that required an explicit comparison class:
\begin{quote}
They're tall relative to other \_\_\_\_\_\_.
\end{quote}
We used the word ``other'' to invoke the presupposition that the referent is a member of the comparison class. Pilot testing suggesting that omitting this word invoked many direct comparisons to singular entities (e.g., \emph{They're tall relative to their short friend}), which were wildly heterogeneous in nature.
After the main comparison class inference trials, participants completed a memory check trial where they had to select (from a checklist of 10 options) all of the adjective--noun phrase pairs (e.g., \emph{tall -- basketball player}, \emph{green -- tennis ball}) that they had seen on the main trials (see SI for full details). 
%This memory check trial was also used as a basis for exclusion.



\subsection{Results}

Participants were excluded if they either responded incorrectly to the warm-up trial (answering something other than \emph{buildings}, \emph{skyscrapers}, \emph{structures} or the like) or answered fewer than 7 out of 10 post-test memory check questions accurately.
750 participants (89.6\%) remained after these exclusion criteria, for a total of 27,000 responses.
We additionally excluded nonsensical responses (443 in total, or 1.6\%)  and responses that resulted from the participant inferring an unintended referent for the pronoun in the target sentence (e.g., given ``Alex is in a forest and hears a woodpecker. He says: \emph{It is loud}'', the participant infers \emph{it} refers to the forest; 267 in total, or 1.0\%).
% : these were primarily responses that just listed the name of the speaker, the adjective alone, or restated the full adjectival sentence (e.g., ``It is tall'') and numbered 420 (1.56\%) in total, leaving 26,580 codeable responses. \
We preprocessed the remaining \rlgetnum{mention_category_counts.csv}{type}{notSubordinate_notSuperordinate}{total_n}{1} responses by correcting for misspellings  (\rlgetnum{correction_counts.csv}{key}{corrected_spelling}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_spelling}{prop}{1}\%), lemmatizing (\rlgetnum{correction_counts.csv}{key}{corrected_lemma}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_lemma}{prop}{1}\%), and collapsing across synonyms (\rlgetnum{correction_counts.csv}{key}{corrected_synonym}{total}{1}; \rlgetnum{correction_counts.csv}{key}{corrected_synonym}{prop}{1}\%; see SI for details).

%\subsubsection{Preprocessing}

%In order to improve alignment across responses, we corrected for misspellings (569; 2.14\%), lemmatized to align plural and singular responses (e.g., \emph{baby} and \emph{babies}; 867, 3.28\%), and replaced some responses with obvious synonyms (e.g., \emph{child}--\emph{kid}; \emph{booze}--\emph{alcoholic drink}; \emph{humans}--\emph{people}; 323, 1.22\%).

Pilot testing suggested that participants primarily provided comparison class paraphrases that were identical to the subordinate noun phrase (NP) by which the referent is introduced (\emph{subordinate-NP}, e.g., basketball players) or a more superordinate category (\emph{superordinate-NP}, e.g., people).
We took advantage of the fact that conveying a category at least as specific as the subordinate-NP generally requires including the subordinate-NP in the response (e.g., \emph{male basketball players} is more specific than \emph{basketball players} and includes the substring \emph{basketball player})\footnote{Negation markers (e.g., ``all kinds of people \emph{except} basketball players'') are one case where this assumption would be invalid, but no responses used negation markers in this way.} to automatically categorize responses as \emph{subordinate} if the preprocessed response contained the subordinate-NP as a substring (\rlgetnum{mention_category_counts.csv}{type}{Subordinate_notSuperordinate}{n}{1}, or \rlgetnum{mention_category_counts.csv}{type}{Subordinate_notSuperordinate}{prop}{1}\% of all responses); all other responses were coded as \emph{superordinate} (10596, or 40.3\%).
%A corollary of this assumption is items that do not mention the subordinate-NP are superordinate categories. 
Thus, ``superordinate'' here refers to categories potentially at different levels of abstraction (e.g., a wrestler may be strong relative to other \emph{wrestlers}, \emph{athletes}, or \emph{people}; in this case, both \emph{athletes} and \emph{people} would  be categorized as superordinate) as well as categories along different conceptual hierarchies (e.g., the price of a particular \emph{top-shelf liquor} could be expensive relative to other \emph{alcoholic drinks} or relative to the same kind of top-shelf liquor at \emph{other stores}; a \emph{pet parrot} could be loud relative to \emph{other pets} or relative to other \emph{birds}; etc.).

To better understand the distribution of free responses, we performed a secondary analysis where we first categorized the superordinate responses by whether or not they contained the modal superordinate-NPs for that item. \footnote{We experimenters guessed the modal superordinate-NPs for each item (e.g., shown in Table 1), and in 89 out of 90 cases, the pre-designated superordinate-NP was the same the empirically most popular response. For the 1 item that our initial guess was incorrect (\emph{desserts} for the set of \emph{smoothies}, \emph{muffins}, and \emph{chocolate fondue}; the modal superordinate NP was in fact \emph{foods}), we replaced our pre-designated superordinate-NP with the empirical modal superordinate-NP (\emph{foods}).}
Of the 10596 responses categorized as ``superordinate'', \rlgetnum{mention_category_counts.csv}{type}{notSubordinate_Superordinate}{n}{1} (\rlgetnum{mention_category_counts.csv}{type}{notSubordinate_Superordinate}{prop}{1}\% of all responses) explicitly mention the modal superordinate-NP for that item and \rlgetnum{mention_category_counts.csv}{type}{notSubordinate_notSuperordinate}{n}{1} (\rlgetnum{mention_category_counts.csv}{type}{notSubordinate_notSuperordinate}{prop}{1}\% of all responses) mention neither the subordinate nor the modal superordinate-NP (``other'' responses).
Of the 15\% of responses that mentioned neither subordinate nor superordinate responses, 77\% (3034 responses) were responses that were produced by at least three participants, suggesting that these were robust alternatives to the other more salient options (i.e., subordinate or modal superordinate). 
% and we manually coded these robust alternative responses in more detail.
These ``other'' responses fell into five natural categories, in roughly equal proportions: categories at an intermediate level of abstraction between subordinate and superordinate (e.g., \emph{melons} is intermediate between \emph{watermelons} and \emph{fruit}; \emph{drinks} is intermediate between \emph{smoothies} and \emph{food};  28\%), categories that were superordinate to the modal superordinate response (e.g., \emph{desserts} is superordinate to \emph{cakes} which is superordinate to \emph{cheesecakes}; \emph{instruments} is superordinate to \emph{guitars} which is superordinate to \emph{ukeleles}; 28\%), categories that were superordinate along different conceptual hierarchies than the modal superordinate category (e.g., chicken at a butcher shop that was expensive relative to other \emph{butcher shops} as opposed to relative to other \emph{meats};  the nighttime that was dark relative to other \emph{places} as opposed to relative to other \emph{times of day}; 14\%),  rough synonyms of the modal superordinate category (e.g., \emph{bouquets} instead of \emph{flower bouquets}; \emph{methods of cooking} instead of \emph{cookware}; 21\% of ``other'' responses), and responses that referred directly to the scale or dimension described by the adjective, which could be construed as the most superordinate category (e.g., \emph{expensive} relative to other \emph{prices}; 9\%).
In the subsequent analyses, we treat the \emph{other} responses that mention neither the subordinate or superordinate-NP as \emph{superordinate}.
The qualitative results do not change, however, when we exclude these \emph{other} responses altogether (see SI for a parallel set of regression results excluding the \emph{other} responses). 


%Of these 3034 ``other'' responses that were mentioned by at least 3 participants, 67\% of them (2076) were clearly superordinate categories either at a level of abstraction in between the subordinate and the modal superordinate (e.g., \emph{fabric} for the \emph{light silk}, instead of \emph{materials}) or at a level superordinate to the modal superordinate (e.g., \emph{animals} for the \emph{loud owl}, instead of \emph{birds}).
%Roughly 10\% of this bunch were very general terms (e.g., \emph{colors}, \emph{fur}) which in context are ambiguous as to the level of hierarchy (e.g., does \emph{fur} mean \emph{panther fur} or \emph{animal fur}?).
%15\% picked out different hierarchies than the ones we had imagined (\emph{chocolates} for the \emph{warm chocolate fondue}, whereas the modal superordinate was \emph{desserts}; \emph{days} for the \emph{hot sauna} as opposed to other \emph{saunas} or \emph{locations}) and roughly 10\% were 


%; we present analyses that exclude these responses in the SI. 
%That is, 22313 (84.7\%) of valid responses could be automatically categorized as either the subordinate-NP or a superordinate-NP that was consistent across items in the item set.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figs/bars_cc_finalExpt_prereg_bars_syncDodge.pdf}
%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
\caption{Comparison Class Inference experimental results. Proportion of paraphrases that contained the Subordinate NPs (e.g., \emph{basketball player}) with which the referent was introduced, as a function of the general expectations (background knowledge) listeners have about the category (x-axis; e.g., gymnasts = low, basketball players = high) and the polarity of the adjective used to describe the category (e.g., \emph{tall} = positive, \emph{short} = negative). Bars represent overall means and error bar is a bootstrapped 95\% confidence interval. Each dot represents the mean of a single item and lines connect Subordinate NPs described with different adjectives (e.g., \emph{tall} and \emph{short} basketball player). Dots are jittered horizontally to improve visual clarity.}\label{fig:ccInferenceMain}
\end{figure}


%\red{For example, in response to  \{basketball players, golfers, jockeys\} are \{tall, short\}, \textbf{X\%} of participants produced either one of the three specific NPs or the more general \emph{people}}. \mht{[how common is athlete here?]}
%We use only these data for the main analyses.
%Of the remaining 4394 (16.6\%) responses, 1966 were unique.
%We hand-annotated these remaining responses; the main results and conclusions are consistent when analyzing the full data set (see SI).


\subsubsection{Regression analysis}

Our first test is to examine how subordinate~vs.~superordinate comparison class inferences are influenced as a function of general expectations (i.e., background knowledge) about the category of the referent and the polarity of the adjective used to describe the referent.
Specifically, we test for the qualitative pattern predicted by the comparison class inference model: an interaction between the general expectations about the category and the polarity of the adjective, which we tested with a maximal Bayesian mixed-effects model.\footnote{
	For the regression analysis, we predict subordinate-NP paraphrases as a function of the general expectations about the subordinate category (low, medium, high; dummy coded with the medium category as the reference level), the adjective (positive vs. negative; difference coded), and their interaction; in addition, we include the maximal mixed effects structure by-item set and by-participant that mirrors this fixed effects structure. The model is subordinate\_inference $\sim$ gen\_expectations * adjective\_polarity + (1 + gen\_expectations * adjective\_polarity | participant) + (1 + gen\_expectations * adjective\_polarity | item\_set).
}
Further, the directionality of these effects arbitrate between the pragmatic~vs.~literal comparison class inference models. 
%As predicted by the comparison class inference model, listener inferences about the comparison class were influenced via an interaction between the general expectations about the subordinate category and the adjective polarity , which
%We confirmed
%Our comparison class inference model makes qualitative predictions about the likely comparison class assumed by a speaker when they hear a scalar adjective (e.g., \emph{he's tall}; Figure \ref{fig:modelCartoon}E).

As predicted by the comparison class inference model, we observe a pair of two-way interactions in the directions predicted by the pragmatic, but not the literal, listener model (Figure \ref{fig:ccInferenceMain}):
When the subordinate category was expected to be near the high-end of the scale (e.g., basketball player), the positive-form adjective (e.g., tall) led to fewer subordinate comparison classes than the negative-form adjective (e.g., short) in comparison to the control, middle of the scale items (e.g., soccer player): posterior mean beta-weight and 95\% Bayesian credible interval: \brmresults{expt_brm_contrasts_10k_2.csv}{adjpol_nphigh}.
This interaction was the result of the high-end of the scale subordinate categories showing more subordinate comparison class inferences for the negative adjective (e.g., short for a basketball player) than for the positive adjective (e.g., tall for a basketball player): \brmresults{expt_brm_contrasts_10k_2.csv}{highneg_highpos}.
No preference was observed for the middle-of-the-scale, control items (e.g., tall for a soccer player~vs.~short for a soccer player; \brmresults{expt_brm_contrasts_10k_2.csv}{adjpol}). 
A comparable interaction was observed for categories that were expected to be near the low-end of the scale (e.g., jockey): Hearing the positive-form adjective led to credibly more subordinate inferences (e.g., tall for a jockey) than hearing a negative-form adjective (e.g., short for a jockey), in comparison to the middle-of-the-scale subordinate categories:  \brmresults{expt_brm_contrasts_10k_2.csv}{adjpol_nplow}.
Again, this effect was driven by the behavior of the low-end of the scale subordinate categories, which showed a stronger preference for the superordinate comparison classes for negative than for positive adjectives (\brmresults{expt_brm_contrasts_10k_2.csv}{lowneg_lowpos}).

Our results are remarkably symmetric across the low-end~vs.~high-end of the scale subordinate categories.
We observe an overall preference for subordinate comparison classes for the control (middle-of-the-scale) subordinate categories (e.g., \emph{soccer players}; \brmresults{expt_brm_contrasts_10k_2.csv}{intercept}) and no overall differences in this preference for items at the high-end of the scale (e.g., \emph{basketball players}; \brmresults{expt_brm_contrasts_10k_2.csv}{nphigh}) or the low-end of the scale (e.g., \emph{gymnasts}; \brmresults{expt_brm_contrasts_10k_2.csv}{nplow}). 
%We observe no credible difference in the overall preference for subordinate~vs.~superordinate comparison class inferences when the subordinate category was either near the high-end of the scale (e.g., \emph{basketball players}; \brmresults{expt_brm_contrasts_10k.csv}{nphigh}) or the low-end of the scale (e.g., \emph{gymnasts}; \brmresults{expt_brm_contrasts_10k.csv}{nplow}).
The inferences that result from adjectives that are in conflict with a listener's general expectations of the categories (e.g., tall gymnasts~vs.~short basketball players) were not different between the low-end and high-end items (\brmresults{expt_brm_contrasts_10k_2.csv}{lowpos_highneg}), nor were the inferences  from adjectives that were consistent with general expectations about a category (e.g., short gymnasts~vs.~tall basketball players; \brmresults{expt_brm_contrasts_10k_2.csv}{lowneg_highpos}).


%\ndg{should point to (very nice) fig 2 somewhere?}


\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/bars_cc_finalExpt_prereg_byItem.pdf}
%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
\caption{Comparison class inference results for 24 of the 90 items. Two examples were selected from each unique degree scale; example items were those that exhibited the greatest and smallest variability in comparison class inferences.}\label{fig:ccInferenceItems}
\end{figure}


Taken together, these results point to  comparison class inferences that are guided by general expectations given by background knowledge about the category and the adjective that a speaker uses to describe the referent, as predicted by the pragmatic model.
Human listeners infer the most likely reference class given what their knowledge of the world tells them about the observations that would be remarkable enough for a speaker to mention.
The overall preference for a subordinate level comparison class and the magnitude of the effect of the adjective varies considerably by item, however (Figure \ref{fig:ccInferenceItems}).
We turn to the quantitative aspect of our model to triangulate the sources of this variability.

\subsection{Quantitative modeling of item-variability}

%We then test the simple effects. For items low on the degree scale (e.g., temperatures in winter), positive form adjectives were significantly more likely to imply subordinate comparison classes (\(\beta = 1.41\); \(SE = 0.15;\) \(z = 9.43\)), while the opposite is true for items high on the scale (e.g., summer days; \(\beta = -2.50\); \(SE = 0.19;\) \(z = -13.15\)). Participants reason pragmatically to flexibly adjust the comparison class, combining world knowledge with informativity as predicted by our model.
\subsubsection{Modeling approach}
Our model is not only useful for understanding the basic computations that underly comparison class inference. 
It is a tool for characterizing, by way of the gradience in the model's predictions, the subtleties of comparison class inference.
The model's primary capacity to predict gradience in comparison class inference is as a function of background knowledge (or, \emph{general expectations}) about properties of categories $P(x)$ (e.g., the heights of basketball players, temperatures in Winter, etc.).
Beyond knowledge about properties of categories, baseline expectations about what comparison classes speakers are likely to use  $P(c \mid k)$ can also influence the model's predictions and be a source of variance in inferences.
%Additional variance in inferences, which cannot be explained by world knowledge, can be captured in the model via the comparison class prior $P(c \mid k$), which encodes baseline 
For the conceptual comparison classes that we focus on here, baseline expectations could vary by the level of abstraction of the categories in question \cite<e.g., exhibiting a basic-level bias;>{rosch1975family} as well as the usage frequency of the NPs used to describe those categories.
We thus construct a family of alternative models in order to gain insight into the sources of variability in comparison class inferences; these models differ in their parameterization of the comparison class prior: (1) a Flat Prior model, which assumes the comparison class prior assigns equal probability between subordinate and superordinate classes; (2) a Basic-level Bias (or, Intercept Only) model, which assumes a basic-level bias; (3) a Frequency Effect (or, Slope Only) model, which assumes an effect of usage frequency, but no basic-level bias; (4) a Base-level and Frequency (or, Slope and Intercept) model, which assumes both a basic-level bias and an effect of usage frequency.\footnote{
Because our stimuli were generated by (a separate group of) participants,  we do not know \emph{a priori} if the NPs we used in the task are basic-level categories or subordinate level categories, and hence, whether the relatively more superordinate class in our stimuli should correspond to a superordinate-level category or a basic-level category.
Further, a basic-level bias could operate differently for a subordinate vs. basic-level inference than for a basic vs. superordinate level inference.
Thus, we endow our data-analytic model with two regression coefficient parameters corresponding to different intercept terms (i.e., a basic-level bias term for basic vs. subordinate categories, and a basic-level bias term for basic~vs.~superordinate categories). Further, we introduce a Bernoulli random variable $z$ for each NP (each relatively subordinate category) to indicate whether it is a subordinate-level term or basic-level term. Further details can be found in the SI.
}
We estimate the distributional knowledge about properties $P(x)$, the parameters of the comparison class prior (model-variant dependent), and the speaker optimality parameter $\alpha$ (of the speaker model in Eq.~\ref{eq:S1}) using Bayesian data analysis.

%Thus, the model provides a strong baseline hypothesis for evaluating comparison class inferences by the influence of world knowledge $P(x)$.

Since we infer the world knowledge priors $P(x)$, our analysis technique is similar to a \emph{descriptive Bayesian approach} \cite{tauber2017}, in which the modeler is simply interested in estimating what the prior knowledge in the model would have to be in order to account for the inference patterns in the data. 
Here, however, we go beyond merely accommodating the item-level variation in the data and attempt to predict the variability in the data using independent measurements of participants'  prior knowledge of categories and properties.
%A fully descriptive approach
% we are not interested in whether the variability can be explained by some knowledge in principle, but whether the variability is explained by actual knowledge about the categories and properties.
%(That is, we would like to \emph{predict} not merely \emph{accommodate} item-level variation.)
%This method is not useful for our purposes since we are interested in characterizing the extent of the variability in comparison class inferences that are explained by actual knowledge about the categories and properties, not whether the variability can be be explained by some knowledge in principle. 
Measuring prior knowledge in this setting is challenging, however,  since direct empirical measurements require explicit estimation of relevant quantities or probabilities \cite<e.g., plausible temperatures of days in winter, plausible heights of basketball players, etc.;>{Franke2016} while many of our domains can be reasoned about intuitively (e.g., the loudness of a diesel engine~vs.~an electric car) without a precise representation of the underlying scale (e.g., how many decibels is the typical sound of a diesel engine?).
For this reason, we take a different approach to a canonical prior elicitation task: We take advantage of the fact that our computational model can understand scalar adjectives with explicit comparison classes (e.g., \emph{an electric car is quiet relative to other cars}) and use the model to simultaneously predict data from a separate experiment about these similar adjectival sentences. In this \emph{Adjective Endorsement} task, a separate group of participants ($n=375$) provide truth judgments for adjectives with explicit comparison classes (e.g., ``Imagine a day in Winter. Do you think it is cold relative to other days of the year?''; see SI for details). We then estimate a single set of parameters for the prior knowledge shared between the two tasks (e.g., the distribution of temperatures for days in Winter) using a joint Bayesian data analytic strategy. 
% synthesize the two data sets using a single Bayesian data analytic model, wherein the parameters that govern prior knowledge are jointly estimated from the two tasks (comparison class inference and adjective endorsements).
%Because the adjective endorsement task provides an explicit comparison class, accommodating this data allows us to calibrate the world knowledge without the effect of comparison class inference.

%\ndg{i can imagine reviewers complaining that this is circular, since the truth judgements depend on the same prior knowledge and in a similar way -- so at most our model is simply jointly accommodating the data.... what do you think?}
%\mht{ya, in true fact, we are doing something in between pure prediction and pure accommodation... we're doing constrained accommodation, constrained by being able to accommodate other judgments as well... should we try to add this nuance in there?}
%\ndg{i think a little bit. maybe add something like: ""}

%\mht{we are trying to make the case that comparison class inferences depend upon world knowledge. we expect other judgments to depend on world knowledge as well. is there a truly-circular model that we could show is not as good? perhaps we could show in the SI that the raw correlation between the two judgments is not as predictive as our modeling approach (though that comparison would have 0 parameters, whereas our model has many). we could also try to articulate a true-circular model, which hopefully wouldn't be as good?}
%\mht{i see now that you may have been drawing attention back to the argument we make in the start of the paragraph about prediction vs accommodation... if so, then maybe we just need to be more nuanced in our discussion of prediction vs. accommodation... }

%well, it is true that our model is jointly accommodating the data, and it is true that the truth judgments depend on the same prior knowledge. that is, in fact, the point. 

%These parameters are not totally free, however; we constrain the parameters governing distributional knowledge by feeding them into a second RSA model (which shares much of the same structure as the comparison class inference model) to predict an independent set of endorsements (or, truth judgments) for statements with explicit comparison classes involving the same scalar adjectives applied to the same categories ($n=375$, see SI for details).
%The full parameterization of the comparison class prior takes the form of a logistic-linear model: $P(c) = \text{logistic}(\beta_0 + \beta_1 \cdot \log (\frac{\hat{f}_{sub}}{\hat{f}_{super}}) )$, where $\hat{f}_k$  represents the frequency of the comparison class  $k$ NP estimated from the Google WebGram corpus, $\beta_1$ is the sensitivity of the comparison class prior to relative frequency, and $\beta_0$ is a basic-level bias.

%The model we propose is quantitative in nature and thus, can predict graded inferences about comparison class as a result of background knowledge.
%Background knowledge for language understanding models is often measured empirically by having participants estimate relevant quantities and probabilities \cite<e.g., plausible temperatures of days in winter, summer, etc...;>{Franke2016}.
%These explicit prior elicitation techniques are limited, however, to domains where participants have an accurate representation of the underlying scale (e.g., temperature); many domains can be reasoned about intuitively (e.g., the loudness of a diesel engine~vs.~an electric car) without a clear representation of the underlying scale (e.g., how many decibels is the typical sound of a diesel engine?).
%For this reason, we take a different approach: We utilize the productivity of our computational model to predict data from a related language experiment (truth judgments about adjectives) and synthesize the two data sets using a Bayesian data analytic model, wherein the parameters that govern background knowledge in the pragmatics models are constrained by both data sets.


%For example, basic-level categories may be more probable conceptual comparison classes because of their utility in everyday reasoning additionally, we might expect the relative probability of basic-level~vs.~subordinate level categories to differ from basic~vs.~superordinate categories.
%To investigate these possibilities, we construct and compare models which differ in how the comparison class prior is parameterized.
% one in which the inference is influenced by the frequency of the noun phrase, and one in which both frequency and a basic-level bias have an influence (maximal model).


We infer the parameters for all models using a Bayesian data analytic model that shares the world knowledge parameters between the two tasks (comparison class inference and adjective endorsements) and infers the parameters for the comparison class prior (details of which depend on the model variant: Basic-level Bias, Frequency Effect, etc...) and the speaker optimality free parameters of the RSA models (SI Figure S3).
We implemented the RSA and Bayesian data analysis models in the probabilistic programming language WebPPL \cite{dippl} and performed inference by running 7 MCMC chains with 500,000 iterations each, discarding the first 250,000 for burn-in.
Convergence was checked through visual inspection of the different chains to ensure similar conclusions would be drawn from each chain independently.

%The comparison class inference model has one speaker optimality: $\alpha^\text{1}_{1}$.
%The adjective endorsement model has two speaker optimality parameters:
%$\{\alpha^\text{2}_{1}, \alpha^\text{2}_{2}\}$.

\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{figs/model_scatters_modelVariants.pdf}
\caption{Model fits for main experiment (Comparison Class Inference) and norming experiment (Adjective Endorsement; see SI) for four models that differ in their parameterization of the comparison class prior used inside the RSA model. Flat prior model assumes all comparison classes are equally likely a priori. Basic-level bias model assumes that there is a preference for a basic-level comparison class. Frequency effect assumes the prior probability of a comparison class tracks the frequency of the NP in a corpus. Basic-level and frequency effect assumes that the prior probability of a comparison class is a function of a basic-level bias and frequency. Dots represents means of the human judgments (proportion Specific-NP for Comparison Class Inference experiment; proportion endorsement for norming study) and the Maximum A-Posteriori estimate of the model's predictions. Lines represents bootstrapped 95\% confidence intervals for the data and 95\% Bayesian credible intervals for the models.}
\label{fig:scatters}
\end{figure}
%We explore each of these components of the model to elucidate the cognitive representations that underly comparison class inferences.



%In addition to making qualitative predictions about the level-of-abstraction of the inferred comparison class,
%\ndg{awk. the whole several layer intro to the bda is confusing and a bit redundant.}

\subsubsection{Model results}%
All model variants were able to accommodate the Adjective Endorsement data set well (Figure \ref{fig:scatters}, bottom row).
%The BDA models adjust the parameters of the world knowledge priors used in RSA in such a way as to make the adjectives felicitous for the categories in question; in this way, the adjective endorsement task (SI) serves as a norming data set.
This result is an important sanity check; it shows that the parameters for world knowledge learned by the model are those that maximize the fit to the Adjective Endorsement data, and thus reflect this common-sense world knowledge that this task taps into.  %constraining the parameters of the world knowledge priors 
 Thus, the Adjective Endorsement task works as a kind of vague, prior elicitation, where we can learn about the world-knowledge priors by asking simple natural language queries to participants. 
Correspondingly, the imputed world knowledge priors inferred by the Bayesian Data Analysis model reflect intuitively reasonable general expectations about the categories (Figure \ref{fig:worldPriors}).

The predictions of the different model variants come apart for the Comparison Class Inference data (Figure \ref{fig:scatters}; top row).
The baseline Flat Prior model predicts variability in comparison class inferences only as a function of world knowledge about the properties, assuming all comparison classes are equally likely \emph{a priori}; this model explains roughly 14\% of the variance between items.
The Frequency effect model assumes NPs with higher usage frequency will more likely be used as comparison classes and is able to explain roughly 22\% of the variance.
The Basic-level bias model explains roughly 71\% of the variance by assuming that basic-level comparison classes may be more likely \emph{a priori}.
Finally, the maximally parameterized \emph{Basic-level and Frequency effect} -- a combination of the two alternative models -- gives rise to the best model predictions in terms of variance explained (77\%) and mean squared error (Table \ref{tab:r2bf}), suggesting that the structure of the comparison class reflects both a basic-level bias and an effect of the frequency of the NP, in addition to other possible factors.
We confirm that the maximally parameterized model is the best explanation of the data by comparing the marginal likelihood of the data under each model to compute a Bayes Factor (BF) as a measure of formal comparison. 
BFs quantify how well the model predicts the data, averaging over the prior distribution over parameters; by taking the average over the model's prior distribution over parameters, the measure explicitly takes into account model complexity because higher complexity models have wider prior distributions over parameters \cite{lee2014bayesian}.
The observed data is several orders of magnitude more likely under the full model than the closest competitor model, the simpler Basic-level bias only model (Table \ref{tab:r2bf}).

%Our BDA model returns posterior distributions over the parameters of the RSA models, governing world knowledge, the comparison class prior, and the speaker optimality parameters.
%We first determine which parameterization of the comparison class prior (flat, basic-level bias, frequency, basic-level bias and frequency) is the best for our comparison class inference RSA model to predict the comparison class inference data.
%We do so by examining the model's posterior predictive distribution over comparison class inference choices.
%The posterior predictive distribution marginalizes over the inferred values of the parameters to show what data the model would expect to see, given the parameters it has learned from the data.
%Posterior predictive checks are an important step in model validation and provide a window into the model's strengths and shortcomings.

%and (2) comparing the marginal likelihood of the data under each model to compute a Bayes Factor (BF) as a measure of formal model comparison. BFs quantify how well the model predicts the data, averaging over the prior distribution over parameters; by taking the average over the model's prior distribution over parameters, the measure explicitly takes into account model complexity because higher complexity models have wider prior distributions over the parameter \cite{lee2014bayesian}.


%\begin{figure}[t!]
%\centering
%\includegraphics[width=\textwidth]{figs/reconstructed_world_priors.pdf}
%%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
%\caption{xxx.}\label{fig:worldPriors}
%\end{figure}


\begin{figure}[t!]
    \centering
      \subfigure[Human comparison class inference data and model predictions for ten items. The log ratio frequency of subordinate NP to superordinate NP is shown in brackets next to the x-axis labels; a more negative number corresponds to a stronger prior belief in the superordinate category as the comparison class.]{\label{fig:residuals}\includegraphics[width=\textwidth]{figs/bars_ccinfOnly_finalExpt_byNP_exampleItems_intercept_slope_300k.pdf}} \\
    \subfigure[Imputed prior distributions over degrees for ten items.Ddistributions were generated from the Maximum A-Posteriori parameter values inferred by conditioning on the Comparison Class Inference and Adjective Endorsement data sets.]{\label{fig:worldPriors}\includegraphics[width=\textwidth]{figs/reconstructed_world_priors2.pdf}} 
    \caption{Quantitative modeling results for ten sets of items. See SI for full data-analytic model.}
    \label{fig:by_item_results}
\end{figure}


%\begin{figure}[t!]
%\centering
%\includegraphics[width=\textwidth]{figs/bars_ccinfOnly_finalExpt_byNP_topResdiuals_intercept_slope_300k.pdf}
%%{\centering \includegraphics{figs/bars_adj_finalExpt_pilot_byItem} }
%\caption{Human data and model predictions for items with small residuals (left 4) and largest residuals (right 4).}\label{fig:residuals}
%\end{figure}


%The parameterization of the comparison class prior does not directly affect the model's predictions for the Adjective Endorsement (Expt.~3) data; these predictions are purely a function of the RSA model (see Supplement) and the distributional world knowledge used for both experiments (e.g., the distributions of heights for people). Thus, it is no surprise that all model variants do a good job at explaining the Adjective Endorsement data, and it is a good sanity check that the adjective endorsement RSA model is of the right form to accommodate the Adjective Endorsement data.
%Our full model does a quite good job at accounting for the gradability of these inferences ($r = 0.88; r^2 = 0.77$)

To gain further insight into the comparison class inference results, we examine the maximal model's posterior distribution over parameters.
The speaker optimality parameters for each task were inferred to be values consistent with the prior literature on RSA models --- means and 95\% Bayesian credible intervals: $\alpha_1 =  $ \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_speakerOptimality_subordinateCC_NA}, $\alpha_2 =  \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_speakerOptimality_adjEndorse_NA}$.
We also find a positive effect of frequency of the noun phrase -- $\beta_1 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_frequency_basic_super}$ -- indicating that more frequent noun phrases give rise to more salient comparison classes.
Our maximal model infers two parameters for the basic-level bias, one which operates if the particular item is a subordinate-level category (and thus, the more superordinate category would be the basic-level; $\beta_0^0 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_sub_basic}$) and one that operates if the item is a basic-level category (and thus, the more superordinate category is the superordinate-level; $\beta_0^1 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_basic_super}$).
We can use these parameters to impute prior probabilities of subordinate, basic, and superordinate comparison classes (assuming a constant effect of usage frequency).
These parameters show that basic and subordinate comparison classes are roughly equally accessible while superordinate comparison classes are substantially less likely (Figure \ref{fig:parameters}).

%The full model infers on a by-item basis whether the basic-level category is the supplied NP or whether the basic-level category is an unmentioned, more superordinate category, because we expect the basic-level bias to operate differently for these different regimes of items.
%As expected, the comparison class prior NPs that were categorized as basic-level categories strongly favored those mentioned categories -- $\beta^1_0 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_basic_super}$.
%For NPs that were categorized as subordinate categories (i.e., the basic-level category was an unmentioned, more superordinate category), the comparison class prior showed no appreciable basic-level bias -- $\beta^0_0 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_intercept_sub_basic}$.
%From these parameters values, we can compute the overall imputed prior probabilities of subordinate, basic, and superordinate comparison classes (assuming a constant effect of usage frequency), which reflect the finding that basic and subordinate comparison classes are highly accessible and superordinate comparison classes less so (Figure \ref{fig:parameters}).
%and a positive effect of frequency of the noun phrase -- $\beta^0_1 = \hdiresults{model_params_cis.csv}{byItem-intercept_single-slope_beta_frequency_sub_basic}$.





\begin{center}
\begin{table}[h]
\centering
\pgfplotstabletypeset[sci zerofill,
    col sep = comma,
    every head row/.style={before row = \toprule, after row = \midrule},
    every last row/.style={after row = \bottomrule},
    columns/model_variant/.style={string type, column name={Model}, column type = l},
    columns/subordinateCC_r2/.style={string type, column name={$r^2_{CC}$}, column type = l,  sci sep align, precision=3},
   columns/subordinateCC_mse/.style={fixed, column name={$MSE_{CC}$}, column type = l, dec sep align, precision=4},
    columns/adjEndorse_r2/.style={string type, column name={$r^2_{norming}$}, column type = l, sci sep align, precision=3},
     columns/adjEndorse_mse/.style={fixed, column name={$MSE_{norming}$}, column type = l, dec sep align, precision=4},
     columns/log_bf/.style={fixed, column name={log BF}, column type = l, dec sep align, precision=0}
     ]{csv_data_4_tex/mse_r2_table.csv}\caption{Model evaluation results. Full Basic-level and Frequency model exhibits the best fit to both data sets in terms of variance explained $(r^2)$ and mean squared error (MSE). (log) Bayes Factor are shown with respect to the full model (i.e., negative numbers indicate positive evidence for the full basic-level and frequency model).}\label{tab:r2bf}
\end{table}
\end{center}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figs/model_comparisonClassPriorParameters.pdf}
\caption{Imputed distributions over the prior probabilities of comparison classes at different levels of abstraction. Basic- and subordinate-level categories comprise \emph{a priori} likely comparison classes, while superordinate categories are less likely to serve as comparison classes.}\label{fig:parameters}
\end{figure}


Our fully parameterized model triangulates its inferences about the comparison class via background knowledge about the property and the comparison class prior, given by a basic-level bias and the corpus frequency of the subordinate NP. Figure \ref{fig:by_item_results} shows these sources of information for ten sets of subordinate NPs. 
One can see that the middle-of-the-scale control items were sometimes inferred to be closer to high-end of the scale (e.g., the \emph{parrot} is considered light in color relative to other birds) and sometimes closer to the low-end of the scale (e.g., the \emph{elephant} is considered slow relative to other animals; Figure~\ref{fig:residuals}). These judgments result in different imputed  world knowledge priors (Figure~\ref{fig:worldPriors}) which leads to different comparison class inferences (Figure~\ref{fig:residuals}, Comparison Class Inferences). For example, the \emph{elephant} patterns more like the \emph{sloth} than it does the \emph{cheetah} in its comparison class inferences: when described as \emph{fast}, it is more likely to be considered \emph{fast for an elephant} than when it is described as \emph{slow} (similar to the effect observed for \emph{cheetah}).

The flexibility of the fully parametrized model allows it to make a wide range of quantitative predictions, which track the human data quite well. For example, comparing the \emph{birds} items (Fig.~\ref{fig:residuals} 2nd column) to the \emph{meat} items (Fig.~\ref{fig:residuals} 3rd column), we observe a dramatic difference in the overall preference for the subordinate vs. superordinate comparison class (with the birds items more strongly preferring the specific subordinate categories e.g., crow, parrot, seagull; and the meat items more strongly preferring the more superordinate category e.g., meat), which can be accounted for by a stronger basic-level bias for \emph{birds} than for \emph{meat}. 
The influence of corpus frequency in the comparison class prior can be seen most clearly for item sets where there are different baseline subordinate~vs.~superordinate preferences within the set of subordinate categories. For example, in the \emph{vehicles} items (Fig.~\ref{fig:residuals} right-most column), we see a strong preference for the subordinate comparison class \emph{trucks} in the human data; at the same time, the baseline preference is stronger for the superordinate \emph{vehicles} when talking about \emph{smart cars} or \emph{sedans}. The model is able to capture this pattern through the frequency component of the prior on comparison classes (the log-frequencies are shown in parentheses next to the category label in Fig.~\ref{fig:residuals}); \emph{trucks} is a more common label than either \emph{smart cars} or \emph{sedans}, which increases the modelâ€™s baseline preference for the trucks comparison class. 

The data on comparison class inferences, however, sometimes exhibits even more variability and sometimes exhibits less variability than predicted by our model. 
For example, the model predicts variability in the inferences from the \emph{weak}~vs.~\emph{strong} \emph{teenagers}~vs.~\emph{adults}, where no such variability is observed empirically (similarly for the entire item set of \emph{spices}, see Fig.~S7). 
Comparison classes inferences concerning the strengths of different walls (tents, condos, mansions) is largely a result of different baseline preferences: \emph{tents} is a rather good comparison class whereas \emph{condos} and \emph{mansions} are less strongly preferred (Fig.~S7).
The variability for writing tools is also quite idiosyncratic: \emph{pens} are largely compared to other \emph{colors} (coded as superordinate), whereas \emph{pencils} and \emph{chalk} are compared to other pencils and chalk respectively as well as different \emph{scripts} (superordinate). 
Taken together, these results indicate that human comparison class inferences are highly flexible, in a way that reflects distributional knowledge about properties of categories, a bias for basic-level categories, and knowledge of usage frequency; still, further variance remains to be explained even in the experimentally controlled data set we collected.

% \ndg{model section makes a big deal out of L1 vs L0 (with / without "theory of mind") models... but we don't actually test that. so, either add a model comparison here or get rid of that in model section.}

%the model struggles to match participants' inferences for the comparison class for \emph{shopping malls}, which has an NP-frequency similar to the other subordinate categories in its set (\emph{ice rinks}, \emph{saunas}) and has a world knowledge prior distribution that falls right in the middle of the scale (Fig.~\ref{fig:worldPriors}); thus, the model predicts no difference in comparison class inferences from adjectives \emph{hot}~vs.~\emph{cold}, while participants are more likely to treat a cold shopping mall similar to a cold ice rink -- \emph{cold for a place} -- arguably, more consistent with world knowledge in an American context (i.e., malls in America tend to be heavily air-conditioned). This discrepancy might result from the comparison class inference task leading to slightly richer interpretations than the norming study, perhaps as a result of participants having to do freely produce a response (as opposed to giving a binary decision, as in the Adjective Endorsement norming study; see SI for details). 


%Figure \ref{fig:residuals} shows the four item sets with largest mean model residuals (as well as the four item sets with smallest mean model residuals).

%Though our fully parameterized model does the best job at accounting for the variability in comparison class inferences, the data set exhibits even more variability than our model can account for. Figure \ref{fig:residuals} shows the four item sets with largest mean model residuals (as well as the four item sets with smallest mean model residuals).
%For example, the model struggles to match participants' inferences for the comparison class for \emph{shopping malls}, which has an NP-frequency similar to the other subordinate categories in its set (ice rinks, saunas) and has a world knowledge prior distribution that falls right in the middle of the scale (Fig.~\ref{fig:worldPriors}); thus, the model predicts no difference in comparison class inferences from adjectives \emph{warm}~vs.~\emph{cold}, while participants are more likely to think a 

%do not show the predicted effect of expectations and also show a fairly strong basic-level bias for the more superordinate category of \emph{spices}.
%A similar pattern can be observed for the \emph{doorways} item and the \emph{condo} and \emph{mansion wall} items, where human inferences much more strongly support the more superordinate categories than the model's inferences.
%\mht{our assumption of gaussian priors might be incorrect}
%In our maximal model, we inferred two separate basic-level bias parameters, one for superordinate~vs.~basic-level comparisons and one for basic~vs.~subordinate-level comparisons.
%These item mismatches suggest \mht{revisit after re-running model comparisons?}
%see an overall weak preference for basic-level categories over subordinate level categories and so the model does not show a strong preference for \emph{spices}.
%These cases are all artifact categories, suggesting that the effect of the basic-level on comparison class inferences may be different for artifacts as for animals, perhaps owing to differences in conceptual representations of these higher-order categories \cite{gelman1988development, kalish1998natural, rhodes2009developmental}.
%%Spices: garlic is more like salt, but people have a strong preference for "spices" here... because sub and basic level categories are assumed to have equal probability, we fail here.
%\ndg{but presumably there were other artifact categories in the data set that worked better?}





%The full model's posterior over the RSA and data-analytic parameters were consistent with prior literature and intuition. The maximum a-posteriori (MAP) estimate and 95\% highest probability density (HPD) intervals for model parameters specific to the \(L_1\) model used for comparison class inference were \(\alpha^{1}_{1} = 1.60 [1.10, 2.50]\), \(\beta = 0.13 [0.11, 0.19]\). Model parameters specific to the \(S_2\) model used for adjective endorsement: \(\alpha^{2}_{1} = 3.50 [0.60, 13.20]\), \(\alpha^{2}_{2} = 3.20 [2.60, 3.80]\). The inferred distributions corresponding to subordinate class priors were consistent with the \emph{a priori} ordering of these subordinate classes (low, medium, high) used in these tasks (Figure \ref{fig:modelParameters} top).

%Finally, the full model's posterior predictive distribution does an excellent job at capturing the quantitative variability in comparison class inferences: \(r^2(30) = 0.96\), and adjective endorsements: \(r^2(30) = 0.98\) (Figure \ref{fig:posteriorPredictiveScatters}). Because of the overall preference for the subordinate comparison class, many of the data points are distributed above 0.5. Even for these fine-grained differences, the model does a good job at explaining the quantitative variability in participants' data (Figure \ref{fig:posteriorPredictiveScatters} right). Thus, the variability in comparison class inferences we observe in our behavioral data can be accounted for the constructs posited in our model (namely, the comparison class prior and degree priors).

\section{General Discussion}

%Inferring the comparison class from such a generative model goes beyond a model of concepts, however; listeners must reason about a speaker's behavior...


%
%
%This omission is problematic for
%
%
%
%
%
%Theories of semantic composition for dealing with relative adjectives assume some comparison class.
%
%
%

%understand that \emph{big} is relative \cite{Sera1987} and that the
%comparison class can change





%Previous research has focused on what occurs during language understanding once a comparison class is determined.
%The question of how listeners decide upon a comparison class when it is not stated explicitly (e.g., ``It's warm relative to other days this winter'') has been addressed neither formally nor empirically.


%The speaker's choice of noun phrase can strongly influence the comparison class  (e.g.,  a \emph{big snowman} is probably big relative to other snowmen), though it need not determine it: saying ``That's a big snowman'' to a 4-year-old might mean \emph{big relative to snowmen a 4-year-old could build} \cite{kamp1975two}.

% In this paper, we investigate the first aspect of this open-ended inference problem, deciding among multiple possible comparison classes.


%The existence of comparison classes for understanding relative adjectives is uncontroversial \cite{cresswell1976semantics, klein1980semantics, kennedy2005scale, bale2008universal, Bale2011, Solt2009}. %\red{more standard citations for this?}



Understanding the meanings of words often, if not always, requires appreciating the context in which those words are uttered.
Yet, context is almost never articulated explicitly, but left to the listener to pragmatically reconstruct.
Inferring comparison classes for relative statements (e.g., scalar adjectives like \emph{tall}) is a case study in the larger problem of pragmatic reconstruction of context and is the communicative variant of the well-known \emph{reference class problem} in philosophy.
In this paper, we propose that listeners flexibly adjust the comparison class by using their knowledge of the world to inform them about the kinds of observations that would be remarkable enough for a speaker to mention.
We implement this proposal by extending an existing language understanding model to reason about comparison classes. Its prediction are empirically confirmed in an open-ended response measure using a diverse range of linguistic stimuli.
The strong fit of this model suggests that comparison class inference can be viewed as cooperative pragmatic inference.

We proposed a minimal extension to an adjective-interpretation Rational Speech Act model to allow it to flexibly reason about the implicit comparison class (e.g., \emph{tall for a person}~vs.~\emph{tall for a basketball player}).
%The model generated the qualitative prediction that listeners should prefer more specific (e.g., subordinate-level) comparison classes when the adjective conflicts with their general expectations about a member of a category (e.g., a basketball player who is short), which we confirmed by asking participants to freely explicate the intended comparison class.
%The fact that participants freely produced these comparison classes suggests that these inferences vary spontaneously in context, without an experimenter directly raising alternative comparison classes.  
%The model also made quantitative predictions about the item-level variation of this inference given knowledge about properties and categories.
We have thus addressed a particular aspect of comparison class inference: deciding among two conceptually-based comparison classes, that we pre-specified as subordinate~vs.~superordinate category labels.
The restriction of our model to considering only two comparison classes was a convenient simplification, but the fact that the majority of responses naturally produced in a free-production measure were either subordinate or superordinate category labels suggests that this was also a reasonable simplification. 
At the same time, the variability in responses was substantially greater than our idealized model could account for: Participants freely produced comparison classes that were either more abstract or less abstract than our superordinate comparison classes as well as comparison classes that fell along different conceptual hierarchies. 
The comparison class prior in our model could be elaborated to encode a more structured theory about how the hypothesis space of comparison classes is constructed (e.g., via a rational clustering model that considers higher-order structures in categories and properties \emph{a la} \citeNP{kemp2012integrated}).
%Still, other kinds of comparison classes are possible (e.g., perceptually-based comparisons and functional comparisons as in ``big for the doll'') and are understood by even very young children \cite{Ebeling1994}.


%The fact that superordinate categories are dis-preferred may be a product of the fact that superordinate categories tend to have higher variability across members than basic-level or subordinate level categories.
%Our modeling work also shows that usage frequency of the noun phrase contributes to the comparison class prior.
%Corpus frequency is a composite measurement of factors relevant for speech production.
%Its utility in this model suggests that utterances without an explicit comparison class (e.g., ``It's warm outside'') may in fact be elliptical sentences, in a way analogous to sentence fragments studied in noisy-channel models of production and comprehension \cite{bergen2015strategic}.

%Our modeling work pointed to a basic-level bias and corpus frequency as two factors that inform inference of the comparison class, but developing a more general theory of the comparison class prior will be an important step for this approach.
Going beyond general conceptual knowledge, many of the ``miscellaneous'' comparison classes that were elicited in our task appealed to other aspects of the context that were made available by the language in the task.
For example, the sentence \emph{Robert is at a decoration shop and looks at a statue made of bronze} brings into the context different concepts that could be parametrically combined to construct comparison classes -- bronze, statues, decorations, shop -- and several participants appealed to these concepts in their responses (e.g., expensive relative to \emph{items at the shop}).
These observations dovetail with prior work that investigated adjective interpretation with comparison classes in young children and which highlight the importance of linguistic cues for comparison classes.
For example, for a 4-year-old, the comparison class for a ``tall pimwit'' is \emph{other pimwits} and does not include other non-pimwit objects \cite{Barner2008}.
Even younger children can flexibly shift between qualitatively different kinds of comparison classes, given strong linguistic cues to distinguish the intended comparison class (e.g., ``Is this a \emph{big mitten}?'' vs. ``Is this mitten \emph{big for the doll}?''; \citeNP{Ebeling1994}).
%This prior work highlights the importance of linguistic cues for comparison class inference, while our work -- in which the speaker does not use a category label (e.g., ``basketball player'', ``winter'') in their adjectival utterance (e.g., ``He's tall''; ``It's warm'') -- demonstrates that such cues are not necessary for (adult) listeners to flexible adjust the comparison class. 
One key future direction then is determining the cues that are available in the naturalistic environment and how adult and child listeners use those cues to infer comparison classes.
The computational machinery we present in our model of comparison class inference is general and should apply equally well to these other kinds of comparison classes, once a hypothesis space of comparison classes is determined.


%For example, how do we know to even consider the comparison classes of basketball players vs. people, and not all objects in general?
%\emph{Objects} may be a comparison class that listeners would consider given the right context and our assumption of constructing comparison classes out of a taxonomic hierarchy is consistent with \emph{objects} being a candidate comparison class.
%However, just because a comparison class is in the hypothesis space does not mean that is a likely comparison class: Indeed, we observe in our quantitative modeling results that a uniform prior distribution over comparison classes is unlikely, and in fact, superordinate categories have the lowest probability of providing the comparison class.


In addition to the novel empirical data and the computational model of comparison class inference, this paper also presents experimental and data-analytic methodological innovations.
On the experimental side, we articulated an explicit generative model for our experimental stimuli, which we deployed on human participants to construct a large and diverse set of linguistic items (n = 540 unique stimuli).
While this procedure was not entirely ``end-to-end'' (i.e., we authors still needed to curate, edit, and add context to the items), the method presents a significant advance beyond the traditional method of constructing a small set of stimuli, often inadvertently optimized to test a theory \cite{Clark1973}.
On the data-analytic side, we coupled a \emph{descriptive Bayesian} approach \cite{tauber2017} with the productivity of probabilistic models of language understanding \cite{Goodman2016, scontras2017probabilistic} to jointly model two complementary language tasks and infer the relevant prior knowledge shared between the tasks.
The major feature of this method is that it allows us to back-out quantitatively detailed domain knowledge that would otherwise be inaccessible through traditional prior elicitation techniques because human participants lack requisite knowledge of the quantitative scales (e.g., how many decibels is the sound of a rooster's crow?);
%\ndg{didn't we do this in previous work?}\mht{we did this in the cogsci paper, and i suppose we did a version of this with generics and targeting parameters of the structured prior..., but the second point of explicitly modeling the language i think is novel}; 
in addition, this method has the feature that  participants respond only to simple, natural language questions rather than estimate numerical quantities for which complicated linking functions must be designed \cite<cf.,>{Franke2016}.
This fully Bayesian language approach  provides further constraints on the language understanding models, which must predict quantitative data from two related language experiments.
This approach highlights how the productivity of natural language can be harnessed to productively design experiments to further constrain and test computational models of language and cognition.

In our stimuli, a speaker utters an adjectival sentence without a strong cue to the comparison class: \emph{He [a basketball player] is tall}.
Why might speakers not bother articulating the comparison class? 
If a listener is trying to infer the comparison class (like the model we present in this paper), then a speaker (who is reasoning about this listener) could save the effort of producing the comparison class when they believe the listener will correctly infer it.\footnote{The speaker here would be operating roughly according to a principle of trying to communicate more information with less effort.}
In the SI, we formalize this kind of higher-order speaker, who reasons about whether or not the listener will correctly infer the comparison class; furthermore, we show that a listener who reasons about this speaker (i.e., a higher-order listener) draws qualitatively similar comparison class inferences to the simpler listener model we present above. 
Future work can be done to interrogate the predictions of the higher-order speaker to better understand when human speakers decide when to make the comparison class explicit.


Comparison classes are useful for learning linguistic meanings because they allow us to generalize the lexical semantics of many different kinds of adjectives and other linguistic messages that convey relative meanings.
For example, the semantic meaning of \emph{tall} and \emph{warm} have equivalent semantic functional forms, which vary only in the dimension the adjective picks up: $\denote{tall} = \text{height}(x) > \theta$ and $\denote{warm} = \text{temperature}(x) > \theta$.
Even generic language (e.g., \emph{Birds have hollow bones}) can be understood by a threshold semantics: $\denote{ \emph{Birds fly south in the winter}} = P(x \text{ flies south in the winter} \mid x \text{ is a bird}) > \theta$ \cite{Tessler2019psychrev}. 
Human cognition can use the minimal semantic template -- $\denote{words} = \text{dimension}(x) > \theta$ -- and infer the intended comparison class (which determines $\theta$) to create an infinity of possible meanings, and natural language seems to embrace these kinds of ambiguities \cite{piantadosi2012communicative}.
It remains an open question, however, whether learning this common structural form of meaning requires a model with built-in structure.
For example, large-scale deep learning (especially transformer-based) sequence models that are trained from huge amounts of text (i.e., Large Language Models, such as BERT or GPT-3; \citeNP{devlin2018bert}, \citeNP{brown2020language}) exhibit surprisingly sophisticated syntactic knowledge, even on syntactic constructions that are rare in natural text \cite{linzen2016assessing, futrell2018rnns, wilcox2021syntax}.
An important future direction for this line of work is to test whether such models learn a shared semantic representation for scalar adjectives (or, relative statements more broadly) and whether the Large Language Models appreciate the context-sensitivity that comes with different comparison classes.
%[ndg: cite piantadosi, tily, gibson here for the functional reasons that language embraces such ambiguity?
% aSuch simple semantic representations, coupled with a powerful inferential cognitive mechanism, could help explain why young children acquire such context-sensitive language (e.g., the word \emph{big}) so early \cite{Sera1987,Ebeling1988,Mintz2002,Sandhofer2007}: General purpose inferential mechanisms can be used to determine the comparison class, thus filling in a large part of the meaning in context.


%Previous work investigating the role of the comparison class in adjective interpretation has deployed strong linguistic cues to convey the speaker's intended comparison class.

%The fact that young children, who do not have the same kind of world knowledge as adults, can flexibly adjust the comparison class raises the question of what kinds of cues are available to them (and to adults) in the naturalistic environment for comparison class inference.

%\mht{minimal cues... also, NP $\neq$ cc}
%A relevant detail of our experiment contexts is that the speaker's sentence did not include a noun phrase to describe the referent (i.e, the speaker said ``He is tall'' as opposed to ``That basketball player is tall'').
%We found that when the adjective is consistent with the listener's general expectations about the category (e.g., the basketball player is tall), listeners prefer comparison classes that are also more general (i.e., tall for a person).
%Intuitively, the speaker's intentional production of a noun phrase could more directly communicate the comparison class by revealing how the speaker is conceptualizing the referent (e.g., \emph{the speaker is conceiving of this person as a basketball player}).
%The extent to which this inference holds could also depend upon the syntactic structure of the sentences:
%Prenominal uses of the adjective (e.g., ``He's a tall basketball player'') might be an even stronger cue.
%Indeed, prenominal uses are argued to be ideal for a child learning the meaning of novel adjectives \cite{Waxman2001, Mintz2002, Sandhofer2007} perhaps because it is such a strong cue to the comparison class.
%Future work should investigate the interaction between syntactic structure and pragmatic inferences regarding the comparison class.

The phenomenon of comparison class inference is a case study in \emph{context inference}, or reasoning about the set of beliefs that are in common ground between speaker and listener.
%Our model is similar in structure to
The model is thus similar in structure to models of presupposition accommodation \cite<e.g.,>{Qing2016projective, Degen2015}, which linguistic theories classically pose as involving a listener adding or revising information to the common ground in order to make sense of an utterance. 
For example, if John says to Mary ``My car is in the shop'', it is not necessary \emph{a priori} that Mary know that John has a car; if Mary did not know (or she temporarily forgot) that John owns a car, Mary can \emph{accommodate} John's utterance by adding to the common ground the presupposition that \emph{John has a car}.
The continuity in the computational formalisms that underly this sort of common ground revision suggest a very basic cognitive mechanism behind the pragmatic reconstruction of context \cite<e.g.,>{levinson2000presumptive}. 

Comparison class inference is also related to the well-known problem of \emph{reference classes} \cite{Reichenbach1949, hajek2007reference}. In communication, pragmatic mechanisms are at play in how listeners decide upon an appropriate comparison class. This investigation thus raises the question of whether or not similar social reasoning occurs in legal or actuarial contexts, where reference class inference can impact everything from the value of a house to the severity of a criminal sentence \cite{colyvan2001crime, cheng2009practical}. 
If so, the very way in which we appreciate justice being served or the value of property stems from a cooperative, communicative framework. 





% \ndg{... finish that sentence? also worth throwing a cite (and maybe a very short discussion) to the qing, goodman, lassiter (2016). i think this is the only previous rsa model of presupposition accommodation?}

%The situation we model is one of asymmetric knowledge: The listener is uncertain about the comparison class, but assumes that the speaker does not represent their uncertainty.
%The comparison class assumed by the speaker, then, acts as a kind of presupposition, and for the formal modeling mechanism we employ has in fact been deployed in modeling presuppositions \cite<e.g., ``When did John quit smoking?'' implies that John used to smoke;>{Qing2016projective}.
%In our experiment, the speaker is talking to a third party and the actual comparison class inference may be a result of the participants' uncertainty about the information in common ground between the interlocutors; future work should examine dyadic interactions to better understand the role of asymmetric knowledge in inferring comparison class.
%\mht{relation to accommodation... pretend as if i the listener didn't know the referent was a basketball player}
%comparison class most likely to make the utterance true while
%prioritizing more specific (lower variance) classes because they are
%more informative. It also made quantitative predictions about how
%background knowledge about the degree scale and what classes are likely
%to be talked about \emph{a priori} should inform this inference in a
%graded fashion. Both qualitative predictions of the model were borne out
%in our behavioral experiment, and the quantitative predictions were
%confirmed using a novel data analytic technique.



%This problem of inferring comparison classes is closely tied to the problem of inferring the intended referent, or deconstructing how the speaker is conceptualizing the referent, of an utterance.
%For example, ``That's a beautiful cat'' could be said felicitously of a drawing of a cat, in which case, the comparison class is \emph{drawings of cats}; thus, the meaning of ``cat'' in the original utterance should be understood as \emph{drawing of a cat} and not an actual cat.
%In a similar way, inferring that the speaker meant the basketball player is short \emph{for a basketball player} might be tantamount to inferring that the speaker is conceiving of the referent as a basketball player.
%This potential symmetry between comparison classes and representations of referents could have implications for theories of alternative utterances: For example, the speaker said ``cat'' and not ``drawing of a cat'' because they believed the listener would not be confused, given the context, that the referent is not an actual cat.




%Our solution at present is to say that \emph{objects} is a comparison class with low probability, but why should it have low prior probability?
%From our modeling work, we found that the \emph{superordinate} level was the least likely to give rise to a comparison class, and that may result from superordinate categories having a
%\mht{Show simulations of comparison class inference with classes that have same mean but increase in variance?}
%For example, the comparison class of ``people'' for heights of individuals is relatively more salient than the class of ``produce'' (or, ``fruits and vegetables'') for the weights of particular fruits and vegetables.
%As a proxy for the prior probability of a comparison class, we used a (logistic) linear function with an intercept to reflect a basic-level bias \cite{rosch1975family} and a slope of the frequency of the noun phrase in a corpus.



%\mht{relation to  the inference about the comparison class being ``the speaker is thinking of the referent as an X''... reduces to same thing?}

%It's been suggested that there exist qualitatively different kinds of
%comparison classes, constructed by reference to either: the perceptual
%context (a \emph{perceptual comparison class}), a goal of an agent or
%intended use of an object (a \emph{functional comparison class}), and
%the kind of the referent (a \emph{conceptual comparison class}; Ebeling
%\& Gelman, 1994). In this paper, we investigated the latter, but the
%question remains about how a listener should decide to switch between
%different kinds of comparison classes. For example, if Ann and Carl are
%deciding what to do on a Friday night, Carl suggest the ballet, to which Ann replies ``The
%ballet is expensive'', Carl should understand Ann's statement relative
% \emph{other ways they could spend their Friday night}, a kind-of
%functional comparison class. Goal inference is thus an associated
%ingredient in comparison class inference, and integrating the two
%should be a target for future work.





%Investigations of how human listeners understand vague adjectives have
%shed light on the precise mechanisms by which people interpret
%context-sensitive language, but have had little to say about how
%listeners decide upon what counts as the appropriate context. In this
%paper, we take the first step towards investigating the flexibility in
%the class against which an entity can be implicitly compared, a very
%basic form of context. Resolving underspecification by means of a comparison is not unique to
%\emph{gradable adjectives} like ``big'' or ``tall'', but is
%a general problem for language understanding. ``John ate \emph{a
%lot} of hot dogs'' probably means four or five hot dogs, whereas
%``John ate \emph{a lot} of potato chips'' could imply a quantity
%over a hundred (SchÃ¶ller \& Franke, 2017); ``Robins lay eggs''
%means roughly that \emph{female robins} lay eggs, whereas
%``Robins fly'' entails something stronger, most or all robins fly
%({\textbf{???}}; {\textbf{???}}). Even noun concepts (e.g.,
%``furniture'') are graded (Rosch \& Mervis, 1975) and can be made
%more precise in context. Investigating how listeners interpret words
%like ``big'' is thus a case study of a crucial, general problem in
%language understanding: Understanding context-sensitive language.
%
%We investigated the question of how a listener decides among multiple
%possible conceptually-based comparison classes (e.g., tall for a
%basketball player vs.~tall for a person). It is notable that our model
%was able to account for inferences about items that did not fall into
%strict hierachies (e.g., \emph{movies} is not subordinate to
%\emph{things you watch online}) as well as items that did (e.g.,
%basketball players and people). This result suggests that our modeling
%framework can easily extend into cross-cutting comparison classes (e.g.,
%\emph{men}, \emph{people of a certain age}, \emph{basketball players}).
%
%\subsection{The phenomenon of comparison class
%inference}

%\subsection{Comparison class prior}
%
%We observe in our quantitative modeling results that a uniform prior
%distribution over the experimentally supplied comparison class
%alternatives is unlikely (Figure \ref{fig:modelParameters} bottom). For
%example, the comparison class of ``people'' for heights of
%individuals is relatively more salient than the class of
%``produce'' (or, ``fruits and vegetables'') for the weights
%of particular fruits and vegetables. We used the frequency of the class
%in a corpus as a proxy for their prior probability \(P(c)\), which was
%sufficient to account for differences in baseline class probability both
%\emph{between}- and \emph{within}-scales.
%
%Corpus frequency is a composite measurement of factors relevant for
%speech production. Its utility in this model suggests that utterances
%without an explicit comparison class (e.g., ``It's warm outside'')
%may in fact be incomplete sentences, in a way analogous to sentence
%fragments studied in noisy-channel models of production and
%comprehension (Bergen \& Goodman, 2015). Another (non-mutually
%exclusive) possibility is that the comparison class prior reflects
%basic-level effects in categorization (Rosch \& Mervis, 1975). Future
%work should attempt to understand these factors to construct a more
%complete theory of the comparison class prior.
%


\section{Conclusion}

The words we say are often too vague to have a single, precise meaning; they only make sense in context.
Aspects of the context, however, can also be
underspecified, leaving the listener in the dark about both the
speaker's intended meaning and about the context through which the
listener is to make sense of the conversation. This work suggests that listeners infer a lot from a little:
Meaning and context from simple, vague utterances.


\section{Acknowledgements}

We thank Polina Tsvilodub for her contributions to the preprocessing of the free production data and for implementing the adjective endorsement task. We also thank Michael Lopez-Brau for work on a preliminary version of this project. This work was supported in part by NSF Graduate Research Fellowship DGE-114747 and NSF SBE Postdoctoral Research Fellowship Grant No. 1911790 to  MHT and a Sloan Research Fellowship, ONR grant N00014-13-1-0788, and DARPA grant FA8750-14-2-0009 to NDG.

\newpage


\bibliographystyle{apacite}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliography{comparison-class}
%
%\newpage
%
%\section{Supplementary Methods}
%
%\subsection{Task 1: Stimuli Generation}
%
%\subsubsection{Participants}
%
%We recruited 50 participants from Amazon's Mechanical
%Turk, restricted to those with U.S. IP addresses with at least a
%95\% work approval rating. The experiment took about 5 minutes and
%participants were compensated \$0.60 for their work.
%
%\subsubsection{Materials}
%
%We used 15 pairs of positive- and negative-form gradable adjectives that describe physical dimensions (Table \ref{tab:1}).
%Some of the adjective pairs described the same dimension and/or had an adjective in common with another pair (e.g., \emph{fast}--\emph{slow} and \emph{quick}--\emph{slow}); these were included because we believed they would bring to mind different kinds of categories.
%
%\subsubsection{Procedure}
%
%On each trial, participants were given a phrasal template, for which they were asked to fill-in the missing elements.
%The template was composed of three sentences, each missing a subject; the content of the sentence described the subject as one generally having the adjective applied to them (Figure \ref{fig:exptOverview}, Step 1).
%These sentences were followed with another that said that the three kinds of subjects from the previous sentences were all instances of a more general category: ``all kinds of \_\_\_''.
%This last sentence was introduced to constrain the kinds of responses to the first three sentences such that the subjects would be all members of the same basic- or superordinate level category.
%
%The template for ``big'' and ``small'' looked like:
%\begin{description}[noitemsep]
%%\tightlist
%\item \_\_\_\_\_ are generally big.
%\item  \_\_\_\_\_ are generally small.
%\item \_\_\_\_\_ are sometimes big and sometimes small.
%\item These three are all kinds of \_\_\_\_\_.
%\end{description}
%%
%Participants filled out one template for each of the 15 pairs of adjectives.
%
%
%\subsubsection{Results}
%From the full set of responses (750, in total), we select sets of items to be used in the subsequent experiments.
%We selected item sets on the basis of hierarchal consistency of the categories (i.e., that the three generated categories corresponded to the same level of abstraction e.g., they were all subordinate categories of the same basic-level category) and based on how well the items would make sense in the kinds of minimal contexts we use in Expt.~2.
%Two examples from each adjective pair are shown in Table \ref{tab:1}.
%We code each of the subordinate categories as to whether they were expected to fall low, high, or in the middle of the degree scale (i.e., those generated in response to the \emph{generally positive adjective}, \emph{generally negative adjective}, or \emph{sometimes positive and sometimes negative}).
%
%
%\subsection{Bayesian data analysis model supplementary details}
%
%Previous modeling papers in the RSA tradition empirically elicit world knowledge $P(x)$ through prior elicitation tasks.
%Such tasks typically involve estimating numerical quantities or making probability judgments, for which the participant must have relevant knowledge concerning the units of measure (e.g., prices in dollars) and/or for which complicated linking functions are designed to relate latent probabilities to the estimation data \cite{Franke2016}.
%In order to estimate world knowledge $P(x)$ for our large, heterogeneous set of items, we take a different approach.
%
%\subsubsection{World knowledge priors}
%
%In our model, world knowledge is represented by probability distributions over degrees (e.g., heights).
%Comparing interpretations of the same adjective across different comparison classes is an inherently relative judgment; thus, only the relative values for the degrees affect model predictions.
%Therefore, we fix each general (basic/superordinate) class in each domain to be a standard normal distribution---$P(x \mid c = c_{general}) = \mathcal{N}(0, 1)$---and assume the specific (subordinate/basic) priors to also be Gaussian distributions---\(P(x \mid c = c_{sub}) = \mathcal{N}(\mu_{sub}, \sigma_{sub})\); the subordinate priors thus have standardized units.
%These parameters vary by the category $k = c_{sub}$ as well as the degree scale (e.g., height vs. weight).
%We infer the plausible values of the parameters governing the subordinate priors from the data.
%
%%This model with uncertainty around the parameters of the world knowledge priors is overparameretized, however, with respect to the comparison class inference data. The data from the comparison class experiment (Task 2) is insufficient to reliably estimate the model parameters governing the world knowledge priors.
%To constrain the world knowledge parameters, we use the same RSA architecture to predict additional data about a related Adjective Endorsement task (Expt.~3).
%In the Adjective Endorsement task ($n=375$), participants are asked to judge whether a gradable adjective (e.g., \emph{warm} or \emph{cold}) would be true of a hypothetical member of a specific category relative to the basic- or superordinate level category (e.g., \emph{You step outside during the winter. Do you think it would be warm relative to other days of the year? ...cold relative to other days of the year?}).
%That is, we have participants judge the truth or falsity of adjectives when the comparison class is explicit (see Supplementary Information for full experimental details).
%
%To model the adjective endorsement data, we modify our comparison class inference RSA model by removing comparison class uncertainty from the model (since the task provides the comparison class explicitly) and, following \citeA{Tessler2019psychrev}, model sentence endorsement as a speaker deciding whether or not to produce the adjectival utterance to the listener: $S_{1}(u \mid k) \propto \exp{(\alpha_2 \cdot {\mathbb E}_{x\sim P_{k}}} \ln{L_0(x \mid u)})$, where $L_0(x \mid u)$ is given by Eq. \ref{eq:L0}.
%The speaker's decision rule marginalizes over their beliefs about the property value of the referent in the category (e.g., the speaker doesn't know the exact height of the referent, but knows that the referent is a basketball player and averages over plausible heights of basketball players: $x\sim P_{k}$).


%\newpage
%\section{Appendix}
%
%
%%Gradable adjectives like \emph{warm} and \emph{cold} are vague
%%descriptions of an underlying quantitative scale (e.g., temperature).
%Classic semantic theories posit the meaning of gradable adjectives to simply be that the scalar degree $x$ (e.g., temperature) is greater than or less than a contextually-supplied standard of comparison  \(\theta_c\): \([\![u_{pos}]\!] = x > \theta_c\), for positive-form adjectival utterance \(u_{pos}\) (e.g., warm) and \([\![u_{neg}]\!] = x < \theta_c\), for negative-form adjectival utterance \(u_{neg}\) (e.g., cold).
% Lassiter \& Goodman (2013) model the context-sensitivity of these adjectival utterances using threshold semantics, where
%the threshold is probabilistically set with respect to a comparison class \(c\) via pragmatic reasoning :
%
%\begin{align}
%L_{1}(x, \theta \mid u, c) &\propto S_{1}(u \mid x, \theta) \cdot P(x \mid c) \cdot P(\theta) \label{eq:L1} \\
%S_{1}(u \mid x, \theta, c) &\propto \exp{(\alpha_1 \cdot (\ln {L_{0}(x \mid u, \theta, c)} - \text{cost}(u)))} \label{eq:S1}\\
%L_{0}(x \mid u, \theta, c) &\propto {\delta_{[\![u]\!](x, \theta)} \cdot P(x \mid c)} \label{eq:L0}
%\end{align}
%
%Eqs. \ref{eq:L1} - \ref{eq:L0} are the Rational Speech Act mode  Lassiter \& Goodman (2013).
%In this model, a pragmatic listener \(L_1\)
%tries to resolve the degree \(x\) (e.g., the temperature)
%from the adjectival utterance she heard \(u\) (e.g., ``it's warm''), by assuming the utterance came from an approximately rational Bayesian
%speaker \(S_1\) trying to inform a naive listener \(L_0\), who in turn
%updates their prior beliefs \(P_c(x)\) via an utterance's literal meaning
%\([\![u]\!](x, \theta)\).
%Formally, the literal meaning is represented by the
%Kronecker delta function \(\delta_{\mbox{ $[\![ u ]\!]$}(x, \theta)}\)
%that returns probabilities proportional to \(1\) when the utterance is
%true (i.e., when \(x > \theta\)) and \(0\) otherwise.
%The key innovation used for modeling gradable adjectives is to have uncertainty over the
%semantic variable---the threshold \(\theta\) (Eq. \ref{eq:L1}). In
%Lassiter and Goodman (2013)'s model, \(\theta\) comes from an improper uniform
%prior distribution over thresholds defined over the real numbers and is resolved by the
%listener reasoning about the different thresholds a speaker might be
%using \(S_{1}(u \mid x, \theta)\) as well as the probabilities of
%different states of the world \(P(x \mid c)\) (e.g., different
%temperatures). Assuming the adjective adds some cost to the speaker's
%utterance (Eq. \ref{eq:S1}), the meaning of a gradable adjectives (e.g.,
%``warm'') is resolved by the pragmatic listener to mean something
%like ``significantly greater temperature than one might expect''
%(Lassiter \& Goodman, 2015). Critically, what ``one might
%expect''---the prior distribution over temperatures \(P(x \mid c)\)---is
%always with respect to some comparison class \(c\) (Eqs. \ref{eq:L1} \&
%\ref{eq:L0})
%

\end{document}
